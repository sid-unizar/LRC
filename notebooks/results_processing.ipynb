{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Process results LRC**\n",
        "This notebook is devoted to process the report files generated by the scripts used for Lexical Relation Classification (LRC) and graded Lexical Entailment (LE). The intend is to generate a dataframe to visualize the results for our models/templates, and compare with SoTA results. \n",
        "\n",
        "To reproduct the results in the paper with other result files, change the variables in a bellow cell that contains:\n",
        "\n",
        "```\n",
        "# folder with all datasets\n",
        "DIR_DATASETS = '/content/LRC-0C0F/datasets/'\n",
        "# folder with the results\n",
        "DIR_RESULTS = '/content/LRC-0C0F/results/'\n",
        "LIST_DIR_RES = [DIR_RESULTS + 'K&H+N/', \n",
        "                DIR_RESULTS + 'BLESS/',\n",
        "                DIR_RESULTS + 'EVALution/', \n",
        "                DIR_RESULTS + 'ROOT09/',\n",
        "                DIR_RESULTS + 'CogALexV/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_lexical_split/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_random_split/'\n",
        "                ]\n",
        "```\n"
      ],
      "metadata": {
        "id": "YUDo5LQQFBYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to download a anonymous github\n",
        "!git clone https://github.com/fedebotu/clone-anonymous-github\n",
        "!pip install -r clone-anonymous-github/requirements.txt\n",
        "\n",
        "# clone our anonymous github\n",
        "%cd /content/clone-anonymous-github/\n",
        "from src.download import download_repo \n",
        "import os\n",
        "\n",
        "config = {'url': 'https://anonymous.4open.science/r/LRC-0C0F/',\n",
        "                    'save_dir': '/content/',\n",
        "                    'max_conns': 256,\n",
        "                    'max_retry': 5}\n",
        "download_repo(config)\n",
        "\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "KRZ1VjIG0qFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download our results. Due to limitations of anonymous github, the results are download from google drive (anonymous special account)\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://drive.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1FzIGKGjbrFU_mdaJhEsCkk3FCYeQL6Ju' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1FzIGKGjbrFU_mdaJhEsCkk3FCYeQL6Ju\" -O results.zip && rm -rf /tmp/cookies.txt"
      ],
      "metadata": {
        "id": "C9BgfOsRED8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf81344-f6db-4ba2-ad1f-02d4ced5aacf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-31 15:50:19--  https://drive.google.com/uc?export=download&confirm=t&id=1FzIGKGjbrFU_mdaJhEsCkk3FCYeQL6Ju\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.101.101, 142.250.101.139, 142.250.101.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.101.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-08-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j3mc6ao07pq8ks7vjqbqa69uo4m2p9ak/1675180200000/13717404105681208342/*/1FzIGKGjbrFU_mdaJhEsCkk3FCYeQL6Ju?e=download&uuid=3b09e023-46be-4f09-ab89-480a12c01ea1 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-01-31 15:50:20--  https://doc-08-ac-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/j3mc6ao07pq8ks7vjqbqa69uo4m2p9ak/1675180200000/13717404105681208342/*/1FzIGKGjbrFU_mdaJhEsCkk3FCYeQL6Ju?e=download&uuid=3b09e023-46be-4f09-ab89-480a12c01ea1\n",
            "Resolving doc-08-ac-docs.googleusercontent.com (doc-08-ac-docs.googleusercontent.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to doc-08-ac-docs.googleusercontent.com (doc-08-ac-docs.googleusercontent.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 238320218 (227M) [application/x-zip-compressed]\n",
            "Saving to: ‘results.zip’\n",
            "\n",
            "results.zip         100%[===================>] 227.28M   161MB/s    in 1.4s    \n",
            "\n",
            "2023-01-31 15:50:22 (161 MB/s) - ‘results.zip’ saved [238320218/238320218]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o results.zip -d LRC-0C0F/"
      ],
      "metadata": {
        "id": "L7Ahhb3PEh54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9av5dK1mVLTm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**LRC task: BLESS, K&H+N, ROOT9, EVALution and CogALexV datasets**\n",
        "\n",
        "Lexical relation classification is the task of predict the lexical relation among two words, such as *(horse,animal) - hyponymy*.\n",
        "\n",
        "####**Models in the literature: baselines**\n",
        "The reported results in the literature for BLESS, K&H+N, ROOT9 and EVALution datasets are precision, recall and f1-score, weighted by the support of the labels. We have collected the results for the following models:\n",
        " - [LexNET](https://aclanthology.org/W16-5304/) (2016)\n",
        " - [SphereRE](https://aclanthology.org/P19-1169/) (2019)\n",
        " - [KEML](https://arxiv.org/abs/2002.10903) (2020)\n",
        " - [RelBERT](https://arxiv.org/abs/2110.15705) (2021)\n",
        "\n",
        "As far as our knowneldge, these are the last SoTA models. \n",
        "\n",
        "The reported results for CogALexV dataset are different. This dataset comes from the [CogALexV shared task (subtask 2)](https://sites.google.com/site/cogalex2016/home/shared-task) and it is well defined what results must be reported. In this dataset, there are five labels, including \"RANDOM\". The report results are the f1-scores for the four labels that are not the \"RANDOM\" one, and the weighted f1-score by the support of these four labels. \n",
        "\n",
        "####**How the baseline results are collected**\n",
        "We collect the results from the regarding papers.\n",
        "The reported results in RelBERT paper do not contain precision and recall values, only micro and macro f1 average, i.e., the accuracy (micro) and the average of the f1-scores (macro) for all labels, repectively.  We have calculated the weighted f1-scores for RelBERT with the data (table 4) in the paper. \n",
        "\n",
        "\n",
        "####**Our proposal for LRC**\n",
        "We fine-tune a pretrained language model (as BERT or RoBERTa) feeding the model with a verbalization of the two words by means of a template (see in a bellow cell the variables `models2abrev` and `templates2abrev` for the chosen models and templates). We need the following components: \n",
        "1. A pretrained language model (PTLM) $M$ and its token vocabulary $V_M$;\n",
        "2. A training set ${\\cal T}=\\{(\\boldsymbol{w}_i, y_i)\\mid i=1,\\dots n\\}$, where $\\boldsymbol{w}_i = (w^1_i, w^2_i)$ is a pair of words and $y_i\\in Y$ is the label of a lexical relation ($|Y|=K$);\n",
        "3. An injective function from the set of labels to the vocabulary of tokens $V_M$, $v\\colon Y\\to V_M$, called the *mask verbalizer* function;\n",
        "4. A training and a testing template, $T_t$ and $T_e$, used to verbalize $\\boldsymbol{w}_i$. \n",
        "In this context, a template $T$ is a function, $T\\colon V\\times V \\to {\\cal S}$, from pairs of the word vocabulary to the set of sentences where the `CLS`, `SEP` and `MASK` special tokens of the PTLM can appear in the sentence. We denote by $T(\\boldsymbol{w})_{C}$ and $T(\\boldsymbol{w})_{M}$ to the `CLS` and `MASK` tokens in the sentence $T(\\boldsymbol{w})$, respectively.\n",
        "\n",
        "Depending on the template used, we adopt one of the following two training objectives: \n",
        "1. It the train template does not contain the `MASK` token, the classification objective estimates the probability $P(Y=y_j| T_t(\\boldsymbol{w}_i)_C)$; \n",
        "2. It the train template contains the `MASK` token, a mask prediction objective is used to estimate $P(T_t(\\boldsymbol{w}_i)_{M}=t_j| T_t(\\boldsymbol{w}_i) )$, where $t_j \\in V_M$ is any token in the vocabulary of the PTLM.\n",
        "\n",
        "At inference time, for a model trained with a classification objective, we use the testing template $T_e$ to predict the label with \n",
        "$argmax_{y_i\\in Y} \\{P(Y=y_i| T_e(\\boldsymbol{w})_C)\\}$, and for the mask objective, $argmax_{y_i\\in Y} \\{P(T_e(\\boldsymbol{w})_{M}=v(y_j)|T_e(\\boldsymbol{w}))\\}$. For this latter case, note that at inference time, we only use the tokens given by the mask verbalizer function $v$.\n",
        "\n",
        "###**Graded lexical entailment task: Hyperlex dataset**\n",
        "Graded Lexical Entailment (graded LE) is a regression task. It consists of giving a score for the hyperonym relationship between two words $(w_1,w_2)$ expressing a degree that \"*$w_1$ is a hyponym of $w_2$*\". [Hyperlex dataset](https://arxiv.org/pdf/1608.02117v2.pdf) contains pairs of words with a score between 0-6. The score is the median of the ratings given by at least $10$ human annotators to the question \"*To what degree is $w1$ a type of $w2$?*\". The inter-annotator agreement (IAA) was calculated with the average Spearman correlation $\\rho$ of a human rater with the average of all the other raters. It was obtained IAA $\\rho = 0.864$. This is considered an \"*upper bound*\" for the performance of automatic systems. Thus, the task is to give a score for all pairs in the dataset and report the Spearman correlation between the median human annotator scores and the calculated ones.\n",
        "\n",
        "The pairs in the dataset were obtained from WordNet. It also annotated the lexical relation in WordNet and the POS. The pairs are nouns and verbs. The annotated lexical relations are:\n",
        "- `hyp-i` where $1\\le$ `i` $\\le 4$: Using the `isA` hierarchy, `hyp-i` means that the word $w_1$ is an hyponym of degree `i` of the word $w_2$. If `i`$=4$, means a degree greater or equal than $4$.\n",
        "- `r-hyp-i` where $1\\le$ `i` $\\le 4$: Same as `hyp-i`, but $w_1$ is a hyperonym of degree `i` of $w_2$.\n",
        "- `syn`, `ant`, `mero`, `cohyp` and `no-rel`: Words are synonyms, antonyms, meronyms, cohyponyms or non the above relations, respectively.\n",
        "\n",
        "\n",
        "The dataset is provided in three configurations:\n",
        "- *all pairs*: It contains all pairs (2616 pairs). It is usually used by unsupervised models.\n",
        "- *random split*: All pairs are randomly divided in train (1831 pairs), validation (130) and test (655) datasets. To train supervised models.\n",
        "- *lexical split*: It is also splitted into train/val/test datasets, but to avoid lexical memorization, there are not words in common between the train+val and test splits. To force this lexical split, the total pairs are reduced to 1133/85/269.\n",
        "\n",
        "####**Models in the literature: baselines**\n",
        "\n",
        "The visited models are: [HyperVec](https://arxiv.org/pdf/1707.07273.pdf) (2017), [Poincaré embeddings](https://arxiv.org/pdf/1705.08039v2.pdf) (2017), [LEAR](https://aclanthology.org/N18-1103.pdf) (2018), [SDNS](https://arxiv.org/pdf/1805.09355v1.pdf) (2018), [GLEN](https://aclanthology.org/P19-1476.pdf) (2019), [POSTLE](https://aclanthology.org/W19-4310.pdf) (2019), [LexSub](https://aclanthology.org/2020.tacl-1.21.pdf) (2020), [Hierchical-fitting](https://www.sciencedirect.com/science/article/abs/pii/S0950705122006517?via%3Dihub) (2022). All papers can be consulted in the paperswithcode web page about Hyperlex (https://paperswithcode.com/dataset/hyperlex).\n",
        "\n",
        "In a nutshell, all the following models start with a set of non-contextual embeddings, such as Skip-gram, Glove, fastText embeddings (except for HyperVec and Poincaré embeddings that starts to train the embeddings from scratch), and using WordNet, ConceptNet or any other resource, they obtain a set of synonyms, antonyms, hyponyms/hyperonyms pairs of words. The set of pairs is used to train new embeddings by means of a specialized loss function that separate the synonyms/hyponyms from antonyms/hyperonyms. The above models differ in the chosen loss function. Once the model is trained, it is defined a distance function to give the hyponymy degree. The distance functions somehow use the Euclidean distance.\n",
        "\n",
        "- HyperVec: For a vocabulary $V$, it is trained from scratch a word embedding $v_w$, for all $w \\in V$, in a similar fashion as Skip-gramm archictecture with negative sampling training objective (SKNS). The usual loss function in SKNS is modified adding some terms to learn *hierarchical embeddings* based on hyperonym/hyperonym hierarchy. It is needed to know the hyponyms/hyeronyms of the words (or at least of some words) during the training. The pairs of training hyponyms/hyperonyms come from WordNet. The final score is calculated with a distance function, *HyperScore*, that mixes the cosine distance and the norms of the calculated embeddings. The authors claim in the paper that all pairs in the evaluation datasets have been removed from the training dataset.\n",
        "\n",
        "- Poincaré embeddings: Similar to HyerVec, embeddings for words in a vocabulary $V$ are trained from scratch. The embeddings are learned optimizing a loss function depending on an hyperbolic distance. Given a set of unordered pairs $D= \\{\\{u,v\\}\\}$, where one of $u$ and $v$ is an hyperonym of the other, the loss function uses as positive examples the pairs of $D$ and, similar to negative sampling, uses as negative examples $10$ pairs that are not in $D$. The unordered pairs of $D$ are obtained from WordNet. The main goal of the hyperbolic embeddings are to reconstruct the complete hierarchy of a network knowing its hyperonym relationships. For graded LE, the authors define a score function based on the norms of the vectors and the hiperbolic distance. \n",
        "\n",
        "- LEAR: The starting point of this method is a set of non-contextual trained word embeddings. In particular, the authors test with SKNS, CBOW, Glove and fastText embeddings. Inspired by the *Attract-Repel* framework, these embeddings are retrained to capture the hypernoym structure of the concepts giving higher and lower norms to word embeddings situated in higher and lower positions of the hierarchy, while trying that the cosine distance mantains its properties about the similiraty of the words. The embeddings are retrained with sets of Attract ($A$), Repel($R$) and Lexical Entailment ($L$) examples. These examples are pairs of words obtained from WordNet and Roget's thesaurus, where the $A$, $R$ and $L$ pairs are $(u,v)$ where $u$ is a synonym, antonym and hyponym of $v$, respectively. The authors use $1,023,082$ synonyms pair, $380,873$ antonym pairs and $ 1,545,630$ hyponym pairs. To solve the graded LE task, a distance over the trained embeddings is defined based on the difference of the norms of the embeddings. Since the hyperonym degree for a pair of words is calculated with a combination of the cosine and the Euclidean distances of the embeddings, and the loss function during training also contains the distance of the embeddings, it should not be trained with pairs that appear in the hyperlex dataset. But this point it is not discussed in the paper. There are only two papers that mention this problem with the training: GLEN and POSTLE. Both papers show that if the LEAR model don't see any word of the hyperlex dataset while the model is retraining, the results are worse. Note that \"*don't see any word*\" is a stronger condition that \"*don't see any pair*\".\n",
        "\n",
        "- SDNS: This paper is entirely devoted to graded LE. As in LEAR, the inputs of the model are non-contextual embeddings (dependency-based word embeddings, a generalization of the SKNS embeddings). A NN is defined that is trained with the scores in the train dataset, so it is a pure regression model. To obtain better results, the authors add to the NN some weights to incorporate: sparse distributional features, model called SDNS+SDF; and some  additional information (SDNS+SDF+AD) of some positive/negatives examples that are pairs of hyponyms/synomys (positive) and hyperonyms/antonyms (negative). These examples are again obtained from WordNet and are used in pretrain stage to push the vectors to the correct side of the decision boundary. They use $102,586$ positive pairs and $42,958$ negative ones. The authors do not inform if these pairs intersects with the Hyperlex pairs. These pairs are used only to push the embedding to the correct side by means of a hinge loss function in a preliminary training.  \n",
        "\n",
        "- GLEN: This model is inspired by LEAR. The difference is the loss function. It also uses sets of pairs of synonyms, antonyms and hyponyms in the training obtained from WordNet. But in this case the authors carry out tests with models that have seen during the training from the 0% to 100% of the words in Hyperlex. Moreover, they conducted the same experiments for LEAR. As it can be appreciated in the bellow table, the performance of LEAR without seeing the Hyperlex pairs is quite poor. The 0% setup will be similar to train a model in the lexical split configuration.\n",
        "\n",
        "<center>\n",
        "\n",
        "|Setup |0% |10% |30% |50% |70% |90% |100%|\n",
        "|-|-|-|-|-|-|-|-|\n",
        "|LEAR |.174 |.188 |.273 |.438 |.548 |.634 |.682\n",
        "|GLEN |.481 |.485 |.478 |.474 |.506 |.504 |.520\n",
        "\n",
        "</center>\n",
        "\n",
        "- POSTLE: This model is a post-specialization of the LEAR embeddings. Since many words has not been seen during training in LEAR model, only the words in the constraints pairs, the original embedding and the embedding produces by LEAR serves as training examples in a post-training, that learns how the embeddings have changed for seen words and try to mimic these changes in not seen words. Again, the authors make controlled experiments with the number of words seen by LEAR during its training. They used two types of NN to do the post-specialization: Deep feed-forward network (DFFN) and Adversarial network (ADV). \n",
        "\n"
      ],
      "metadata": {
        "id": "Sg9HP-HNVQh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "</center>\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "FfuuTlJjOI9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- LexSub: Like other previous models, the inputs in LexSub are non-contextual embeddings (Glove embeddings) and by means of a loss function that takes into account hyponyms, antonymns, synomys, the embeddings are retrained. The authors train LEAR and LexSub with: firstly, less lexical resources than in LEAR's paper; secondly, with the same resources. The latter reported Spearman correlation for LEAR should similar than the correlation reported in LEAR's paper, but it doesn't match at all : $0.533$ vs. $0.686$ (may be due to the initial embeddings?). They report the following results:\n",
        "\n",
        "<center>\n",
        "\n",
        "|     | less resorc. | more resor |\n",
        "|----|----|----|\n",
        "|LEAR|0.1384 | 0.5024|\n",
        "|LexSub| 0.2615 | 0.5327|\n",
        "\n",
        "</center>\n",
        " \n",
        "\n",
        "- Hierarchical-fitting: Finally, HF trains again non-contextual embeddings using lexical pairs from WordNet, but with a novel loss function called by the authors *Hierarchical-fitting*.\n",
        "\n",
        "####**How baseline results are obtained**\n",
        "Except for SDNS model, all of the rest models are unsurpervised. But, as we disccuss above, the unsupervised models see the words in the Hyperlex dataset during training in the form of pair constraints. Thus, these models can be compared with our models trained with the datasets under the Hyperlex random configuration, since $100\\%$ of the words in train/val datasets are in the test dataset. GLEN and POSTLE papers also perform controlled experiments in such a way that during their training, it is not seen any word of the Hyperlex dataset, and they also do the same for the LEAR model. In this case, these models are compared with our models trained with the lexical configuration of Hyperlex. Thus, we get the comparable results with the random split from the regarding papers of all models, and the comparable results with the lexical split from GLEN, POSTLE and SDNS papers.\n",
        "\n",
        "####**Our proposal**\n",
        "An elemental form to give a LE grade is to use the probability $p_{hyp}$ for the the `hyp` label calculated by one of our masked/non-masked models: Greater/lower $p_{hyp}$, more/less sure we are that the pair represent an hyponym. But, let's say that one pair gets the probabilities $p_{hyp}=0.8$ and $p_{syn}=0.15$, an another pair obtains $p_{hyp}=0.8$ and $p_{syn}=0.05$. If we have two pairs with similar probabilities to be an hyponym, but one of them with higher probability to be a synonym, as in the above example, this could be an indication that the first pair should be given a higher *hyponym* degree. We can argue the same, but in the reverse sense, with other labels such as $p_{mero}$ or $p_{norel}$. Thus, if the calculated probabilities really represented the certainty about the lexical relations of two words, we could find a linear combination of the probabilities to obtain a grade for LE, that is, we could give some weights $\\beta_{hyp}, \\beta_{syn},\\beta_{norel}, \\dots$, such that knowing the probabilities of a pair $p_{hyp}, p_{syn},p_{norel}, \\dots$, we would get the hyperonym degree as the value, \n",
        "\n",
        " $$grade = \\beta_{hyp}p_{hyp}+\\beta_{syn}p_{syn}+\\beta_{norel}p_{norel} + \\dots$$\n",
        "Similarly, we can also use the logits instead of the probabilities to find the weights $\\beta_i$.\n",
        "\n",
        "One simple way to obtain the weights is to use the validation set to fit a linear regression model where the response variable is the `grade` given by the human annotators and the predictors are the logtis produce by the fine-tuned model (we also tried to use the probabilities insted of logits, and the results were quite similar). The estimated regresion coefficients will be our weights. \n",
        "\n",
        "Our method follows:\n",
        "1. We collapse all `hyp-i` and `r-hpy-i` labels to `hyp` and `r-hyp`, respectively. Thus, we train the model/template with $7$ classes: `ant`, `syn`, `hyp`, `r-hyp`, `cohyp`, `mero` and `no_rel`. In early testing, it seemed that having too many labels and too little data was hurting performance. \n",
        "2. A model/template is trained with the train/val datasets as it is described in the paper. It is run $10$ epochs and the final model is the best one on the val dataset regarding the metrics: For non-masked trained models, the metric is the macro average; and for masked models, it is the cross-entropy loss for the masked tokens. \n",
        "3. Post-processing: Once the model/template is trained, it is calculated the predicted logit of the labels for each pair in the val dataset. Thus, we get a matrix $A = [l_i^j]$, where $l_i^j$ is the logit of the $j$-th label for the $i$-th pair in the val dataset. A linear regression model is fitted to predict the vector of the median human ratings $\\boldsymbol{r}_{val}=(r_1,\\dots,r_n)$. So, we obtain a vector of $7$ weights, $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_7)$, such that,\n",
        "$$\\boldsymbol{r}_{val}\\approx \\boldsymbol{\\beta} \\cdot M$$\n",
        "4. The vector $\\boldsymbol{\\beta}$ is used to predict our rating: Given a pair in the test dataset, it is calculated its vector of logits  $\\boldsymbol{l}=(l_1,\\dots, l_7)$ for the labels; our final rating is $\\boldsymbol{\\beta} \\cdot \\boldsymbol{l}$.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "iN2gDfbUOShQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import ast\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "Ymw0Mwigmlxq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder with all datasets\n",
        "DIR_DATASETS = '/content/LRC-0C0F/datasets/'\n",
        "# folder with the results\n",
        "DIR_RESULTS = '/content/LRC-0C0F/results/'\n",
        "LIST_DIR_RES = [DIR_RESULTS + 'K_H+N/', \n",
        "                DIR_RESULTS + 'BLESS/',\n",
        "                DIR_RESULTS + 'EVALution/', \n",
        "                DIR_RESULTS + 'ROOT09/',\n",
        "                DIR_RESULTS + 'CogALexV/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_lexical_split/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_random_split/'\n",
        "                ]\n"
      ],
      "metadata": {
        "id": "EYI5qYdwpJ4S"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models2abrev = {'bert-large-uncased-whole-word-masking'.lower():'Bert',\n",
        "\t\t\t\t   'roberta-large'.lower():'Roberta',\n",
        "                   'roberta-base'.lower(): 'roberta-base',\n",
        "                   'bert-base-uncased'.lower(): \"bert-base\"\n",
        "\t\t\t\t  }\n",
        "templates2abrev = {\"' <W1> ' <SEP> ' <W2> '\".lower(): 'T1', \n",
        "                  \" <W1> <SEP> <W2> \".lower(): 'T2',\n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>.\".lower() : 'T3',\n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>: <W1> is the <LABEL> of <W2>.\".lower() : 'T4',\n",
        "                  \"' <W1> ' <MASK> ' <W2> '\".lower(): 'TM1', \n",
        "                  \" <W1> <MASK> <W2> \".lower(): 'TM2',                     \n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>: <W1> is the <MASK> of <W2>.\".lower() : 'TM3', \n",
        "                  }\n",
        "reverse_models2abrev = {v:k for k, v in models2abrev.items()}\n",
        "reverse_templates2abrev = {v:k for k, v in templates2abrev.items()}               "
      ],
      "metadata": {
        "id": "ziFTtdyEmm81"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Read txt and csv result files and save results in `dict_res`**\n",
        "`dict_res` is a dictionary of dictionaries with the following structure:\n",
        "- `dict_res[dataset]`: dictionary with all results for a dataset. One from:\n",
        " ```\n",
        " ['bless', 'k&h+n','cogalexv','evalution','root09']\n",
        " ```\n",
        "-`dict_res[dataset][model]`: dictionary with all results for a dataset and model\n",
        "-`dict_res[dataset][model][template]`: dictionary with all results for a dataset, model and template\n",
        "-`dict_res[dataset][model][template]['res']`: list of dataframes from csv files. It should contain $5$ dataframes\n",
        "-`dict_res[dataset][model][template]['report']`:  list of classification reports in txt files. It should contain $5$ reports. The reports are the output dictionaries of the function `classification_report` in the `sklearn.metrics` package, adding: \n",
        "  1. For CogALexV dataset, the weighted f1-score taking into account all labels except the `random` label, if such label exists. Otherwise, the  weighted f1-score is set to $-1$; \n",
        "  2. For Hyperlex dataset, the Spearman correlations with the median of the human annotators and the following calculated scores: the logit of one label; the learned combination of all label logits; the learned combination of all label probs; it is also added these correlations rstricted to nouns and verbs.\n",
        "-`dict_res[dataset][model][template]['mean_report']`: classification report with the means and stds of the $5$ classification reports. \n",
        "\n",
        "Classification reports ( the outputs of `classification_report` function) are also dictionaries of dictionaries:\n",
        " \n",
        " ```\n",
        " { 'label1': {'precison':xxx, 'recall':yyy, 'f1-score':zzz, 'correlation':ccc (correlation only for hyperlex dataset)}\n",
        "   ....\n",
        "   'labeln': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'accuracy': xxx, \n",
        "   'macro avg': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'weighted avg': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'weigthed f1-score not random':xxxxx},\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_nouns_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_nouns_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_verbs_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_verbs_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'coefs_logit':{'l0':lll,...        ,'l7':lll, 'intercept':iii},\n",
        "   'coefs_prob':{'l0':lll,...,'l6':lll, 'intercept':iii},\n",
        "```\n",
        "The `dict_res[dataset][model][template]['mean_report']` classification report is created using the $5$ classification reports of one experiment for a dataset/model/template. It contains the same structure that the above classification report adding the std for precision, recall, f1-score values and the accuracy:\n",
        " ```\n",
        "{ 'label1': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz, 'correlation':meancccc, (correlation only for hyperlex dataset)\n",
        "             'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz, 'std_correlation':ssc}\n",
        "   ....\n",
        "   'labeln': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,'correlation':meancccc,\n",
        "              'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz, 'std_correlation':ssc}\n",
        "   'accuracy': meanxxx, \n",
        "   'std_accuracy':sss\n",
        "   'macro avg': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,\n",
        "                 'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz}\n",
        "   'weighted avg': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,\n",
        "                    'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz},\n",
        "   'weigthed f1-score not random':meanxxx,\n",
        "   'std_weigthed f1-score not random':ssxxx,}\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_nouns_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_verbs_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_nouns_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_verbs_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'coefs_logit':{'l0':meanl,...        ,'l7':meanl, 'intercept':meani,\n",
        "                 'std_l0':meanl,...      ,'std_l7':meanl, 'std_intercept':meani},\n",
        "   'coefs_prob':{'l0':meanl,...   ,'l6':meanl, 'intercept':meani,\n",
        "                 'std_l0':meanl,... ,'std_l6':meanl, 'std_intercept':meani}\n",
        "```"
      ],
      "metadata": {
        "id": "gKjAjyP-6Doj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read txt and csv result files from our scripts**. \n",
        "\n",
        "They are read from a list of folders in variable `LIST_DIR_RES`"
      ],
      "metadata": {
        "id": "7GMv5n05_Qvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weighted_f1_no_random(dict_report):\n",
        "    ''' \n",
        "    Function to calculate the weigthed f1-score by support of all labels\n",
        "    except the random label, if the random label exists. If it not exists\n",
        "    return -1\n",
        "    '''\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'random']\n",
        "    weighted_f1_no_random = -1\n",
        "    if 'random' in dict_report.keys():\n",
        "        total_support_no_random = 0\n",
        "        for k in dict_report:\n",
        "            if k.lower() not in except_list:\n",
        "                weighted_f1_no_random += dict_report[k]['support']*dict_report[k]['f1-score']\n",
        "                total_support_no_random += dict_report[k]['support']\n",
        "        weighted_f1_no_random = weighted_f1_no_random/total_support_no_random\n",
        "    \n",
        "    return weighted_f1_no_random"
      ],
      "metadata": {
        "id": "-bCbRBKOWzPP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_spearman_string_to_dict(spearman_string):\n",
        "    spearman_string = re.sub(\"SpearmanrResult\", \"\", spearman_string)\n",
        "    spearman_string = re.sub(\"correlation=\",\"'correlation':\",spearman_string)\n",
        "    spearman_string = re.sub(\"pvalue=\",\"'pvalue':\",spearman_string)\n",
        "    spearman_string = re.sub(\"\\(\", \"{\", spearman_string)\n",
        "    spearman_string = re.sub(\"\\)\", \"}\", spearman_string)\n",
        "    spearman = ast.literal_eval(spearman_string)\n",
        "    return spearman\n",
        "\n",
        "dict_res = {}\n",
        "LABEL_WEIGHTED_NOT_RANDOM = 'weighted f1-score not random'\n",
        "\n",
        "for dir in LIST_DIR_RES:\n",
        "    list_txt_files = glob.glob(dir + \"*.txt\")\n",
        "    list_csv_files = [re.sub(\"txt$\", \"csv\", f) for f in list_txt_files]\n",
        "    for txt_file, csv_file in zip(list_txt_files, list_csv_files):\n",
        "        # read and process txt results file\n",
        "        with open(txt_file) as ftxt:\n",
        "            # line 1: arguments\n",
        "            #print(txt_file)\n",
        "            par = ast.literal_eval(ftxt.readline())\n",
        "            if par['dataset'].lower() == 'hyperlex':\n",
        "                if 'hyperlex/random' in par['train_file']:\n",
        "                    par['dataset'] = 'hyperlex-random'\n",
        "                else:\n",
        "                    par['dataset'] = 'hyperlex-lexical'\n",
        "            # line 2: date\n",
        "            ftxt.readline()\n",
        "            # line 3: report\n",
        "            report = ast.literal_eval(ftxt.readline().lower())\n",
        "            # line 4: hyperlex correlation by label\n",
        "            spearman_string = ftxt.readline()\n",
        "            if spearman_string != '':\n",
        "                spearman = from_spearman_string_to_dict(spearman_string)\n",
        "                # add spearman correlations to report\n",
        "                for label in list(spearman.keys()):\n",
        "                    report[label]['correlation'] = spearman[label]['correlation']\n",
        "            # line 5: overall hyperlex correlation logits\n",
        "            all_spearman_logit_string = ftxt.readline()\n",
        "            # line 6: checkpoint overall hyperlex correlation logits\n",
        "            all_spearman_check_string = ftxt.readline()\n",
        "            # line 7: overall hyperlex correlation probs\n",
        "            all_spearman_prob_string = ftxt.readline()\n",
        "            if all_spearman_logit_string != '':\n",
        "                all_spearman_logit = from_spearman_string_to_dict(all_spearman_logit_string)\n",
        "                all_spearman_check = from_spearman_string_to_dict(all_spearman_check_string)\n",
        "                if (all_spearman_logit['correlation'] != all_spearman_check['correlation']):\n",
        "                    raise Exception('Error checkpoint: Different Spearman correlations in ' + txt_file)\n",
        "                all_spearman_prob = from_spearman_string_to_dict(all_spearman_prob_string)\n",
        "                report['spearman_logit'] = all_spearman_logit\n",
        "                report['spearman_prob'] = all_spearman_prob\n",
        "            # line 8: hyperlex logit coefs\n",
        "            coefs_logit_string = ftxt.readline()\n",
        "            # line 9: hyperlex probs coefs\n",
        "            coefs_prob_string = ftxt.readline()\n",
        "            if coefs_logit_string != '':\n",
        "                coefs_logit = ast.literal_eval(coefs_logit_string)\n",
        "                report['coefs_logit'] = {'l'+str(i):c for i,c in enumerate(coefs_logit['coefs'])}\n",
        "                report['coefs_logit']['intercept'] = coefs_logit['intercept']\n",
        "                coefs_prob = ast.literal_eval(coefs_prob_string)\n",
        "                report['coefs_prob'] = {'l'+str(i):c for i,c in enumerate(coefs_prob['coefs'])}\n",
        "                report['coefs_prob']['intercept'] = coefs_prob['intercept']\n",
        "        \n",
        "        # read csv results file      \n",
        "        res_csv = pd.read_csv(csv_file, quotechar='\"',keep_default_na=False)\n",
        "        # save csv and txt files to dict_res\n",
        "        a = dict_res.setdefault(par['dataset'].lower(), {})\n",
        "        b = a.setdefault(par['model'].lower(), {})\n",
        "        if \"RandomMask\" in dir:\n",
        "            par['train_templates'][0] = re.sub(\"<mask>\", \"<maskr>\", par['train_templates'][0].lower())\n",
        "        c = b.setdefault(par['train_templates'][0].lower(), {})\n",
        "        d1 = c.setdefault('res',[])\n",
        "        d2 = c.setdefault('report',[])\n",
        "        d1.append(res_csv)\n",
        "        report[LABEL_WEIGHTED_NOT_RANDOM] = calculate_weighted_f1_no_random(report)\n",
        "        if 'spearman_logit' in list(report.keys()):\n",
        "            #distinguir non-masked/masked\n",
        "            col_names = [nc for nc in res_csv.columns]\n",
        "            logit_col_names = filter(lambda x: re.match('.*logit$',x), col_names)\n",
        "            prob_col_names = [re.sub(\"_logit$\", \"\", c) for c in logit_col_names]\n",
        "            X_logit = res_csv.filter(regex=('.*logit$')).to_numpy(copy=True)\n",
        "            X_prob = res_csv[prob_col_names].to_numpy(copy=True)\n",
        "            X_prob = np.delete(X_prob, 5, axis=1)\n",
        "            coefs_l = np.array(coefs_logit['coefs'])\n",
        "            coefs_p = np.array(coefs_prob['coefs'])\n",
        "            grades = res_csv['grade']\n",
        "            if 'hyperlex/random' in par['test_file']:\n",
        "                test_data = pd.read_csv(DIR_DATASETS + 'hyperlex/random/test.tsv',\n",
        "                                        sep = '\\t', header=None)\n",
        "            else:\n",
        "                test_data = pd.read_csv(DIR_DATASETS + 'hyperlex/lexical/test.tsv',\n",
        "                                        sep = '\\t', header=None)\n",
        "\n",
        "            check_spearman_logit = spearmanr(np.dot(X_logit,coefs_l), grades)\n",
        "            check_spearman_prob =  spearmanr(np.dot(X_prob,coefs_p), grades)\n",
        "            # checkpoint: It must be equal (upto thousandths) the Spearman correlations\n",
        "            # calculated in the experiments and the ones calculated in this test\n",
        "            if abs(check_spearman_logit[0] - report['spearman_logit']['correlation']) > 0.001:\n",
        "                raise Exception('Check: Different logit prob from experiments and test: ' + str(check_spearman_logit[0]) + '  ' + str(report['spearman_logit']['correlation']) )\n",
        "            if abs(check_spearman_prob[0] - report['spearman_prob']['correlation']) > 0.001:\n",
        "                raise Exception('Check: Different spearman prob from experiments and test: ' + str(check_spearman_prob[0]) + '  ' + str(report['spearman_prob']['correlation']) )\n",
        "               \n",
        "            nouns_idx = test_data.iloc[:,2] == 'N'\n",
        "            verbs_idx = test_data.iloc[:,2] == 'V'\n",
        "            spearman_nouns_logit = spearmanr(np.dot(X_logit[nouns_idx,:],coefs_l), grades[nouns_idx])\n",
        "            spearman_verbs_logit = spearmanr(np.dot(X_logit[verbs_idx,:],coefs_l), grades[verbs_idx])\n",
        "            spearman_nouns_prob = spearmanr(np.dot(X_prob[nouns_idx,:],coefs_p), grades[nouns_idx])\n",
        "            spearman_verbs_prob = spearmanr(np.dot(X_prob[verbs_idx,:],coefs_p), grades[verbs_idx])\n",
        "            report['spearman_nouns_logit'] = from_spearman_string_to_dict(str(spearman_nouns_logit))\n",
        "            report['spearman_verbs_logit'] = from_spearman_string_to_dict(str(spearman_verbs_logit))\n",
        "            report['spearman_nouns_prob'] = from_spearman_string_to_dict(str(spearman_nouns_prob))\n",
        "            report['spearman_verbs_prob'] = from_spearman_string_to_dict(str(spearman_verbs_prob))\n",
        "        d2.append(report)"
      ],
      "metadata": {
        "id": "U7EIgN6fmpS5"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add the mean report**. \n",
        "\n"
      ],
      "metadata": {
        "id": "IJo1begf_tRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value(one_report, keys):\n",
        "    val = one_report\n",
        "    for k in keys:\n",
        "        val = val[k]\n",
        "    return val\n",
        "\n",
        "def get_values(list_reports, keys):\n",
        "    values = np.array([get_value(one_report, keys) for one_report in list_reports])\n",
        "    return values\n",
        "\n",
        "def calculateMeansRec(list_reports, one_dict, past_keys, exclude_keys):   \n",
        "    for k in list(one_dict.keys()):\n",
        "        if k not in exclude_keys:\n",
        "            copy_past_keys = copy.deepcopy(past_keys)\n",
        "            copy_past_keys.append(k)\n",
        "            if not isinstance(one_dict[k], dict):\n",
        "                values = get_values(list_reports, copy_past_keys)\n",
        "                one_dict[k]= values.mean()\n",
        "                one_dict['std_'+k] = values.std()\n",
        "            else:\n",
        "                calculateMeansRec(list_reports, one_dict[k], copy_past_keys, exclude_keys)\n",
        "\n",
        "def flat_list_reportsRec(list_reports, one_dict, past_keys, exclude_keys):   \n",
        "    for k in list(one_dict.keys()):\n",
        "        if k not in exclude_keys:\n",
        "            copy_past_keys = copy.deepcopy(past_keys)\n",
        "            copy_past_keys.append(k)\n",
        "            if not isinstance(one_dict[k], dict):\n",
        "                values = get_values(list_reports, copy_past_keys)\n",
        "                one_dict[k]= values.tolist()\n",
        "            else:\n",
        "                flat_list_reportsRec(list_reports, one_dict[k], copy_past_keys, exclude_keys)\n",
        "\n",
        "def calculateMeans(list_reports):\n",
        "    '''\n",
        "    Given a list of structurally equal reports, the function\n",
        "    returns a report with the means and stds. A report is a dictionary\n",
        "    whose values are either a dictionary or a real number.\n",
        "    '''\n",
        "    means_report = copy.deepcopy(list_reports[0])\n",
        "    calculateMeansRec(list_reports, means_report, past_keys=[], exclude_keys=['support'])\n",
        "\n",
        "    return means_report\n",
        "\n",
        "def flat_list_reports(list_reports):\n",
        "    '''\n",
        "    Given a list of structurally equal reports, the function returns\n",
        "    a structurally equal report join in a list all values that are real numbers.\n",
        "    A report is a dictionary whose values are either a dictionary or a real number.\n",
        "    '''\n",
        "    flat_report = copy.deepcopy(list_reports[0])\n",
        "    flat_list_reportsRec(list_reports, flat_report, past_keys=[], exclude_keys=['support'])\n",
        "\n",
        "    return flat_report"
      ],
      "metadata": {
        "id": "Nz9IJTVwL5Qh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in dict_res:\n",
        "    print(d.upper())\n",
        "    for m in dict_res[d]:\n",
        "        print(\" -\".join(['Calculating:', d, m]))\n",
        "        for t in dict_res[d][m]:\n",
        "            list_reports = dict_res[d][m][t]['report']\n",
        "            print(\" -\".join([\"    \", t, templates2abrev[t] ]))\n",
        "            dict_res[d][m][t]['mean_report'] = calculateMeans(list_reports)\n",
        "            dict_res[d][m][t]['flat_reports'] = flat_list_reports(list_reports)"
      ],
      "metadata": {
        "id": "BkbbrrSy-Ekb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dictionary with best results**\n",
        "Given the dictionary of results `dict_res`, the following functions create `dict_best`, a python dictionary of dictionaries with the best results for any measure and model/template. It has the structure:\n",
        " - `dict_best[dataset]`: It is a dictionary with the best results for a dataset. One from:\n",
        " ```\n",
        " ['bless', 'k&h+n','cogalexv','evalution','root09']\n",
        " ```\n",
        " - `dict_best[dataset]`: It is a dictionary with the best results for a dataset:\n",
        " \n",
        " \n",
        " ```\n",
        " {label1: { 'precision': {'best_val': xxxx,\n",
        "                          'best_model':mmmm,\n",
        "                          'best_template':tttt,\n",
        "                          'p-vals': {model: { template_1: pppp,\n",
        "                                              ....\n",
        "                                              template_k:pppp}}},\n",
        "              'recall': {'best_val': xxxx,\n",
        "                         'best_model':mmmm,\n",
        "                         'best_template':tttt,\n",
        "                         'p-vals': {model: { template_1: pppp,\n",
        "                                             ....\n",
        "                                             template_k:pppp}}},\n",
        "              'f1-score': {'best_val': xxxx,\n",
        "                           'best_model':mmmm,\n",
        "                           'best_template':tttt,\n",
        "                           'p-vals': {model: { template_1: pppp,\n",
        "                                                ....\n",
        "                                                template_k:pppp}}},\n",
        "           //only for hyperlex dataset\n",
        "           'correlation': {'best_val': xxxx,\n",
        "                           'best_model':mmmm,\n",
        "                           'best_template':tttt,\n",
        "                           'p-vals': {model: { template_1: pppp,\n",
        "                                                ....\n",
        "                                                template_k:pppp}}}}}                                                \n",
        "  ....\n",
        "  labeln: {same label 1 ...}\n",
        "  'accuracy': {'best_val': xxxx,\n",
        "               'best_model':mmmm,\n",
        "               'best_template':tttt,\n",
        "               'p-vals': {model: { template_1: pppp,\n",
        "                                    ....\n",
        "                                    template_k:pppp}}},\n",
        "  'macro avg':{similar to label 1...},\n",
        "  'weighted avg' :{similar to label 1...},\n",
        "  'weigthed f1-score not random': {similar to accuracy...},\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {...},\n",
        "   'spearman_prob': {...},\n",
        "   'spearman_nouns_logit': {...},\n",
        "   'spearman_verbs_logit': {...},\n",
        "   'spearman_nouns_prob': {...},\n",
        "   'spearman_verbs_prob': {...},\n",
        "   'coefs_logit':{...},\n",
        "   'coefs_prob':{...}\n",
        " }\n",
        "```                                      \n",
        "\n",
        "\n",
        "For example, \n",
        "- `dict_best['k&h+n']['ant']['precision']['best_val']` contains the best **mean** precision value among all models and templates for the antonyms label, \n",
        "- `dict_best['k&h+n']['ant']['precision']['best_model']` is the model for which the best precision has been obtained,\n",
        "- and  `dict_best['k&h+n']['ant']['precision']['best_template']` is the template for which the best precision has been obtained.\n",
        "\n",
        "The `dict_best` dictionary also contains the p-values comparing the mean of the best model/template with any other mean for a model/template. It is used a Welch's test to check if there is statistical evidence that the means are different. So, for instance the value in\n",
        "```\n",
        "dict_best['k&h+n']['ant']['precision']['p-vals']['roberta-large'][' <w1> <sep> <w2> ']\n",
        "```\n",
        "contains the p-value to check if the mean of the $5$ precision values of our experiments for antonyms in K&H+N dataset trained with the RoBERTa large model and the template ` <W1> <SEP> <W2> ` is different from the mean of the $5$ precision values obtained with the model/template that has got the best mean precision value."
      ],
      "metadata": {
        "id": "Qmxgt2_auQuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty(one_dict, empty_best_dict, suffix=''):\n",
        "    for label in one_dict:\n",
        "        empty_best_dict[label+suffix]={}\n",
        "        if isinstance(one_dict[label], dict):\n",
        "            for sublabel in one_dict[label]:\n",
        "                if sublabel != 'support':\n",
        "                    empty_best_dict[label+suffix][sublabel] = {}\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_val'] = -1.0\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_model'] = ''\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_template'] = ''\n",
        "        else:\n",
        "            empty_best_dict[label+suffix]['best_val'] = -1.0\n",
        "            empty_best_dict[label+suffix]['best_model'] = ''\n",
        "            empty_best_dict[label+suffix]['best_template'] = ''\n",
        "\n",
        "def create_empty_best_dict(dict_res, dataset):\n",
        "    m0 = list(dict_res[dataset].keys())[0]\n",
        "    t0 = list(dict_res[dataset][m0].keys())[0]\n",
        "    empty_best_dict = {}\n",
        "    one_dict = dict_res[dataset][m0][t0]['report'][0]\n",
        "    create_empty(one_dict, empty_best_dict)\n",
        "    return empty_best_dict\n",
        "\n",
        "def create (dict_mean_rep, m, t, best_dict_dataset):\n",
        "    for label in dict_mean_rep:\n",
        "        if label in list(best_dict_dataset.keys()):\n",
        "            if isinstance(dict_mean_rep[label], dict):\n",
        "                for sublabel in dict_mean_rep[label]:\n",
        "                    if sublabel in list(best_dict_dataset[label].keys()):\n",
        "                        if dict_mean_rep[label][sublabel] > best_dict_dataset[label][sublabel]['best_val']:\n",
        "                            best_dict_dataset[label][sublabel]['best_val'] = dict_mean_rep[label][sublabel]\n",
        "                            best_dict_dataset[label][sublabel]['best_model'] = m\n",
        "                            best_dict_dataset[label][sublabel]['best_template'] = t\n",
        "            else:\n",
        "                if dict_mean_rep[label] >= best_dict_dataset[label]['best_val']:\n",
        "                    best_dict_dataset[label]['best_val'] = dict_mean_rep[label]\n",
        "                    best_dict_dataset[label]['best_model'] = m\n",
        "                    best_dict_dataset[label]['best_template'] = t\n",
        "\n",
        "def create_best_dict_dataset(dict_res, d):\n",
        "    '''\n",
        "    For a dataset d, create the best dictionary without p-values\n",
        "    '''\n",
        "    best_dict_dataset = create_empty_best_dict(dict_res, d)\n",
        "    for m in dict_res[d]:\n",
        "        for t in dict_res[d][m]:\n",
        "            create(dict_res[d][m][t]['mean_report'], m, t, best_dict_dataset)\n",
        "    return best_dict_dataset\n",
        "\n",
        "def create_pvals_dataset(best_dict, dict_res, dataset, label, sublabel=None):\n",
        "    '''\n",
        "    Given a best_dict without p-values and dictionary of results, label and sublabel\n",
        "    create a dictionary containing the p-values for label and sublabel.\n",
        "    '''\n",
        "    dict_pvals = {}\n",
        "    if sublabel is not None:\n",
        "        mref = best_dict[dataset][label][sublabel]['best_model']\n",
        "        tref = best_dict[dataset][label][sublabel]['best_template']\n",
        "        list_reports = dict_res[dataset][mref][tref]['report']\n",
        "        list_ref_max = [rep[label][sublabel] for rep in list_reports]\n",
        "    else:\n",
        "        mref = best_dict[dataset][label]['best_model']\n",
        "        tref = best_dict[dataset][label]['best_template']\n",
        "        list_reports = dict_res[dataset][mref][tref]['report']\n",
        "        list_ref_max = [rep[label] for rep in list_reports]\n",
        "    \n",
        "    for m in dict_res[dataset]:\n",
        "        for t in dict_res[dataset][m]:\n",
        "            if not (m == mref and t == tref):\n",
        "                if sublabel is not None:\n",
        "                    list_reports_others = dict_res[dataset][m][t]['report']\n",
        "                    list_vals = [rep[label][sublabel] for rep in list_reports_others]\n",
        "                else:\n",
        "                    list_reports_others = dict_res[dataset][m][t]['report']\n",
        "                    list_vals = [rep[label] for rep in list_reports_others]\n",
        "                \n",
        "                pval = stats.ttest_ind(list_ref_max, list_vals, equal_var=False)[1]\n",
        "                dict_1 = dict_pvals.setdefault(m,{})\n",
        "                dict_1[t] = pval\n",
        "    return dict_pvals\n"
      ],
      "metadata": {
        "id": "BkqDiBWlbwau"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dict_best(dict_res):\n",
        "    ''' \n",
        "    Given a dictionary of results, it returns a dictionary of best results\n",
        "    '''\n",
        "    # create best dictionary without p-vals\n",
        "    dict_best_1={}\n",
        "    for d in dict_res:\n",
        "        dict_best_1[d] = create_best_dict_dataset(dict_res, d)\n",
        "    \n",
        "    # create best dictionary adding p-vals\n",
        "    dict_best = copy.deepcopy(dict_best_1)    \n",
        "    for d in dict_best_1:\n",
        "        for label in dict_best_1[d]:\n",
        "            if 'best_val' in list(dict_best_1[d][label]):\n",
        "                dict_best[d][label]['p-vals'] = create_pvals_dataset(dict_best_1, dict_res, d, label) \n",
        "            else:\n",
        "                for sublabel in dict_best_1[d][label]:\n",
        "                    dict_best[d][label][sublabel]['p-vals'] = create_pvals_dataset(dict_best_1, dict_res, d, label,sublabel)\n",
        "    return dict_best\n",
        "    \n",
        "dict_best = create_dict_best(dict_res)"
      ],
      "metadata": {
        "id": "DhD5FCWPf_lx"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CogALexV**\n",
        "To process the CogALexV results. Note that the results for templates `T1`-`T4` correspond to the non-masked models, and templates `TM1`-`TM3`for the masked ones. See the dictionary `templates2abrev` in a cell above."
      ],
      "metadata": {
        "id": "74KSBwjbExz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframe_results_cogalexv(dict_res, dataset):\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'std_'+LABEL_WEIGHTED_NOT_RANDOM, 'random']\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]}\n",
        "    col_tuples=[]\n",
        "    create_tuples = True\n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            mean_report = dict_res[dataset.lower()][m][t]['mean_report']\n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "            for k in mean_report: \n",
        "                if k not in except_list:\n",
        "                    l = dict_df.setdefault(k, [])\n",
        "                    if k != LABEL_WEIGHTED_NOT_RANDOM:\n",
        "                        l.append(mean_report[k]['f1-score'])\n",
        "                        if create_tuples:\n",
        "                            col_tuples.append((k,'f1-score'))\n",
        "                    else:\n",
        "                        l.append(mean_report[k])\n",
        "            create_tuples = False       \n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    col_tuples.append(('all','weighted f1-score'))\n",
        "    col_index = pd.MultiIndex.from_tuples(col_tuples)\n",
        "    res_df = res_df.iloc[:, 3:].copy()\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = col_index\n",
        "    return res_df\n",
        "\n",
        "def get_dataframe_results_max_min_cogalexv(dict_res, dataset):\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'std_'+LABEL_WEIGHTED_NOT_RANDOM, 'random']\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]}\n",
        "    col_tuples=[]\n",
        "    create_tuples_labels = True\n",
        "    create_tuples_all = True\n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            flat_reports = dict_res[dataset.lower()][m][t]['flat_reports']\n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "            for k in flat_reports: \n",
        "                if k not in except_list:\n",
        "                    l_max = dict_df.setdefault('max-'+ k, [])\n",
        "                    l_min = dict_df.setdefault('min-'+ k, [])\n",
        "                    l_std = dict_df.setdefault('std-'+ k, [])\n",
        "                    if k != LABEL_WEIGHTED_NOT_RANDOM:\n",
        "                        l_max.append(np.array(flat_reports[k]['f1-score']).max())\n",
        "                        l_min.append(np.array(flat_reports[k]['f1-score']).min())\n",
        "                        l_std.append(np.array(flat_reports[k]['f1-score']).std())\n",
        "                        if create_tuples_labels:\n",
        "                            col_tuples.append((k,'max-f1-score'))\n",
        "                            col_tuples.append((k,'min-f1-score'))\n",
        "                            col_tuples.append((k,'std-f1-score'))\n",
        "                    else:\n",
        "                        l_max.append(np.array(flat_reports[k]).max())\n",
        "                        l_min.append(np.array(flat_reports[k]).min())\n",
        "                        l_std.append(np.array(flat_reports[k]).std())\n",
        "                        if create_tuples_all:\n",
        "                            col_tuples.append((k,'max-all'))\n",
        "                            col_tuples.append((k,'min-all'))\n",
        "                            col_tuples.append((k,'std-all'))\n",
        "            create_tuples_labels = False   \n",
        "            create_tuples_all = False  \n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    #col_tuples.append(('all','weighted f1-score'))\n",
        "    col_index = pd.MultiIndex.from_tuples(col_tuples)\n",
        "    res_df = res_df.iloc[:, 3:].copy()\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = col_index\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "moVbJ7meDFOH"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cogalexv_final = get_dataframe_results_cogalexv(dict_res, 'CogALexV')\n",
        "df_cogalexv_final = df_cogalexv_final.sort_index(level=['model','template'])"
      ],
      "metadata": {
        "id": "bcUBUbN10gve"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_SOTAS_COGALEXV = DIR_RESULTS + 'sotas_results_literature/cogalexv_Sotas_RC.txt'\n",
        "df_sotas_cogalexv = pd.read_csv(FILE_SOTAS_COGALEXV, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "multi_row_tuples = list(zip(['Sota']*df_sotas_cogalexv.shape[0], list(df_sotas_cogalexv.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "df_sotas_cogalexv.index=multi_row\n",
        "df_res_sotas_cogalexv = pd.concat([df_cogalexv_final,df_sotas_cogalexv])\n",
        "\n",
        "#change column order\n",
        "order1 = {'ant':0,'hyper':1,'part_of':2, 'syn':3, 'all':4}\n",
        "multi_col_list = list(df_res_sotas_cogalexv.columns)\n",
        "multi_col_list.sort(key=lambda x: order1[x[0].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "df_res_sotas_cogalexv = pd.DataFrame(df_res_sotas_cogalexv, columns=multi_col)\n",
        "df_res_sotas_cogalexv"
      ],
      "metadata": {
        "id": "k6REijakbq8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4a055f2b-abcb-4e9d-d27c-b5eb2df7c193"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            ant     hyper   part_of       syn  \\\n",
              "                       f1-score  f1-score  f1-score  f1-score   \n",
              "model        template                                           \n",
              "Bert         T1        0.770474  0.680452  0.715095  0.563975   \n",
              "             T2        0.768676  0.675201  0.727876  0.528010   \n",
              "             T3        0.788689  0.681299  0.735502  0.565756   \n",
              "             T4        0.119323  0.044062  0.077908  0.000000   \n",
              "             TM1       0.798389  0.682221  0.745577  0.584719   \n",
              "             TM2       0.781551  0.687806  0.742477  0.560105   \n",
              "             TM3       0.778578  0.682159  0.742217  0.562916   \n",
              "Roberta      T1        0.872557  0.703143  0.752050  0.603664   \n",
              "             T2        0.863005  0.681854  0.744814  0.583635   \n",
              "             T3        0.883666  0.718214  0.784419  0.628822   \n",
              "             T4        0.236519  0.003704  0.164753  0.084852   \n",
              "             TM1       0.879658  0.708541  0.772999  0.598834   \n",
              "             TM2       0.870704  0.723055  0.787251  0.621032   \n",
              "             TM3       0.870720  0.717585  0.787410  0.615927   \n",
              "bert-base    T1        0.553600  0.590749  0.657085  0.361485   \n",
              "             T2        0.528956  0.544102  0.610356  0.277698   \n",
              "             T3        0.565381  0.605407  0.683618  0.374989   \n",
              "             T4        0.081221  0.000000  0.101444  0.006059   \n",
              "             TM1       0.644556  0.625010  0.707385  0.431301   \n",
              "             TM2       0.570093  0.621784  0.685384  0.393129   \n",
              "             TM3       0.635968  0.648195  0.720730  0.430058   \n",
              "roberta-base T1        0.805550  0.677129  0.732487  0.569636   \n",
              "             T2        0.782981  0.651829  0.692575  0.536492   \n",
              "             T3        0.819614  0.675679  0.731475  0.576918   \n",
              "             T4        0.026575  0.000000  0.102297  0.092189   \n",
              "             TM1       0.809081  0.678229  0.743181  0.560552   \n",
              "             TM2       0.801448  0.673224  0.742456  0.555826   \n",
              "             TM3       0.814963  0.679439  0.729843  0.560999   \n",
              "Sota         LexNET    0.425000  0.526000  0.493000  0.297000   \n",
              "             SphereRE  0.479000  0.538000  0.539000  0.286000   \n",
              "             KEML      0.492000  0.547000  0.652000  0.292000   \n",
              "             RelBert   0.794000  0.616000  0.702000  0.505000   \n",
              "\n",
              "                                    all  \n",
              "                      weighted f1-score  \n",
              "model        template                    \n",
              "Bert         T1                0.690274  \n",
              "             T2                0.683411  \n",
              "             T3                0.700158  \n",
              "             T4                0.063480  \n",
              "             TM1               0.708948  \n",
              "             TM2               0.700283  \n",
              "             TM3               0.698097  \n",
              "Roberta      T1                0.742749  \n",
              "             T2                0.727845  \n",
              "             T3                0.761832  \n",
              "             T4                0.118573  \n",
              "             TM1               0.749557  \n",
              "             TM2               0.758490  \n",
              "             TM3               0.755786  \n",
              "bert-base    T1                0.546293  \n",
              "             T2                0.498959  \n",
              "             T3                0.562078  \n",
              "             T4                0.043619  \n",
              "             TM1               0.607497  \n",
              "             TM2               0.572578  \n",
              "             TM3               0.614543  \n",
              "roberta-base T1                0.704082  \n",
              "             T2                0.675341  \n",
              "             T3                0.709073  \n",
              "             T4                0.044251  \n",
              "             TM1               0.705707  \n",
              "             TM2               0.700768  \n",
              "             TM3               0.705455  \n",
              "Sota         LexNET            0.445000  \n",
              "             SphereRE          0.471000  \n",
              "             KEML              0.500000  \n",
              "             RelBert           0.664000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9071c81-f98f-4d79-b7b3-ade9cbe59de6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ant</th>\n",
              "      <th>hyper</th>\n",
              "      <th>part_of</th>\n",
              "      <th>syn</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>weighted f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.770474</td>\n",
              "      <td>0.680452</td>\n",
              "      <td>0.715095</td>\n",
              "      <td>0.563975</td>\n",
              "      <td>0.690274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.768676</td>\n",
              "      <td>0.675201</td>\n",
              "      <td>0.727876</td>\n",
              "      <td>0.528010</td>\n",
              "      <td>0.683411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.788689</td>\n",
              "      <td>0.681299</td>\n",
              "      <td>0.735502</td>\n",
              "      <td>0.565756</td>\n",
              "      <td>0.700158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.119323</td>\n",
              "      <td>0.044062</td>\n",
              "      <td>0.077908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.798389</td>\n",
              "      <td>0.682221</td>\n",
              "      <td>0.745577</td>\n",
              "      <td>0.584719</td>\n",
              "      <td>0.708948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.781551</td>\n",
              "      <td>0.687806</td>\n",
              "      <td>0.742477</td>\n",
              "      <td>0.560105</td>\n",
              "      <td>0.700283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.778578</td>\n",
              "      <td>0.682159</td>\n",
              "      <td>0.742217</td>\n",
              "      <td>0.562916</td>\n",
              "      <td>0.698097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.872557</td>\n",
              "      <td>0.703143</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>0.603664</td>\n",
              "      <td>0.742749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.863005</td>\n",
              "      <td>0.681854</td>\n",
              "      <td>0.744814</td>\n",
              "      <td>0.583635</td>\n",
              "      <td>0.727845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.883666</td>\n",
              "      <td>0.718214</td>\n",
              "      <td>0.784419</td>\n",
              "      <td>0.628822</td>\n",
              "      <td>0.761832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.236519</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>0.164753</td>\n",
              "      <td>0.084852</td>\n",
              "      <td>0.118573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.879658</td>\n",
              "      <td>0.708541</td>\n",
              "      <td>0.772999</td>\n",
              "      <td>0.598834</td>\n",
              "      <td>0.749557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.870704</td>\n",
              "      <td>0.723055</td>\n",
              "      <td>0.787251</td>\n",
              "      <td>0.621032</td>\n",
              "      <td>0.758490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.870720</td>\n",
              "      <td>0.717585</td>\n",
              "      <td>0.787410</td>\n",
              "      <td>0.615927</td>\n",
              "      <td>0.755786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.553600</td>\n",
              "      <td>0.590749</td>\n",
              "      <td>0.657085</td>\n",
              "      <td>0.361485</td>\n",
              "      <td>0.546293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.528956</td>\n",
              "      <td>0.544102</td>\n",
              "      <td>0.610356</td>\n",
              "      <td>0.277698</td>\n",
              "      <td>0.498959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.565381</td>\n",
              "      <td>0.605407</td>\n",
              "      <td>0.683618</td>\n",
              "      <td>0.374989</td>\n",
              "      <td>0.562078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.081221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.101444</td>\n",
              "      <td>0.006059</td>\n",
              "      <td>0.043619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.644556</td>\n",
              "      <td>0.625010</td>\n",
              "      <td>0.707385</td>\n",
              "      <td>0.431301</td>\n",
              "      <td>0.607497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.570093</td>\n",
              "      <td>0.621784</td>\n",
              "      <td>0.685384</td>\n",
              "      <td>0.393129</td>\n",
              "      <td>0.572578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.635968</td>\n",
              "      <td>0.648195</td>\n",
              "      <td>0.720730</td>\n",
              "      <td>0.430058</td>\n",
              "      <td>0.614543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.805550</td>\n",
              "      <td>0.677129</td>\n",
              "      <td>0.732487</td>\n",
              "      <td>0.569636</td>\n",
              "      <td>0.704082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.782981</td>\n",
              "      <td>0.651829</td>\n",
              "      <td>0.692575</td>\n",
              "      <td>0.536492</td>\n",
              "      <td>0.675341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.819614</td>\n",
              "      <td>0.675679</td>\n",
              "      <td>0.731475</td>\n",
              "      <td>0.576918</td>\n",
              "      <td>0.709073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.026575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102297</td>\n",
              "      <td>0.092189</td>\n",
              "      <td>0.044251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.809081</td>\n",
              "      <td>0.678229</td>\n",
              "      <td>0.743181</td>\n",
              "      <td>0.560552</td>\n",
              "      <td>0.705707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.801448</td>\n",
              "      <td>0.673224</td>\n",
              "      <td>0.742456</td>\n",
              "      <td>0.555826</td>\n",
              "      <td>0.700768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.814963</td>\n",
              "      <td>0.679439</td>\n",
              "      <td>0.729843</td>\n",
              "      <td>0.560999</td>\n",
              "      <td>0.705455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Sota</th>\n",
              "      <th>LexNET</th>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.526000</td>\n",
              "      <td>0.493000</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SphereRE</th>\n",
              "      <td>0.479000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.539000</td>\n",
              "      <td>0.286000</td>\n",
              "      <td>0.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KEML</th>\n",
              "      <td>0.492000</td>\n",
              "      <td>0.547000</td>\n",
              "      <td>0.652000</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RelBert</th>\n",
              "      <td>0.794000</td>\n",
              "      <td>0.616000</td>\n",
              "      <td>0.702000</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9071c81-f98f-4d79-b7b3-ade9cbe59de6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9071c81-f98f-4d79-b7b3-ade9cbe59de6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9071c81-f98f-4d79-b7b3-ade9cbe59de6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cogalexv_max_min_final = get_dataframe_results_max_min_cogalexv(dict_res, 'CogALexV')\n",
        "df_cogalexv_max_min_final = df_cogalexv_max_min_final.sort_index(level=['model','template'])\n",
        "df_cogalexv_max_min_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "g5mp3eKfVGC_",
        "outputId": "507dab85-3a23-485b-9290-294207fa6f3e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               ant                                  hyper  \\\n",
              "                      max-f1-score min-f1-score std-f1-score max-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.788540     0.754986     0.011111     0.693042   \n",
              "             T2           0.784203     0.752044     0.012690     0.697095   \n",
              "             T3           0.797122     0.776406     0.007539     0.687879   \n",
              "             T4           0.220661     0.000000     0.099786     0.138219   \n",
              "             TM1          0.805556     0.789250     0.006764     0.694891   \n",
              "             TM2          0.797768     0.765537     0.011892     0.696296   \n",
              "             TM3          0.812589     0.763948     0.017595     0.691771   \n",
              "Roberta      T1           0.890469     0.836524     0.019903     0.735724   \n",
              "             T2           0.905109     0.830137     0.024201     0.725067   \n",
              "             T3           0.896067     0.874644     0.007488     0.734211   \n",
              "             T4           0.523985     0.000000     0.181011     0.018519   \n",
              "             TM1          0.888252     0.863572     0.008898     0.714286   \n",
              "             TM2          0.886657     0.862119     0.008853     0.732432   \n",
              "             TM3          0.879536     0.861671     0.007292     0.724965   \n",
              "bert-base    T1           0.563415     0.543478     0.006968     0.598854   \n",
              "             T2           0.550336     0.508906     0.015003     0.560647   \n",
              "             T3           0.580132     0.539642     0.015095     0.615385   \n",
              "             T4           0.189547     0.000000     0.077721     0.000000   \n",
              "             TM1          0.655172     0.631136     0.008916     0.634538   \n",
              "             TM2          0.593123     0.550512     0.016477     0.631436   \n",
              "             TM3          0.653061     0.615836     0.012420     0.656381   \n",
              "roberta-base T1           0.821683     0.789400     0.011938     0.703601   \n",
              "             T2           0.802837     0.761134     0.013577     0.673130   \n",
              "             T3           0.832386     0.810512     0.008108     0.701513   \n",
              "             T4           0.080925     0.000000     0.033812     0.000000   \n",
              "             TM1          0.826896     0.792614     0.012819     0.690667   \n",
              "             TM2          0.813097     0.787535     0.008702     0.686192   \n",
              "             TM3          0.827586     0.803443     0.009157     0.698727   \n",
              "\n",
              "                                                     part_of               \\\n",
              "                      min-f1-score std-f1-score max-f1-score min-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.662953     0.011157     0.744292     0.676724   \n",
              "             T2           0.642336     0.021950     0.749474     0.702820   \n",
              "             T3           0.675250     0.005329     0.740566     0.729670   \n",
              "             T4           0.000000     0.056808     0.289157     0.000000   \n",
              "             TM1          0.670537     0.008846     0.758772     0.726437   \n",
              "             TM2          0.671576     0.008973     0.748837     0.740088   \n",
              "             TM3          0.668648     0.007784     0.766520     0.724221   \n",
              "Roberta      T1           0.662050     0.024285     0.764835     0.730361   \n",
              "             T2           0.643836     0.030330     0.777778     0.718182   \n",
              "             T3           0.699454     0.013026     0.798144     0.769231   \n",
              "             T4           0.000000     0.007407     0.321878     0.100045   \n",
              "             TM1          0.697274     0.006072     0.789238     0.749415   \n",
              "             TM2          0.713092     0.007152     0.828054     0.757991   \n",
              "             TM3          0.709497     0.005322     0.823799     0.771930   \n",
              "bert-base    T1           0.574386     0.008526     0.684564     0.634573   \n",
              "             T2           0.522667     0.012943     0.628450     0.586207   \n",
              "             T3           0.598930     0.005760     0.711111     0.648188   \n",
              "             T4           0.000000     0.000000     0.138322     0.031250   \n",
              "             TM1          0.620321     0.005249     0.716895     0.694690   \n",
              "             TM2          0.616231     0.006214     0.692810     0.672566   \n",
              "             TM3          0.642150     0.006246     0.734066     0.703540   \n",
              "roberta-base T1           0.651748     0.016891     0.746137     0.707158   \n",
              "             T2           0.623881     0.018566     0.723769     0.669663   \n",
              "             T3           0.657895     0.014903     0.745098     0.720358   \n",
              "             T4           0.000000     0.000000     0.173318     0.013363   \n",
              "             TM1          0.658263     0.012111     0.751131     0.738197   \n",
              "             TM2          0.660140     0.009889     0.747204     0.733485   \n",
              "             TM3          0.658192     0.013215     0.745098     0.714588   \n",
              "\n",
              "                                            syn                            \\\n",
              "                      std-f1-score max-f1-score min-f1-score std-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.022261     0.597895     0.526807     0.023576   \n",
              "             T2           0.016871     0.545024     0.502392     0.016426   \n",
              "             T3           0.003565     0.603306     0.505593     0.033071   \n",
              "             T4           0.112552     0.000000     0.000000     0.000000   \n",
              "             TM1          0.011805     0.616034     0.569593     0.016333   \n",
              "             TM2          0.003289     0.586777     0.541761     0.015535   \n",
              "             TM3          0.014963     0.587196     0.552995     0.012320   \n",
              "Roberta      T1           0.012531     0.622407     0.579775     0.015731   \n",
              "             T2           0.020200     0.600877     0.546535     0.021163   \n",
              "             T3           0.009349     0.641822     0.600000     0.014810   \n",
              "             T4           0.080385     0.222222     0.008299     0.090397   \n",
              "             TM1          0.015826     0.646091     0.574380     0.025951   \n",
              "             TM2          0.023326     0.639004     0.605932     0.013809   \n",
              "             TM3          0.018926     0.639130     0.596950     0.015386   \n",
              "bert-base    T1           0.018373     0.419048     0.309859     0.040520   \n",
              "             T2           0.016889     0.310273     0.250000     0.022780   \n",
              "             T3           0.023506     0.402464     0.343249     0.023584   \n",
              "             T4           0.036721     0.022388     0.000000     0.008720   \n",
              "             TM1          0.009327     0.446352     0.410596     0.014221   \n",
              "             TM2          0.008676     0.409186     0.370203     0.013042   \n",
              "             TM3          0.010298     0.437500     0.424116     0.004361   \n",
              "roberta-base T1           0.013355     0.597849     0.545842     0.017984   \n",
              "             T2           0.023624     0.561404     0.513131     0.015437   \n",
              "             T3           0.010589     0.605578     0.564756     0.015188   \n",
              "             T4           0.051620     0.181244     0.000000     0.059557   \n",
              "             TM1          0.004568     0.593220     0.528455     0.026487   \n",
              "             TM2          0.004731     0.570815     0.543568     0.009878   \n",
              "             TM3          0.011037     0.581443     0.533613     0.016158   \n",
              "\n",
              "                      weighted f1-score not random                      \n",
              "                                           max-all   min-all   std-all  \n",
              "model        template                                                   \n",
              "Bert         T1                           0.704759  0.677656  0.008870  \n",
              "             T2                           0.700384  0.662954  0.014347  \n",
              "             T3                           0.712255  0.687849  0.008922  \n",
              "             T4                           0.117960 -0.000833  0.047692  \n",
              "             TM1                          0.717220  0.702957  0.004713  \n",
              "             TM2                          0.706387  0.692681  0.004527  \n",
              "             TM3                          0.714033  0.687597  0.009353  \n",
              "Roberta      T1                           0.760516  0.712373  0.017246  \n",
              "             T2                           0.763227  0.701886  0.021401  \n",
              "             T3                           0.771118  0.754640  0.005476  \n",
              "             T4                           0.259748  0.019451  0.080573  \n",
              "             TM1                          0.764715  0.742481  0.007857  \n",
              "             TM2                          0.766471  0.750697  0.005891  \n",
              "             TM3                          0.766335  0.746589  0.007618  \n",
              "bert-base    T1                           0.563546  0.530574  0.011261  \n",
              "             T2                           0.515928  0.476910  0.015441  \n",
              "             T3                           0.575422  0.551220  0.007997  \n",
              "             T4                           0.072364  0.018762  0.020278  \n",
              "             TM1                          0.616324  0.596140  0.006960  \n",
              "             TM2                          0.577598  0.564060  0.004835  \n",
              "             TM3                          0.620461  0.607989  0.005315  \n",
              "roberta-base T1                           0.723223  0.698204  0.009664  \n",
              "             T2                           0.694423  0.662422  0.015018  \n",
              "             T3                           0.721743  0.700958  0.007519  \n",
              "             T4                           0.069518  0.017802  0.020213  \n",
              "             TM1                          0.722879  0.690554  0.011276  \n",
              "             TM2                          0.710174  0.693918  0.005279  \n",
              "             TM3                          0.712762  0.692731  0.006741  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e81c2ee6-33d7-4706-925f-80b1ba39a451\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">ant</th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyper</th>\n",
              "      <th colspan=\"3\" halign=\"left\">part_of</th>\n",
              "      <th colspan=\"3\" halign=\"left\">syn</th>\n",
              "      <th colspan=\"3\" halign=\"left\">weighted f1-score not random</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-all</th>\n",
              "      <th>min-all</th>\n",
              "      <th>std-all</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.788540</td>\n",
              "      <td>0.754986</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.693042</td>\n",
              "      <td>0.662953</td>\n",
              "      <td>0.011157</td>\n",
              "      <td>0.744292</td>\n",
              "      <td>0.676724</td>\n",
              "      <td>0.022261</td>\n",
              "      <td>0.597895</td>\n",
              "      <td>0.526807</td>\n",
              "      <td>0.023576</td>\n",
              "      <td>0.704759</td>\n",
              "      <td>0.677656</td>\n",
              "      <td>0.008870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.784203</td>\n",
              "      <td>0.752044</td>\n",
              "      <td>0.012690</td>\n",
              "      <td>0.697095</td>\n",
              "      <td>0.642336</td>\n",
              "      <td>0.021950</td>\n",
              "      <td>0.749474</td>\n",
              "      <td>0.702820</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.545024</td>\n",
              "      <td>0.502392</td>\n",
              "      <td>0.016426</td>\n",
              "      <td>0.700384</td>\n",
              "      <td>0.662954</td>\n",
              "      <td>0.014347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.797122</td>\n",
              "      <td>0.776406</td>\n",
              "      <td>0.007539</td>\n",
              "      <td>0.687879</td>\n",
              "      <td>0.675250</td>\n",
              "      <td>0.005329</td>\n",
              "      <td>0.740566</td>\n",
              "      <td>0.729670</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.505593</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>0.712255</td>\n",
              "      <td>0.687849</td>\n",
              "      <td>0.008922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.220661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099786</td>\n",
              "      <td>0.138219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056808</td>\n",
              "      <td>0.289157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117960</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>0.047692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.789250</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.694891</td>\n",
              "      <td>0.670537</td>\n",
              "      <td>0.008846</td>\n",
              "      <td>0.758772</td>\n",
              "      <td>0.726437</td>\n",
              "      <td>0.011805</td>\n",
              "      <td>0.616034</td>\n",
              "      <td>0.569593</td>\n",
              "      <td>0.016333</td>\n",
              "      <td>0.717220</td>\n",
              "      <td>0.702957</td>\n",
              "      <td>0.004713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.797768</td>\n",
              "      <td>0.765537</td>\n",
              "      <td>0.011892</td>\n",
              "      <td>0.696296</td>\n",
              "      <td>0.671576</td>\n",
              "      <td>0.008973</td>\n",
              "      <td>0.748837</td>\n",
              "      <td>0.740088</td>\n",
              "      <td>0.003289</td>\n",
              "      <td>0.586777</td>\n",
              "      <td>0.541761</td>\n",
              "      <td>0.015535</td>\n",
              "      <td>0.706387</td>\n",
              "      <td>0.692681</td>\n",
              "      <td>0.004527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.812589</td>\n",
              "      <td>0.763948</td>\n",
              "      <td>0.017595</td>\n",
              "      <td>0.691771</td>\n",
              "      <td>0.668648</td>\n",
              "      <td>0.007784</td>\n",
              "      <td>0.766520</td>\n",
              "      <td>0.724221</td>\n",
              "      <td>0.014963</td>\n",
              "      <td>0.587196</td>\n",
              "      <td>0.552995</td>\n",
              "      <td>0.012320</td>\n",
              "      <td>0.714033</td>\n",
              "      <td>0.687597</td>\n",
              "      <td>0.009353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.890469</td>\n",
              "      <td>0.836524</td>\n",
              "      <td>0.019903</td>\n",
              "      <td>0.735724</td>\n",
              "      <td>0.662050</td>\n",
              "      <td>0.024285</td>\n",
              "      <td>0.764835</td>\n",
              "      <td>0.730361</td>\n",
              "      <td>0.012531</td>\n",
              "      <td>0.622407</td>\n",
              "      <td>0.579775</td>\n",
              "      <td>0.015731</td>\n",
              "      <td>0.760516</td>\n",
              "      <td>0.712373</td>\n",
              "      <td>0.017246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.905109</td>\n",
              "      <td>0.830137</td>\n",
              "      <td>0.024201</td>\n",
              "      <td>0.725067</td>\n",
              "      <td>0.643836</td>\n",
              "      <td>0.030330</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.718182</td>\n",
              "      <td>0.020200</td>\n",
              "      <td>0.600877</td>\n",
              "      <td>0.546535</td>\n",
              "      <td>0.021163</td>\n",
              "      <td>0.763227</td>\n",
              "      <td>0.701886</td>\n",
              "      <td>0.021401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.896067</td>\n",
              "      <td>0.874644</td>\n",
              "      <td>0.007488</td>\n",
              "      <td>0.734211</td>\n",
              "      <td>0.699454</td>\n",
              "      <td>0.013026</td>\n",
              "      <td>0.798144</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.009349</td>\n",
              "      <td>0.641822</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.014810</td>\n",
              "      <td>0.771118</td>\n",
              "      <td>0.754640</td>\n",
              "      <td>0.005476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.523985</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181011</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007407</td>\n",
              "      <td>0.321878</td>\n",
              "      <td>0.100045</td>\n",
              "      <td>0.080385</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.008299</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.019451</td>\n",
              "      <td>0.080573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.888252</td>\n",
              "      <td>0.863572</td>\n",
              "      <td>0.008898</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.697274</td>\n",
              "      <td>0.006072</td>\n",
              "      <td>0.789238</td>\n",
              "      <td>0.749415</td>\n",
              "      <td>0.015826</td>\n",
              "      <td>0.646091</td>\n",
              "      <td>0.574380</td>\n",
              "      <td>0.025951</td>\n",
              "      <td>0.764715</td>\n",
              "      <td>0.742481</td>\n",
              "      <td>0.007857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.886657</td>\n",
              "      <td>0.862119</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>0.732432</td>\n",
              "      <td>0.713092</td>\n",
              "      <td>0.007152</td>\n",
              "      <td>0.828054</td>\n",
              "      <td>0.757991</td>\n",
              "      <td>0.023326</td>\n",
              "      <td>0.639004</td>\n",
              "      <td>0.605932</td>\n",
              "      <td>0.013809</td>\n",
              "      <td>0.766471</td>\n",
              "      <td>0.750697</td>\n",
              "      <td>0.005891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.879536</td>\n",
              "      <td>0.861671</td>\n",
              "      <td>0.007292</td>\n",
              "      <td>0.724965</td>\n",
              "      <td>0.709497</td>\n",
              "      <td>0.005322</td>\n",
              "      <td>0.823799</td>\n",
              "      <td>0.771930</td>\n",
              "      <td>0.018926</td>\n",
              "      <td>0.639130</td>\n",
              "      <td>0.596950</td>\n",
              "      <td>0.015386</td>\n",
              "      <td>0.766335</td>\n",
              "      <td>0.746589</td>\n",
              "      <td>0.007618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.563415</td>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.006968</td>\n",
              "      <td>0.598854</td>\n",
              "      <td>0.574386</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>0.684564</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.018373</td>\n",
              "      <td>0.419048</td>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.040520</td>\n",
              "      <td>0.563546</td>\n",
              "      <td>0.530574</td>\n",
              "      <td>0.011261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.550336</td>\n",
              "      <td>0.508906</td>\n",
              "      <td>0.015003</td>\n",
              "      <td>0.560647</td>\n",
              "      <td>0.522667</td>\n",
              "      <td>0.012943</td>\n",
              "      <td>0.628450</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.016889</td>\n",
              "      <td>0.310273</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.022780</td>\n",
              "      <td>0.515928</td>\n",
              "      <td>0.476910</td>\n",
              "      <td>0.015441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.580132</td>\n",
              "      <td>0.539642</td>\n",
              "      <td>0.015095</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.598930</td>\n",
              "      <td>0.005760</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.648188</td>\n",
              "      <td>0.023506</td>\n",
              "      <td>0.402464</td>\n",
              "      <td>0.343249</td>\n",
              "      <td>0.023584</td>\n",
              "      <td>0.575422</td>\n",
              "      <td>0.551220</td>\n",
              "      <td>0.007997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.189547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138322</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.036721</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008720</td>\n",
              "      <td>0.072364</td>\n",
              "      <td>0.018762</td>\n",
              "      <td>0.020278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.631136</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.634538</td>\n",
              "      <td>0.620321</td>\n",
              "      <td>0.005249</td>\n",
              "      <td>0.716895</td>\n",
              "      <td>0.694690</td>\n",
              "      <td>0.009327</td>\n",
              "      <td>0.446352</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.014221</td>\n",
              "      <td>0.616324</td>\n",
              "      <td>0.596140</td>\n",
              "      <td>0.006960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.593123</td>\n",
              "      <td>0.550512</td>\n",
              "      <td>0.016477</td>\n",
              "      <td>0.631436</td>\n",
              "      <td>0.616231</td>\n",
              "      <td>0.006214</td>\n",
              "      <td>0.692810</td>\n",
              "      <td>0.672566</td>\n",
              "      <td>0.008676</td>\n",
              "      <td>0.409186</td>\n",
              "      <td>0.370203</td>\n",
              "      <td>0.013042</td>\n",
              "      <td>0.577598</td>\n",
              "      <td>0.564060</td>\n",
              "      <td>0.004835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.615836</td>\n",
              "      <td>0.012420</td>\n",
              "      <td>0.656381</td>\n",
              "      <td>0.642150</td>\n",
              "      <td>0.006246</td>\n",
              "      <td>0.734066</td>\n",
              "      <td>0.703540</td>\n",
              "      <td>0.010298</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.424116</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>0.620461</td>\n",
              "      <td>0.607989</td>\n",
              "      <td>0.005315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.821683</td>\n",
              "      <td>0.789400</td>\n",
              "      <td>0.011938</td>\n",
              "      <td>0.703601</td>\n",
              "      <td>0.651748</td>\n",
              "      <td>0.016891</td>\n",
              "      <td>0.746137</td>\n",
              "      <td>0.707158</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.597849</td>\n",
              "      <td>0.545842</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>0.723223</td>\n",
              "      <td>0.698204</td>\n",
              "      <td>0.009664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.802837</td>\n",
              "      <td>0.761134</td>\n",
              "      <td>0.013577</td>\n",
              "      <td>0.673130</td>\n",
              "      <td>0.623881</td>\n",
              "      <td>0.018566</td>\n",
              "      <td>0.723769</td>\n",
              "      <td>0.669663</td>\n",
              "      <td>0.023624</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.513131</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.694423</td>\n",
              "      <td>0.662422</td>\n",
              "      <td>0.015018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.832386</td>\n",
              "      <td>0.810512</td>\n",
              "      <td>0.008108</td>\n",
              "      <td>0.701513</td>\n",
              "      <td>0.657895</td>\n",
              "      <td>0.014903</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.720358</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.564756</td>\n",
              "      <td>0.015188</td>\n",
              "      <td>0.721743</td>\n",
              "      <td>0.700958</td>\n",
              "      <td>0.007519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.080925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173318</td>\n",
              "      <td>0.013363</td>\n",
              "      <td>0.051620</td>\n",
              "      <td>0.181244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059557</td>\n",
              "      <td>0.069518</td>\n",
              "      <td>0.017802</td>\n",
              "      <td>0.020213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.826896</td>\n",
              "      <td>0.792614</td>\n",
              "      <td>0.012819</td>\n",
              "      <td>0.690667</td>\n",
              "      <td>0.658263</td>\n",
              "      <td>0.012111</td>\n",
              "      <td>0.751131</td>\n",
              "      <td>0.738197</td>\n",
              "      <td>0.004568</td>\n",
              "      <td>0.593220</td>\n",
              "      <td>0.528455</td>\n",
              "      <td>0.026487</td>\n",
              "      <td>0.722879</td>\n",
              "      <td>0.690554</td>\n",
              "      <td>0.011276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.813097</td>\n",
              "      <td>0.787535</td>\n",
              "      <td>0.008702</td>\n",
              "      <td>0.686192</td>\n",
              "      <td>0.660140</td>\n",
              "      <td>0.009889</td>\n",
              "      <td>0.747204</td>\n",
              "      <td>0.733485</td>\n",
              "      <td>0.004731</td>\n",
              "      <td>0.570815</td>\n",
              "      <td>0.543568</td>\n",
              "      <td>0.009878</td>\n",
              "      <td>0.710174</td>\n",
              "      <td>0.693918</td>\n",
              "      <td>0.005279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.803443</td>\n",
              "      <td>0.009157</td>\n",
              "      <td>0.698727</td>\n",
              "      <td>0.658192</td>\n",
              "      <td>0.013215</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.714588</td>\n",
              "      <td>0.011037</td>\n",
              "      <td>0.581443</td>\n",
              "      <td>0.533613</td>\n",
              "      <td>0.016158</td>\n",
              "      <td>0.712762</td>\n",
              "      <td>0.692731</td>\n",
              "      <td>0.006741</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e81c2ee6-33d7-4706-925f-80b1ba39a451')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e81c2ee6-33d7-4706-925f-80b1ba39a451 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e81c2ee6-33d7-4706-925f-80b1ba39a451');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**K&H+N, BLESS, EVALution, ROOT9**\n",
        "To process the results of these datasets. Note that the results for templates `T1`-`T4` correspond to the non-masked models, and templates `TM1`-`TM3`for the masked ones. See the dictionary `templates2abrev` in a cell above."
      ],
      "metadata": {
        "id": "Tdub9432I4PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframe_results_max_min_measure(dict_res, dataset, measure, list_sub_measures=['precision', 'recall', 'f1-score']):\n",
        "    inc_index = 3*len(list_sub_measures)\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]} \n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            d_measures = dict_res[dataset.lower()][m][t]['flat_reports'][measure]\n",
        "            \n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "\n",
        "            for meas in d_measures: \n",
        "                if meas in list_sub_measures:               \n",
        "                    l_max = dict_df.setdefault('max-'+meas, [])\n",
        "                    l_max.append(np.array(d_measures[meas]).max())\n",
        "                    l_min = dict_df.setdefault('min-'+meas, [])\n",
        "                    l_min.append(np.array(d_measures[meas]).min())\n",
        "                    l_std = dict_df.setdefault('std-'+meas, [])\n",
        "                    l_std.append(np.array(d_measures[meas]).std())\n",
        "\n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    multi_col = pd.MultiIndex.from_tuples(zip([dataset]*inc_index, res_df.columns[3:(3+inc_index)]),sortorder=None)\n",
        "    res_df = res_df.iloc[:,3:(3+inc_index)]\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = multi_col\n",
        "    return res_df\n",
        "\n",
        "def get_dataframe_results_measure(dict_res, dataset, measure, list_sub_measures=['precision', 'recall', 'f1-score']):\n",
        "    inc_index = len(list_sub_measures)\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]} \n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            d_measures = dict_res[dataset.lower()][m][t]['mean_report'][measure]\n",
        "            \n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "\n",
        "            for meas in d_measures: \n",
        "                if meas in list_sub_measures:               \n",
        "                    l = dict_df.setdefault(meas, [])\n",
        "                    l.append(d_measures[meas])\n",
        "\n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    multi_col = pd.MultiIndex.from_tuples(zip([dataset]*inc_index, res_df.columns[3:(3+inc_index)]),sortorder=None)\n",
        "    res_df = res_df.iloc[:,3:(3+inc_index)]\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = multi_col\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "35lg0QNuJJlq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']\n",
        "list_complete_res = []\n",
        "\n",
        "for one_dataset in all_datasets:\n",
        "    if one_dataset.lower() in list(dict_res.keys()):\n",
        "        df_res = get_dataframe_results_measure(dict_res, one_dataset, 'weighted avg') # one of 'weighted avg' and 'macro avg'\n",
        "        list_complete_res.append(df_res)\n",
        "    else:\n",
        "        print(\"WARNING: There are not results for dataset \" + one_dataset)\n",
        "df_datasets_final = pd.concat(list_complete_res, axis = 1)\n",
        "\n",
        "#change column order\n",
        "order1 = {'k&h+n':0,'bless':1,'evalution':2, 'root09':3}\n",
        "order2 = {'precision':0,'recall':1,'f1-score':2}\n",
        "multi_col_list = list(df_datasets_final.columns)\n",
        "multi_col_list.sort(key=lambda x: 10*order1[x[0].lower()] + order2[x[1].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "df_datasets_final = pd.DataFrame(df_datasets_final, columns=multi_col)\n",
        "df_datasets_final.sort_index(level=['model','template'], inplace=True)"
      ],
      "metadata": {
        "id": "ajsY7HIUJPl6"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOTAS_FILE_REST = DIR_RESULTS + 'sotas_results_literature/khnBlessEvalRoot_Sotas_RC.txt'\n",
        "sotas_res = pd.read_csv(SOTAS_FILE_REST, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "multi_row_tuples = list(zip(['Sota']*sotas_res.shape[0], list(sotas_res.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "sotas_res.index=multi_row\n",
        "df_res_sotas = pd.concat([df_datasets_final,sotas_res])\n",
        "\n",
        "#change column order\n",
        "order1 = {'k&h+n':0,'bless':1,'evalution':2, 'root09':3}\n",
        "order2 = {'precision':0,'recall':1,'f1-score':2}\n",
        "multi_col_list = list(df_res_sotas.columns)\n",
        "multi_col_list.sort(key=lambda x: 10*order1[x[0].lower()] + order2[x[1].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "\n",
        "df_res_sotas = pd.DataFrame(df_res_sotas, columns=multi_col)\n",
        "df_res_sotas.apply(round, ndigits=30)\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "aDAQ_3tS3mMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97a0cd4a-dc53-4a13-bc35-22c8cd0821f1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          K&H+N                         BLESS            \\\n",
              "                      precision    recall  f1-score precision    recall   \n",
              "model        template                                                     \n",
              "Bert         T1        0.989279  0.989372  0.989283  0.952062  0.950972   \n",
              "             T2        0.988852  0.988955  0.988862  0.949638  0.948079   \n",
              "             T3        0.989652  0.989761  0.989652  0.952650  0.951996   \n",
              "             T4        0.740641  0.587883  0.510361  0.243637  0.200090   \n",
              "             TM1       0.987078  0.987216  0.987016  0.942206  0.940726   \n",
              "             TM2       0.987278  0.987438  0.987288  0.945898  0.944463   \n",
              "             TM3       0.985504  0.985755  0.985431  0.947861  0.946723   \n",
              "Roberta      T1        0.988899  0.988968  0.988911  0.954583  0.953895   \n",
              "             T2        0.988876  0.988955  0.988888  0.955340  0.954347   \n",
              "             T3        0.988944  0.989024  0.988954  0.956224  0.955492   \n",
              "             T4        0.603157  0.325923  0.311507  0.511042  0.194275   \n",
              "             TM1       0.988515  0.988593  0.988496  0.948353  0.946241   \n",
              "             TM2       0.987667  0.987772  0.987678  0.947049  0.945482   \n",
              "             TM3       0.985518  0.985185  0.985282  0.951303  0.950460   \n",
              "bert-base    T1        0.987638  0.987689  0.987644  0.943748  0.941871   \n",
              "             T2        0.986912  0.986993  0.986923  0.943035  0.941058   \n",
              "             T3        0.986869  0.986771  0.986793  0.943626  0.942203   \n",
              "             T4        0.547507  0.428935  0.316018  0.369564  0.228386   \n",
              "             TM1       0.986135  0.986256  0.986119  0.938743  0.936055   \n",
              "             TM2       0.985471  0.985672  0.985414  0.940231  0.939491   \n",
              "             TM3       0.985178  0.985324  0.985048  0.940717  0.939220   \n",
              "roberta-base T1        0.983463  0.983724  0.983494  0.950049  0.948893   \n",
              "             T2        0.987663  0.987703  0.987669  0.948488  0.947235   \n",
              "             T3        0.987439  0.987480  0.987445  0.949862  0.948772   \n",
              "             T4        0.660224  0.455352  0.299090  0.504303  0.138557   \n",
              "             TM1       0.986559  0.986311  0.986338  0.941440  0.939882   \n",
              "             TM2       0.982519  0.982848  0.982503  0.945688  0.944252   \n",
              "             TM3       0.986242  0.986298  0.986152  0.945586  0.943890   \n",
              "Sota         LexNET    0.985000  0.986000  0.985000  0.894000  0.893000   \n",
              "             KEML      0.993000  0.993000  0.993000  0.944000  0.943000   \n",
              "             SphereRE  0.990000  0.989000  0.990000  0.938000  0.938000   \n",
              "             RelBERT        NaN       NaN  0.949000       NaN       NaN   \n",
              "\n",
              "                                EVALution                        ROOT09  \\\n",
              "                       f1-score precision    recall  f1-score precision   \n",
              "model        template                                                     \n",
              "Bert         T1        0.951223  0.747669  0.747562  0.747197  0.926534   \n",
              "             T2        0.948395  0.738647  0.739003  0.736617  0.930032   \n",
              "             T3        0.952156  0.753333  0.750271  0.750660  0.930945   \n",
              "             T4        0.087649  0.115621  0.148537  0.052673  0.929053   \n",
              "             TM1       0.940908  0.754933  0.743987  0.744853  0.926502   \n",
              "             TM2       0.944739  0.738177  0.728711  0.722481  0.925350   \n",
              "             TM3       0.946968  0.729665  0.726327  0.723992  0.926887   \n",
              "Roberta      T1        0.953960  0.768848  0.764789  0.764326  0.937188   \n",
              "             T2        0.954563  0.758895  0.759372  0.757473  0.936314   \n",
              "             T3        0.955660  0.773299  0.771181  0.770558  0.938197   \n",
              "             T4        0.132698  0.229777  0.190683  0.087452  0.935851   \n",
              "             TM1       0.946602  0.771562  0.761863  0.760673  0.936317   \n",
              "             TM2       0.945714  0.771351  0.764680  0.764198  0.929912   \n",
              "             TM3       0.950598  0.774145  0.754171  0.746288  0.926417   \n",
              "bert-base    T1        0.942220  0.690299  0.690574  0.689041  0.925621   \n",
              "             T2        0.941427  0.674987  0.672264  0.672354  0.919290   \n",
              "             T3        0.942477  0.696403  0.693716  0.694414  0.922468   \n",
              "             T4        0.165348  0.213162  0.218202  0.119044  0.920844   \n",
              "             TM1       0.936486  0.706863  0.699675  0.698001  0.917368   \n",
              "             TM2       0.939514  0.689736  0.686457  0.683546  0.917999   \n",
              "             TM3       0.939404  0.696653  0.691874  0.686207  0.917967   \n",
              "roberta-base T1        0.949115  0.748904  0.744204  0.745112  0.932277   \n",
              "             T2        0.947487  0.746132  0.744420  0.744220  0.931237   \n",
              "             T3        0.949017  0.756320  0.753304  0.753849  0.934262   \n",
              "             T4        0.042571  0.120698  0.094908  0.023306  0.923711   \n",
              "             TM1       0.940137  0.758330  0.745179  0.747326  0.927174   \n",
              "             TM2       0.944434  0.740216  0.723835  0.727344  0.925764   \n",
              "             TM3       0.944177  0.740151  0.737486  0.729413  0.924481   \n",
              "Sota         LexNET    0.893000  0.601000  0.607000  0.600000  0.813000   \n",
              "             KEML      0.944000  0.663000  0.660000  0.660000  0.878000   \n",
              "             SphereRE  0.938000  0.620000  0.621000  0.620000  0.860000   \n",
              "             RelBERT   0.921000       NaN       NaN  0.701000       NaN   \n",
              "\n",
              "                                           \n",
              "                         recall  f1-score  \n",
              "model        template                      \n",
              "Bert         T1        0.926418  0.926258  \n",
              "             T2        0.928549  0.928900  \n",
              "             T3        0.931056  0.930907  \n",
              "             T4        0.927922  0.928181  \n",
              "             TM1       0.925227  0.925272  \n",
              "             TM2       0.924726  0.924586  \n",
              "             TM3       0.923660  0.923911  \n",
              "Roberta      T1        0.936133  0.936310  \n",
              "             T2        0.935945  0.936030  \n",
              "             T3        0.937073  0.937334  \n",
              "             T4        0.933626  0.934019  \n",
              "             TM1       0.935569  0.935642  \n",
              "             TM2       0.928612  0.928193  \n",
              "             TM3       0.926042  0.925744  \n",
              "bert-base    T1        0.924036  0.924396  \n",
              "             T2        0.917706  0.918047  \n",
              "             T3        0.921216  0.921417  \n",
              "             T4        0.919210  0.919461  \n",
              "             TM1       0.917330  0.917135  \n",
              "             TM2       0.916578  0.916741  \n",
              "             TM3       0.914886  0.915287  \n",
              "roberta-base T1        0.931181  0.931330  \n",
              "             T2        0.930743  0.930811  \n",
              "             T3        0.933124  0.933323  \n",
              "             T4        0.923347  0.923347  \n",
              "             TM1       0.925729  0.925664  \n",
              "             TM2       0.925541  0.925317  \n",
              "             TM3       0.924287  0.923989  \n",
              "Sota         LexNET    0.814000  0.813000  \n",
              "             KEML      0.877000  0.878000  \n",
              "             SphereRE  0.862000  0.861000  \n",
              "             RelBERT        NaN  0.910000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b9647b1d-6a0a-4a6d-8aaf-35e51760e3d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">K&amp;H+N</th>\n",
              "      <th colspan=\"3\" halign=\"left\">BLESS</th>\n",
              "      <th colspan=\"3\" halign=\"left\">EVALution</th>\n",
              "      <th colspan=\"3\" halign=\"left\">ROOT09</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.989279</td>\n",
              "      <td>0.989372</td>\n",
              "      <td>0.989283</td>\n",
              "      <td>0.952062</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.951223</td>\n",
              "      <td>0.747669</td>\n",
              "      <td>0.747562</td>\n",
              "      <td>0.747197</td>\n",
              "      <td>0.926534</td>\n",
              "      <td>0.926418</td>\n",
              "      <td>0.926258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988852</td>\n",
              "      <td>0.988955</td>\n",
              "      <td>0.988862</td>\n",
              "      <td>0.949638</td>\n",
              "      <td>0.948079</td>\n",
              "      <td>0.948395</td>\n",
              "      <td>0.738647</td>\n",
              "      <td>0.739003</td>\n",
              "      <td>0.736617</td>\n",
              "      <td>0.930032</td>\n",
              "      <td>0.928549</td>\n",
              "      <td>0.928900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.989652</td>\n",
              "      <td>0.989761</td>\n",
              "      <td>0.989652</td>\n",
              "      <td>0.952650</td>\n",
              "      <td>0.951996</td>\n",
              "      <td>0.952156</td>\n",
              "      <td>0.753333</td>\n",
              "      <td>0.750271</td>\n",
              "      <td>0.750660</td>\n",
              "      <td>0.930945</td>\n",
              "      <td>0.931056</td>\n",
              "      <td>0.930907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.740641</td>\n",
              "      <td>0.587883</td>\n",
              "      <td>0.510361</td>\n",
              "      <td>0.243637</td>\n",
              "      <td>0.200090</td>\n",
              "      <td>0.087649</td>\n",
              "      <td>0.115621</td>\n",
              "      <td>0.148537</td>\n",
              "      <td>0.052673</td>\n",
              "      <td>0.929053</td>\n",
              "      <td>0.927922</td>\n",
              "      <td>0.928181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.987078</td>\n",
              "      <td>0.987216</td>\n",
              "      <td>0.987016</td>\n",
              "      <td>0.942206</td>\n",
              "      <td>0.940726</td>\n",
              "      <td>0.940908</td>\n",
              "      <td>0.754933</td>\n",
              "      <td>0.743987</td>\n",
              "      <td>0.744853</td>\n",
              "      <td>0.926502</td>\n",
              "      <td>0.925227</td>\n",
              "      <td>0.925272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.987278</td>\n",
              "      <td>0.987438</td>\n",
              "      <td>0.987288</td>\n",
              "      <td>0.945898</td>\n",
              "      <td>0.944463</td>\n",
              "      <td>0.944739</td>\n",
              "      <td>0.738177</td>\n",
              "      <td>0.728711</td>\n",
              "      <td>0.722481</td>\n",
              "      <td>0.925350</td>\n",
              "      <td>0.924726</td>\n",
              "      <td>0.924586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985504</td>\n",
              "      <td>0.985755</td>\n",
              "      <td>0.985431</td>\n",
              "      <td>0.947861</td>\n",
              "      <td>0.946723</td>\n",
              "      <td>0.946968</td>\n",
              "      <td>0.729665</td>\n",
              "      <td>0.726327</td>\n",
              "      <td>0.723992</td>\n",
              "      <td>0.926887</td>\n",
              "      <td>0.923660</td>\n",
              "      <td>0.923911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.988899</td>\n",
              "      <td>0.988968</td>\n",
              "      <td>0.988911</td>\n",
              "      <td>0.954583</td>\n",
              "      <td>0.953895</td>\n",
              "      <td>0.953960</td>\n",
              "      <td>0.768848</td>\n",
              "      <td>0.764789</td>\n",
              "      <td>0.764326</td>\n",
              "      <td>0.937188</td>\n",
              "      <td>0.936133</td>\n",
              "      <td>0.936310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988876</td>\n",
              "      <td>0.988955</td>\n",
              "      <td>0.988888</td>\n",
              "      <td>0.955340</td>\n",
              "      <td>0.954347</td>\n",
              "      <td>0.954563</td>\n",
              "      <td>0.758895</td>\n",
              "      <td>0.759372</td>\n",
              "      <td>0.757473</td>\n",
              "      <td>0.936314</td>\n",
              "      <td>0.935945</td>\n",
              "      <td>0.936030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.988944</td>\n",
              "      <td>0.989024</td>\n",
              "      <td>0.988954</td>\n",
              "      <td>0.956224</td>\n",
              "      <td>0.955492</td>\n",
              "      <td>0.955660</td>\n",
              "      <td>0.773299</td>\n",
              "      <td>0.771181</td>\n",
              "      <td>0.770558</td>\n",
              "      <td>0.938197</td>\n",
              "      <td>0.937073</td>\n",
              "      <td>0.937334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.603157</td>\n",
              "      <td>0.325923</td>\n",
              "      <td>0.311507</td>\n",
              "      <td>0.511042</td>\n",
              "      <td>0.194275</td>\n",
              "      <td>0.132698</td>\n",
              "      <td>0.229777</td>\n",
              "      <td>0.190683</td>\n",
              "      <td>0.087452</td>\n",
              "      <td>0.935851</td>\n",
              "      <td>0.933626</td>\n",
              "      <td>0.934019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.988515</td>\n",
              "      <td>0.988593</td>\n",
              "      <td>0.988496</td>\n",
              "      <td>0.948353</td>\n",
              "      <td>0.946241</td>\n",
              "      <td>0.946602</td>\n",
              "      <td>0.771562</td>\n",
              "      <td>0.761863</td>\n",
              "      <td>0.760673</td>\n",
              "      <td>0.936317</td>\n",
              "      <td>0.935569</td>\n",
              "      <td>0.935642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.987667</td>\n",
              "      <td>0.987772</td>\n",
              "      <td>0.987678</td>\n",
              "      <td>0.947049</td>\n",
              "      <td>0.945482</td>\n",
              "      <td>0.945714</td>\n",
              "      <td>0.771351</td>\n",
              "      <td>0.764680</td>\n",
              "      <td>0.764198</td>\n",
              "      <td>0.929912</td>\n",
              "      <td>0.928612</td>\n",
              "      <td>0.928193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985518</td>\n",
              "      <td>0.985185</td>\n",
              "      <td>0.985282</td>\n",
              "      <td>0.951303</td>\n",
              "      <td>0.950460</td>\n",
              "      <td>0.950598</td>\n",
              "      <td>0.774145</td>\n",
              "      <td>0.754171</td>\n",
              "      <td>0.746288</td>\n",
              "      <td>0.926417</td>\n",
              "      <td>0.926042</td>\n",
              "      <td>0.925744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.987638</td>\n",
              "      <td>0.987689</td>\n",
              "      <td>0.987644</td>\n",
              "      <td>0.943748</td>\n",
              "      <td>0.941871</td>\n",
              "      <td>0.942220</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.690574</td>\n",
              "      <td>0.689041</td>\n",
              "      <td>0.925621</td>\n",
              "      <td>0.924036</td>\n",
              "      <td>0.924396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.986912</td>\n",
              "      <td>0.986993</td>\n",
              "      <td>0.986923</td>\n",
              "      <td>0.943035</td>\n",
              "      <td>0.941058</td>\n",
              "      <td>0.941427</td>\n",
              "      <td>0.674987</td>\n",
              "      <td>0.672264</td>\n",
              "      <td>0.672354</td>\n",
              "      <td>0.919290</td>\n",
              "      <td>0.917706</td>\n",
              "      <td>0.918047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.986869</td>\n",
              "      <td>0.986771</td>\n",
              "      <td>0.986793</td>\n",
              "      <td>0.943626</td>\n",
              "      <td>0.942203</td>\n",
              "      <td>0.942477</td>\n",
              "      <td>0.696403</td>\n",
              "      <td>0.693716</td>\n",
              "      <td>0.694414</td>\n",
              "      <td>0.922468</td>\n",
              "      <td>0.921216</td>\n",
              "      <td>0.921417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.547507</td>\n",
              "      <td>0.428935</td>\n",
              "      <td>0.316018</td>\n",
              "      <td>0.369564</td>\n",
              "      <td>0.228386</td>\n",
              "      <td>0.165348</td>\n",
              "      <td>0.213162</td>\n",
              "      <td>0.218202</td>\n",
              "      <td>0.119044</td>\n",
              "      <td>0.920844</td>\n",
              "      <td>0.919210</td>\n",
              "      <td>0.919461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.986135</td>\n",
              "      <td>0.986256</td>\n",
              "      <td>0.986119</td>\n",
              "      <td>0.938743</td>\n",
              "      <td>0.936055</td>\n",
              "      <td>0.936486</td>\n",
              "      <td>0.706863</td>\n",
              "      <td>0.699675</td>\n",
              "      <td>0.698001</td>\n",
              "      <td>0.917368</td>\n",
              "      <td>0.917330</td>\n",
              "      <td>0.917135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.985471</td>\n",
              "      <td>0.985672</td>\n",
              "      <td>0.985414</td>\n",
              "      <td>0.940231</td>\n",
              "      <td>0.939491</td>\n",
              "      <td>0.939514</td>\n",
              "      <td>0.689736</td>\n",
              "      <td>0.686457</td>\n",
              "      <td>0.683546</td>\n",
              "      <td>0.917999</td>\n",
              "      <td>0.916578</td>\n",
              "      <td>0.916741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985178</td>\n",
              "      <td>0.985324</td>\n",
              "      <td>0.985048</td>\n",
              "      <td>0.940717</td>\n",
              "      <td>0.939220</td>\n",
              "      <td>0.939404</td>\n",
              "      <td>0.696653</td>\n",
              "      <td>0.691874</td>\n",
              "      <td>0.686207</td>\n",
              "      <td>0.917967</td>\n",
              "      <td>0.914886</td>\n",
              "      <td>0.915287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.983463</td>\n",
              "      <td>0.983724</td>\n",
              "      <td>0.983494</td>\n",
              "      <td>0.950049</td>\n",
              "      <td>0.948893</td>\n",
              "      <td>0.949115</td>\n",
              "      <td>0.748904</td>\n",
              "      <td>0.744204</td>\n",
              "      <td>0.745112</td>\n",
              "      <td>0.932277</td>\n",
              "      <td>0.931181</td>\n",
              "      <td>0.931330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.987663</td>\n",
              "      <td>0.987703</td>\n",
              "      <td>0.987669</td>\n",
              "      <td>0.948488</td>\n",
              "      <td>0.947235</td>\n",
              "      <td>0.947487</td>\n",
              "      <td>0.746132</td>\n",
              "      <td>0.744420</td>\n",
              "      <td>0.744220</td>\n",
              "      <td>0.931237</td>\n",
              "      <td>0.930743</td>\n",
              "      <td>0.930811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.987439</td>\n",
              "      <td>0.987480</td>\n",
              "      <td>0.987445</td>\n",
              "      <td>0.949862</td>\n",
              "      <td>0.948772</td>\n",
              "      <td>0.949017</td>\n",
              "      <td>0.756320</td>\n",
              "      <td>0.753304</td>\n",
              "      <td>0.753849</td>\n",
              "      <td>0.934262</td>\n",
              "      <td>0.933124</td>\n",
              "      <td>0.933323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.660224</td>\n",
              "      <td>0.455352</td>\n",
              "      <td>0.299090</td>\n",
              "      <td>0.504303</td>\n",
              "      <td>0.138557</td>\n",
              "      <td>0.042571</td>\n",
              "      <td>0.120698</td>\n",
              "      <td>0.094908</td>\n",
              "      <td>0.023306</td>\n",
              "      <td>0.923711</td>\n",
              "      <td>0.923347</td>\n",
              "      <td>0.923347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.986559</td>\n",
              "      <td>0.986311</td>\n",
              "      <td>0.986338</td>\n",
              "      <td>0.941440</td>\n",
              "      <td>0.939882</td>\n",
              "      <td>0.940137</td>\n",
              "      <td>0.758330</td>\n",
              "      <td>0.745179</td>\n",
              "      <td>0.747326</td>\n",
              "      <td>0.927174</td>\n",
              "      <td>0.925729</td>\n",
              "      <td>0.925664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.982519</td>\n",
              "      <td>0.982848</td>\n",
              "      <td>0.982503</td>\n",
              "      <td>0.945688</td>\n",
              "      <td>0.944252</td>\n",
              "      <td>0.944434</td>\n",
              "      <td>0.740216</td>\n",
              "      <td>0.723835</td>\n",
              "      <td>0.727344</td>\n",
              "      <td>0.925764</td>\n",
              "      <td>0.925541</td>\n",
              "      <td>0.925317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.986242</td>\n",
              "      <td>0.986298</td>\n",
              "      <td>0.986152</td>\n",
              "      <td>0.945586</td>\n",
              "      <td>0.943890</td>\n",
              "      <td>0.944177</td>\n",
              "      <td>0.740151</td>\n",
              "      <td>0.737486</td>\n",
              "      <td>0.729413</td>\n",
              "      <td>0.924481</td>\n",
              "      <td>0.924287</td>\n",
              "      <td>0.923989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Sota</th>\n",
              "      <th>LexNET</th>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.986000</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.601000</td>\n",
              "      <td>0.607000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.813000</td>\n",
              "      <td>0.814000</td>\n",
              "      <td>0.813000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KEML</th>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.943000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.663000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.877000</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SphereRE</th>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.989000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.621000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.862000</td>\n",
              "      <td>0.861000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RelBERT</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.949000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.921000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.910000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b9647b1d-6a0a-4a6d-8aaf-35e51760e3d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b9647b1d-6a0a-4a6d-8aaf-35e51760e3d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b9647b1d-6a0a-4a6d-8aaf-35e51760e3d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']\n",
        "list_complete_res = []\n",
        "\n",
        "for one_dataset in all_datasets:\n",
        "    if one_dataset.lower() in list(dict_res.keys()):\n",
        "        df_res = get_dataframe_results_max_min_measure(dict_res, one_dataset, 'weighted avg') # one of 'weighted avg' and 'macro avg'\n",
        "        list_complete_res.append(df_res)\n",
        "    else:\n",
        "        print(\"WARNING: There are not results for dataset \" + one_dataset)\n",
        "df_datasets_max_min_final = pd.concat(list_complete_res, axis = 1)\n",
        "\n",
        "df_datasets_max_min_final.sort_index(level=['model','template'], inplace=True)\n",
        "df_datasets_max_min_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tf_86X5VSyPJ",
        "outputId": "9a3efefc-9c9b-4867-a24a-3457ce3079d5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              K&H+N                                         \\\n",
              "                      max-precision min-precision std-precision max-recall   \n",
              "model        template                                                        \n",
              "Bert         T1            0.990061      0.987516      0.000963   0.990123   \n",
              "             T2            0.990036      0.987008      0.001060   0.990123   \n",
              "             T3            0.990883      0.987878      0.001022   0.990958   \n",
              "             T4            0.775743      0.674940      0.035524   0.769354   \n",
              "             TM1           0.989056      0.982043      0.002680   0.989080   \n",
              "             TM2           0.989949      0.986245      0.001364   0.990054   \n",
              "             TM3           0.989805      0.982281      0.002426   0.989914   \n",
              "Roberta      T1            0.989841      0.986780      0.001096   0.989775   \n",
              "             T2            0.989835      0.987854      0.000857   0.989914   \n",
              "             T3            0.990133      0.986657      0.001273   0.990123   \n",
              "             T4            0.743041      0.260714      0.179172   0.615775   \n",
              "             TM1           0.989355      0.987437      0.000640   0.989428   \n",
              "             TM2           0.989101      0.986003      0.001152   0.989219   \n",
              "             TM3           0.987562      0.983772      0.001526   0.987619   \n",
              "bert-base    T1            0.988275      0.986896      0.000539   0.988384   \n",
              "             T2            0.988046      0.985607      0.000889   0.988106   \n",
              "             T3            0.988825      0.983998      0.001578   0.988941   \n",
              "             T4            0.664307      0.195018      0.178643   0.492940   \n",
              "             TM1           0.988435      0.982377      0.002223   0.988593   \n",
              "             TM2           0.988935      0.982502      0.002433   0.989080   \n",
              "             TM3           0.987908      0.983026      0.001857   0.988036   \n",
              "roberta-base T1            0.987826      0.968645      0.007421   0.987897   \n",
              "             T2            0.988303      0.986700      0.000570   0.988384   \n",
              "             T3            0.988901      0.986157      0.000887   0.988941   \n",
              "             T4            0.664321      0.645879      0.007210   0.476386   \n",
              "             TM1           0.989165      0.983657      0.002170   0.989219   \n",
              "             TM2           0.986707      0.978421      0.003474   0.986784   \n",
              "             TM3           0.988230      0.983090      0.001930   0.988245   \n",
              "\n",
              "                                                                       \\\n",
              "                      min-recall std-recall max-f1-score min-f1-score   \n",
              "model        template                                                   \n",
              "Bert         T1         0.987689   0.000924     0.990072     0.987522   \n",
              "             T2         0.987132   0.001049     0.990042     0.987037   \n",
              "             T3         0.988036   0.000991     0.990883     0.987902   \n",
              "             T4         0.483620   0.102657     0.757430     0.345001   \n",
              "             TM1        0.982263   0.002623     0.989043     0.981875   \n",
              "             TM2        0.986367   0.001341     0.989970     0.986266   \n",
              "             TM3        0.982611   0.002355     0.989794     0.982343   \n",
              "Roberta      T1         0.986924   0.001050     0.989794     0.986825   \n",
              "             T2         0.987897   0.000867     0.989857     0.987828   \n",
              "             T3         0.986854   0.001219     0.990122     0.986724   \n",
              "             T4         0.046324   0.190253     0.580858     0.035667   \n",
              "             TM1        0.987480   0.000652     0.989348     0.987418   \n",
              "             TM2        0.985880   0.001203     0.989131     0.985923   \n",
              "             TM3        0.982889   0.001954     0.987564     0.983200   \n",
              "bert-base    T1         0.986924   0.000558     0.988305     0.986905   \n",
              "             T2         0.985672   0.000873     0.988070     0.985632   \n",
              "             T3         0.983585   0.001747     0.988859     0.983743   \n",
              "             T4         0.324616   0.055719     0.418042     0.270556   \n",
              "             TM1        0.982333   0.002262     0.988437     0.982336   \n",
              "             TM2        0.982889   0.002327     0.988956     0.982454   \n",
              "             TM3        0.983307   0.001751     0.987759     0.982582   \n",
              "roberta-base T1         0.969882   0.006934     0.987850     0.968813   \n",
              "             T2         0.986715   0.000605     0.988329     0.986694   \n",
              "             T3         0.986089   0.000913     0.988915     0.986108   \n",
              "             T4         0.441678   0.016612     0.342407     0.270708   \n",
              "             TM1        0.982472   0.002649     0.989172     0.983125   \n",
              "             TM2        0.979064   0.003248     0.986715     0.978287   \n",
              "             TM3        0.983376   0.001797     0.988224     0.982965   \n",
              "\n",
              "                                           BLESS  ...    EVALution  \\\n",
              "                      std-f1-score max-precision  ... std-f1-score   \n",
              "model        template                             ...                \n",
              "Bert         T1           0.000961      0.956109  ...     0.009210   \n",
              "             T2           0.001045      0.953170  ...     0.011065   \n",
              "             T3           0.001014      0.957277  ...     0.005743   \n",
              "             T4           0.146788      0.541156  ...     0.047084   \n",
              "             TM1          0.002737      0.943610  ...     0.007153   \n",
              "             TM2          0.001366      0.949011  ...     0.015470   \n",
              "             TM3          0.002427      0.954229  ...     0.018915   \n",
              "Roberta      T1           0.001076      0.956329  ...     0.010243   \n",
              "             T2           0.000868      0.957248  ...     0.009601   \n",
              "             T3           0.001251      0.959666  ...     0.007716   \n",
              "             T4           0.181140      0.536980  ...     0.027860   \n",
              "             TM1          0.000647      0.951622  ...     0.016098   \n",
              "             TM2          0.001178      0.952524  ...     0.006487   \n",
              "             TM3          0.001788      0.957729  ...     0.031670   \n",
              "bert-base    T1           0.000541      0.945973  ...     0.010015   \n",
              "             T2           0.000893      0.945103  ...     0.009959   \n",
              "             T3           0.001671      0.946859  ...     0.004844   \n",
              "             T4           0.058305      0.461790  ...     0.037842   \n",
              "             TM1          0.002217      0.944484  ...     0.012299   \n",
              "             TM2          0.002492      0.945021  ...     0.009803   \n",
              "             TM3          0.001918      0.945217  ...     0.011889   \n",
              "roberta-base T1           0.007353      0.952677  ...     0.009372   \n",
              "             T2           0.000587      0.952293  ...     0.004836   \n",
              "             T3           0.000903      0.951282  ...     0.002267   \n",
              "             T4           0.034454      0.583433  ...     0.013159   \n",
              "             TM1          0.002448      0.944738  ...     0.012418   \n",
              "             TM2          0.003517      0.952338  ...     0.012260   \n",
              "             TM3          0.001952      0.949587  ...     0.023671   \n",
              "\n",
              "                             ROOT09                                         \\\n",
              "                      max-precision min-precision std-precision max-recall   \n",
              "model        template                                                        \n",
              "Bert         T1            0.932149      0.917239      0.005031   0.929803   \n",
              "             T2            0.932559      0.927252      0.002082   0.931369   \n",
              "             T3            0.933409      0.926036      0.002640   0.933563   \n",
              "             T4            0.935060      0.924590      0.003776   0.933250   \n",
              "             TM1           0.931731      0.919070      0.004255   0.931683   \n",
              "             TM2           0.933665      0.914036      0.008265   0.932936   \n",
              "             TM3           0.931012      0.922350      0.003266   0.927609   \n",
              "Roberta      T1            0.940670      0.934449      0.002258   0.938891   \n",
              "             T2            0.939915      0.933373      0.002098   0.939517   \n",
              "             T3            0.940311      0.936691      0.001280   0.938264   \n",
              "             T4            0.939571      0.930950      0.003201   0.938264   \n",
              "             TM1           0.940061      0.930925      0.003079   0.940144   \n",
              "             TM2           0.936819      0.913685      0.008962   0.936070   \n",
              "             TM3           0.936363      0.907132      0.010221   0.933250   \n",
              "bert-base    T1            0.927133      0.923645      0.001415   0.926042   \n",
              "             T2            0.920412      0.917975      0.000867   0.918521   \n",
              "             T3            0.926868      0.920097      0.002341   0.924475   \n",
              "             T4            0.921739      0.918282      0.001298   0.920715   \n",
              "             TM1           0.927143      0.913169      0.005176   0.926355   \n",
              "             TM2           0.922013      0.910831      0.004343   0.920715   \n",
              "             TM3           0.920768      0.916316      0.001844   0.919461   \n",
              "roberta-base T1            0.934250      0.929058      0.001898   0.934190   \n",
              "             T2            0.936081      0.927822      0.003073   0.935443   \n",
              "             T3            0.937320      0.928950      0.002946   0.937010   \n",
              "             T4            0.927172      0.919651      0.002396   0.926355   \n",
              "             TM1           0.935865      0.919793      0.005838   0.935130   \n",
              "             TM2           0.927513      0.923833      0.001390   0.928236   \n",
              "             TM3           0.930174      0.915562      0.005576   0.929489   \n",
              "\n",
              "                                                                       \\\n",
              "                      min-recall std-recall max-f1-score min-f1-score   \n",
              "model        template                                                   \n",
              "Bert         T1         0.917581   0.004543     0.930315     0.917237   \n",
              "             T2         0.924475   0.002644     0.931695     0.925530   \n",
              "             T3         0.926355   0.002681     0.933420     0.926088   \n",
              "             T4         0.923848   0.003275     0.933742     0.924117   \n",
              "             TM1        0.917581   0.005513     0.931686     0.918737   \n",
              "             TM2        0.915074   0.007205     0.933235     0.913595   \n",
              "             TM3        0.916327   0.003952     0.928325     0.917986   \n",
              "Roberta      T1         0.934190   0.002035     0.938886     0.934528   \n",
              "             T2         0.933250   0.002112     0.939588     0.933267   \n",
              "             T3         0.935443   0.001228     0.938763     0.935859   \n",
              "             T4         0.929176   0.003586     0.938024     0.929781   \n",
              "             TM1        0.929803   0.003308     0.940101     0.929951   \n",
              "             TM2        0.911626   0.009268     0.935949     0.908673   \n",
              "             TM3        0.908493   0.008989     0.933150     0.907248   \n",
              "bert-base    T1         0.921341   0.001768     0.926192     0.922099   \n",
              "             T2         0.916641   0.000783     0.918852     0.917180   \n",
              "             T3         0.918834   0.001968     0.924968     0.919539   \n",
              "             T4         0.915700   0.001842     0.920854     0.916076   \n",
              "             TM1        0.914134   0.004696     0.926672     0.913217   \n",
              "             TM2        0.911313   0.003835     0.920889     0.910897   \n",
              "             TM3        0.906926   0.004630     0.919563     0.908317   \n",
              "roberta-base T1         0.926982   0.002342     0.934093     0.927217   \n",
              "             T2         0.927922   0.002789     0.935578     0.927755   \n",
              "             T3         0.926669   0.003604     0.937046     0.927069   \n",
              "             T4         0.919774   0.002131     0.926457     0.919622   \n",
              "             TM1        0.912253   0.007798     0.935278     0.913088   \n",
              "             TM2        0.922281   0.002027     0.927559     0.922636   \n",
              "             TM3        0.916327   0.004813     0.929074     0.915541   \n",
              "\n",
              "                                    \n",
              "                      std-f1-score  \n",
              "model        template               \n",
              "Bert         T1           0.004693  \n",
              "             T2           0.002468  \n",
              "             T3           0.002664  \n",
              "             T4           0.003366  \n",
              "             TM1          0.005264  \n",
              "             TM2          0.007874  \n",
              "             TM3          0.003618  \n",
              "Roberta      T1           0.002080  \n",
              "             T2           0.002084  \n",
              "             T3           0.001196  \n",
              "             T4           0.003459  \n",
              "             TM1          0.003262  \n",
              "             TM2          0.010440  \n",
              "             TM3          0.009539  \n",
              "bert-base    T1           0.001613  \n",
              "             T2           0.000650  \n",
              "             T3           0.001974  \n",
              "             T4           0.001753  \n",
              "             TM1          0.005008  \n",
              "             TM2          0.003954  \n",
              "             TM3          0.004084  \n",
              "roberta-base T1           0.002272  \n",
              "             T2           0.002883  \n",
              "             T3           0.003476  \n",
              "             T4           0.002184  \n",
              "             TM1          0.007618  \n",
              "             TM2          0.001802  \n",
              "             TM3          0.005228  \n",
              "\n",
              "[28 rows x 36 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d185c5b7-744e-4d76-83bc-8542e6e99295\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"9\" halign=\"left\">K&amp;H+N</th>\n",
              "      <th>BLESS</th>\n",
              "      <th>...</th>\n",
              "      <th>EVALution</th>\n",
              "      <th colspan=\"9\" halign=\"left\">ROOT09</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>max-precision</th>\n",
              "      <th>min-precision</th>\n",
              "      <th>std-precision</th>\n",
              "      <th>max-recall</th>\n",
              "      <th>min-recall</th>\n",
              "      <th>std-recall</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-precision</th>\n",
              "      <th>...</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-precision</th>\n",
              "      <th>min-precision</th>\n",
              "      <th>std-precision</th>\n",
              "      <th>max-recall</th>\n",
              "      <th>min-recall</th>\n",
              "      <th>std-recall</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.990061</td>\n",
              "      <td>0.987516</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.990123</td>\n",
              "      <td>0.987689</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.990072</td>\n",
              "      <td>0.987522</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.956109</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009210</td>\n",
              "      <td>0.932149</td>\n",
              "      <td>0.917239</td>\n",
              "      <td>0.005031</td>\n",
              "      <td>0.929803</td>\n",
              "      <td>0.917581</td>\n",
              "      <td>0.004543</td>\n",
              "      <td>0.930315</td>\n",
              "      <td>0.917237</td>\n",
              "      <td>0.004693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.990036</td>\n",
              "      <td>0.987008</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>0.990123</td>\n",
              "      <td>0.987132</td>\n",
              "      <td>0.001049</td>\n",
              "      <td>0.990042</td>\n",
              "      <td>0.987037</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.953170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011065</td>\n",
              "      <td>0.932559</td>\n",
              "      <td>0.927252</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>0.931369</td>\n",
              "      <td>0.924475</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>0.931695</td>\n",
              "      <td>0.925530</td>\n",
              "      <td>0.002468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.990883</td>\n",
              "      <td>0.987878</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.990958</td>\n",
              "      <td>0.988036</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.990883</td>\n",
              "      <td>0.987902</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.957277</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005743</td>\n",
              "      <td>0.933409</td>\n",
              "      <td>0.926036</td>\n",
              "      <td>0.002640</td>\n",
              "      <td>0.933563</td>\n",
              "      <td>0.926355</td>\n",
              "      <td>0.002681</td>\n",
              "      <td>0.933420</td>\n",
              "      <td>0.926088</td>\n",
              "      <td>0.002664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.775743</td>\n",
              "      <td>0.674940</td>\n",
              "      <td>0.035524</td>\n",
              "      <td>0.769354</td>\n",
              "      <td>0.483620</td>\n",
              "      <td>0.102657</td>\n",
              "      <td>0.757430</td>\n",
              "      <td>0.345001</td>\n",
              "      <td>0.146788</td>\n",
              "      <td>0.541156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047084</td>\n",
              "      <td>0.935060</td>\n",
              "      <td>0.924590</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.933250</td>\n",
              "      <td>0.923848</td>\n",
              "      <td>0.003275</td>\n",
              "      <td>0.933742</td>\n",
              "      <td>0.924117</td>\n",
              "      <td>0.003366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.989056</td>\n",
              "      <td>0.982043</td>\n",
              "      <td>0.002680</td>\n",
              "      <td>0.989080</td>\n",
              "      <td>0.982263</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>0.989043</td>\n",
              "      <td>0.981875</td>\n",
              "      <td>0.002737</td>\n",
              "      <td>0.943610</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007153</td>\n",
              "      <td>0.931731</td>\n",
              "      <td>0.919070</td>\n",
              "      <td>0.004255</td>\n",
              "      <td>0.931683</td>\n",
              "      <td>0.917581</td>\n",
              "      <td>0.005513</td>\n",
              "      <td>0.931686</td>\n",
              "      <td>0.918737</td>\n",
              "      <td>0.005264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.989949</td>\n",
              "      <td>0.986245</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.990054</td>\n",
              "      <td>0.986367</td>\n",
              "      <td>0.001341</td>\n",
              "      <td>0.989970</td>\n",
              "      <td>0.986266</td>\n",
              "      <td>0.001366</td>\n",
              "      <td>0.949011</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015470</td>\n",
              "      <td>0.933665</td>\n",
              "      <td>0.914036</td>\n",
              "      <td>0.008265</td>\n",
              "      <td>0.932936</td>\n",
              "      <td>0.915074</td>\n",
              "      <td>0.007205</td>\n",
              "      <td>0.933235</td>\n",
              "      <td>0.913595</td>\n",
              "      <td>0.007874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.989805</td>\n",
              "      <td>0.982281</td>\n",
              "      <td>0.002426</td>\n",
              "      <td>0.989914</td>\n",
              "      <td>0.982611</td>\n",
              "      <td>0.002355</td>\n",
              "      <td>0.989794</td>\n",
              "      <td>0.982343</td>\n",
              "      <td>0.002427</td>\n",
              "      <td>0.954229</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018915</td>\n",
              "      <td>0.931012</td>\n",
              "      <td>0.922350</td>\n",
              "      <td>0.003266</td>\n",
              "      <td>0.927609</td>\n",
              "      <td>0.916327</td>\n",
              "      <td>0.003952</td>\n",
              "      <td>0.928325</td>\n",
              "      <td>0.917986</td>\n",
              "      <td>0.003618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.989841</td>\n",
              "      <td>0.986780</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.989775</td>\n",
              "      <td>0.986924</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.989794</td>\n",
              "      <td>0.986825</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.956329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010243</td>\n",
              "      <td>0.940670</td>\n",
              "      <td>0.934449</td>\n",
              "      <td>0.002258</td>\n",
              "      <td>0.938891</td>\n",
              "      <td>0.934190</td>\n",
              "      <td>0.002035</td>\n",
              "      <td>0.938886</td>\n",
              "      <td>0.934528</td>\n",
              "      <td>0.002080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.989835</td>\n",
              "      <td>0.987854</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.989914</td>\n",
              "      <td>0.987897</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.989857</td>\n",
              "      <td>0.987828</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.957248</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009601</td>\n",
              "      <td>0.939915</td>\n",
              "      <td>0.933373</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>0.939517</td>\n",
              "      <td>0.933250</td>\n",
              "      <td>0.002112</td>\n",
              "      <td>0.939588</td>\n",
              "      <td>0.933267</td>\n",
              "      <td>0.002084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.990133</td>\n",
              "      <td>0.986657</td>\n",
              "      <td>0.001273</td>\n",
              "      <td>0.990123</td>\n",
              "      <td>0.986854</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.986724</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.959666</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007716</td>\n",
              "      <td>0.940311</td>\n",
              "      <td>0.936691</td>\n",
              "      <td>0.001280</td>\n",
              "      <td>0.938264</td>\n",
              "      <td>0.935443</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.938763</td>\n",
              "      <td>0.935859</td>\n",
              "      <td>0.001196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.743041</td>\n",
              "      <td>0.260714</td>\n",
              "      <td>0.179172</td>\n",
              "      <td>0.615775</td>\n",
              "      <td>0.046324</td>\n",
              "      <td>0.190253</td>\n",
              "      <td>0.580858</td>\n",
              "      <td>0.035667</td>\n",
              "      <td>0.181140</td>\n",
              "      <td>0.536980</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027860</td>\n",
              "      <td>0.939571</td>\n",
              "      <td>0.930950</td>\n",
              "      <td>0.003201</td>\n",
              "      <td>0.938264</td>\n",
              "      <td>0.929176</td>\n",
              "      <td>0.003586</td>\n",
              "      <td>0.938024</td>\n",
              "      <td>0.929781</td>\n",
              "      <td>0.003459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.989355</td>\n",
              "      <td>0.987437</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.989428</td>\n",
              "      <td>0.987480</td>\n",
              "      <td>0.000652</td>\n",
              "      <td>0.989348</td>\n",
              "      <td>0.987418</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.951622</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016098</td>\n",
              "      <td>0.940061</td>\n",
              "      <td>0.930925</td>\n",
              "      <td>0.003079</td>\n",
              "      <td>0.940144</td>\n",
              "      <td>0.929803</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>0.940101</td>\n",
              "      <td>0.929951</td>\n",
              "      <td>0.003262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.989101</td>\n",
              "      <td>0.986003</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.985880</td>\n",
              "      <td>0.001203</td>\n",
              "      <td>0.989131</td>\n",
              "      <td>0.985923</td>\n",
              "      <td>0.001178</td>\n",
              "      <td>0.952524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006487</td>\n",
              "      <td>0.936819</td>\n",
              "      <td>0.913685</td>\n",
              "      <td>0.008962</td>\n",
              "      <td>0.936070</td>\n",
              "      <td>0.911626</td>\n",
              "      <td>0.009268</td>\n",
              "      <td>0.935949</td>\n",
              "      <td>0.908673</td>\n",
              "      <td>0.010440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.987562</td>\n",
              "      <td>0.983772</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.987619</td>\n",
              "      <td>0.982889</td>\n",
              "      <td>0.001954</td>\n",
              "      <td>0.987564</td>\n",
              "      <td>0.983200</td>\n",
              "      <td>0.001788</td>\n",
              "      <td>0.957729</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031670</td>\n",
              "      <td>0.936363</td>\n",
              "      <td>0.907132</td>\n",
              "      <td>0.010221</td>\n",
              "      <td>0.933250</td>\n",
              "      <td>0.908493</td>\n",
              "      <td>0.008989</td>\n",
              "      <td>0.933150</td>\n",
              "      <td>0.907248</td>\n",
              "      <td>0.009539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.988275</td>\n",
              "      <td>0.986896</td>\n",
              "      <td>0.000539</td>\n",
              "      <td>0.988384</td>\n",
              "      <td>0.986924</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.988305</td>\n",
              "      <td>0.986905</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.945973</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010015</td>\n",
              "      <td>0.927133</td>\n",
              "      <td>0.923645</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.926042</td>\n",
              "      <td>0.921341</td>\n",
              "      <td>0.001768</td>\n",
              "      <td>0.926192</td>\n",
              "      <td>0.922099</td>\n",
              "      <td>0.001613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988046</td>\n",
              "      <td>0.985607</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.988106</td>\n",
              "      <td>0.985672</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.988070</td>\n",
              "      <td>0.985632</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.945103</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009959</td>\n",
              "      <td>0.920412</td>\n",
              "      <td>0.917975</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.918521</td>\n",
              "      <td>0.916641</td>\n",
              "      <td>0.000783</td>\n",
              "      <td>0.918852</td>\n",
              "      <td>0.917180</td>\n",
              "      <td>0.000650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.988825</td>\n",
              "      <td>0.983998</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.988941</td>\n",
              "      <td>0.983585</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0.988859</td>\n",
              "      <td>0.983743</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.946859</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004844</td>\n",
              "      <td>0.926868</td>\n",
              "      <td>0.920097</td>\n",
              "      <td>0.002341</td>\n",
              "      <td>0.924475</td>\n",
              "      <td>0.918834</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.924968</td>\n",
              "      <td>0.919539</td>\n",
              "      <td>0.001974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.664307</td>\n",
              "      <td>0.195018</td>\n",
              "      <td>0.178643</td>\n",
              "      <td>0.492940</td>\n",
              "      <td>0.324616</td>\n",
              "      <td>0.055719</td>\n",
              "      <td>0.418042</td>\n",
              "      <td>0.270556</td>\n",
              "      <td>0.058305</td>\n",
              "      <td>0.461790</td>\n",
              "      <td>...</td>\n",
              "      <td>0.037842</td>\n",
              "      <td>0.921739</td>\n",
              "      <td>0.918282</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>0.920715</td>\n",
              "      <td>0.915700</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.920854</td>\n",
              "      <td>0.916076</td>\n",
              "      <td>0.001753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.988435</td>\n",
              "      <td>0.982377</td>\n",
              "      <td>0.002223</td>\n",
              "      <td>0.988593</td>\n",
              "      <td>0.982333</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.988437</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>0.002217</td>\n",
              "      <td>0.944484</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012299</td>\n",
              "      <td>0.927143</td>\n",
              "      <td>0.913169</td>\n",
              "      <td>0.005176</td>\n",
              "      <td>0.926355</td>\n",
              "      <td>0.914134</td>\n",
              "      <td>0.004696</td>\n",
              "      <td>0.926672</td>\n",
              "      <td>0.913217</td>\n",
              "      <td>0.005008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.988935</td>\n",
              "      <td>0.982502</td>\n",
              "      <td>0.002433</td>\n",
              "      <td>0.989080</td>\n",
              "      <td>0.982889</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.988956</td>\n",
              "      <td>0.982454</td>\n",
              "      <td>0.002492</td>\n",
              "      <td>0.945021</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009803</td>\n",
              "      <td>0.922013</td>\n",
              "      <td>0.910831</td>\n",
              "      <td>0.004343</td>\n",
              "      <td>0.920715</td>\n",
              "      <td>0.911313</td>\n",
              "      <td>0.003835</td>\n",
              "      <td>0.920889</td>\n",
              "      <td>0.910897</td>\n",
              "      <td>0.003954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.987908</td>\n",
              "      <td>0.983026</td>\n",
              "      <td>0.001857</td>\n",
              "      <td>0.988036</td>\n",
              "      <td>0.983307</td>\n",
              "      <td>0.001751</td>\n",
              "      <td>0.987759</td>\n",
              "      <td>0.982582</td>\n",
              "      <td>0.001918</td>\n",
              "      <td>0.945217</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011889</td>\n",
              "      <td>0.920768</td>\n",
              "      <td>0.916316</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.919461</td>\n",
              "      <td>0.906926</td>\n",
              "      <td>0.004630</td>\n",
              "      <td>0.919563</td>\n",
              "      <td>0.908317</td>\n",
              "      <td>0.004084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.987826</td>\n",
              "      <td>0.968645</td>\n",
              "      <td>0.007421</td>\n",
              "      <td>0.987897</td>\n",
              "      <td>0.969882</td>\n",
              "      <td>0.006934</td>\n",
              "      <td>0.987850</td>\n",
              "      <td>0.968813</td>\n",
              "      <td>0.007353</td>\n",
              "      <td>0.952677</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009372</td>\n",
              "      <td>0.934250</td>\n",
              "      <td>0.929058</td>\n",
              "      <td>0.001898</td>\n",
              "      <td>0.934190</td>\n",
              "      <td>0.926982</td>\n",
              "      <td>0.002342</td>\n",
              "      <td>0.934093</td>\n",
              "      <td>0.927217</td>\n",
              "      <td>0.002272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988303</td>\n",
              "      <td>0.986700</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.988384</td>\n",
              "      <td>0.986715</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.988329</td>\n",
              "      <td>0.986694</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.952293</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004836</td>\n",
              "      <td>0.936081</td>\n",
              "      <td>0.927822</td>\n",
              "      <td>0.003073</td>\n",
              "      <td>0.935443</td>\n",
              "      <td>0.927922</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.935578</td>\n",
              "      <td>0.927755</td>\n",
              "      <td>0.002883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.988901</td>\n",
              "      <td>0.986157</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.988941</td>\n",
              "      <td>0.986089</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.988915</td>\n",
              "      <td>0.986108</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.951282</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002267</td>\n",
              "      <td>0.937320</td>\n",
              "      <td>0.928950</td>\n",
              "      <td>0.002946</td>\n",
              "      <td>0.937010</td>\n",
              "      <td>0.926669</td>\n",
              "      <td>0.003604</td>\n",
              "      <td>0.937046</td>\n",
              "      <td>0.927069</td>\n",
              "      <td>0.003476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.664321</td>\n",
              "      <td>0.645879</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>0.476386</td>\n",
              "      <td>0.441678</td>\n",
              "      <td>0.016612</td>\n",
              "      <td>0.342407</td>\n",
              "      <td>0.270708</td>\n",
              "      <td>0.034454</td>\n",
              "      <td>0.583433</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013159</td>\n",
              "      <td>0.927172</td>\n",
              "      <td>0.919651</td>\n",
              "      <td>0.002396</td>\n",
              "      <td>0.926355</td>\n",
              "      <td>0.919774</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.926457</td>\n",
              "      <td>0.919622</td>\n",
              "      <td>0.002184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.989165</td>\n",
              "      <td>0.983657</td>\n",
              "      <td>0.002170</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.982472</td>\n",
              "      <td>0.002649</td>\n",
              "      <td>0.989172</td>\n",
              "      <td>0.983125</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>0.944738</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012418</td>\n",
              "      <td>0.935865</td>\n",
              "      <td>0.919793</td>\n",
              "      <td>0.005838</td>\n",
              "      <td>0.935130</td>\n",
              "      <td>0.912253</td>\n",
              "      <td>0.007798</td>\n",
              "      <td>0.935278</td>\n",
              "      <td>0.913088</td>\n",
              "      <td>0.007618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.986707</td>\n",
              "      <td>0.978421</td>\n",
              "      <td>0.003474</td>\n",
              "      <td>0.986784</td>\n",
              "      <td>0.979064</td>\n",
              "      <td>0.003248</td>\n",
              "      <td>0.986715</td>\n",
              "      <td>0.978287</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.952338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.927513</td>\n",
              "      <td>0.923833</td>\n",
              "      <td>0.001390</td>\n",
              "      <td>0.928236</td>\n",
              "      <td>0.922281</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.927559</td>\n",
              "      <td>0.922636</td>\n",
              "      <td>0.001802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.988230</td>\n",
              "      <td>0.983090</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>0.988245</td>\n",
              "      <td>0.983376</td>\n",
              "      <td>0.001797</td>\n",
              "      <td>0.988224</td>\n",
              "      <td>0.982965</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.949587</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023671</td>\n",
              "      <td>0.930174</td>\n",
              "      <td>0.915562</td>\n",
              "      <td>0.005576</td>\n",
              "      <td>0.929489</td>\n",
              "      <td>0.916327</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>0.929074</td>\n",
              "      <td>0.915541</td>\n",
              "      <td>0.005228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28 rows × 36 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d185c5b7-744e-4d76-83bc-8542e6e99295')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d185c5b7-744e-4d76-83bc-8542e6e99295 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d185c5b7-744e-4d76-83bc-8542e6e99295');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Graded LE: Hyperlex**\n",
        "It is reported the Spearman correlation between the median human rates contained in Hyperlex dataset and:\n",
        " - The calculated score by means of the logits (reported results in the paper).\n",
        " - The score considering only the probability to be an hyponym.\n",
        " - Instead of using the logits, the calculated probabilities for each label are used, except one of them (take into account that the sum of all probabilities is $1$, thus one of the values is useless for fitting a linear model)."
      ],
      "metadata": {
        "id": "WtuJP0UsJjgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hyperlex_list = []\n",
        "for d in ['hyperlex-lexical', 'hyperlex-random']:\n",
        "    df_data_hyperlex_hyp = get_dataframe_results_measure(dict_res, d, 'hyp', ['correlation'])\n",
        "    df_data_hyperlex_hyp.columns = pd.MultiIndex.from_tuples([(d,'hyp-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_hyp)\n",
        "\n",
        "    df_data_hyperlex_logit = get_dataframe_results_measure(dict_res, d, 'spearman_logit', ['correlation'])\n",
        "    df_data_hyperlex_logit.columns = pd.MultiIndex.from_tuples([(d,'logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_logit)\n",
        "\n",
        "    df_data_hyperlex_prob = get_dataframe_results_measure(dict_res, d, 'spearman_prob', ['correlation'])\n",
        "    df_data_hyperlex_prob.columns = pd.MultiIndex.from_tuples([(d,'prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_prob)\n",
        "\n",
        "    df_data_hyperlex_nouns_logit = get_dataframe_results_measure(dict_res, d, 'spearman_nouns_logit', ['correlation'])\n",
        "    df_data_hyperlex_nouns_logit.columns = pd.MultiIndex.from_tuples([(d,'noun-logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_nouns_logit)\n",
        "\n",
        "    df_data_hyperlex_verbs_logit = get_dataframe_results_measure(dict_res, d, 'spearman_verbs_logit', ['correlation'])\n",
        "    df_data_hyperlex_verbs_logit.columns = pd.MultiIndex.from_tuples([(d,'verb-logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_verbs_logit)\n",
        "\n",
        "    df_data_hyperlex_nouns_prob = get_dataframe_results_measure(dict_res, d, 'spearman_nouns_prob', ['correlation'])\n",
        "    df_data_hyperlex_nouns_prob.columns = pd.MultiIndex.from_tuples([(d,'noun-prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_nouns_prob)\n",
        "\n",
        "    df_data_hyperlex_verbs_prob = get_dataframe_results_measure(dict_res, d, 'spearman_verbs_prob', ['correlation'])\n",
        "    df_data_hyperlex_verbs_prob.columns = pd.MultiIndex.from_tuples([(d,'verb-prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_verbs_prob)\n",
        "    \n",
        "\n",
        "df_data_hyperlex = pd.concat(df_hyperlex_list, axis=1)\n",
        "df_data_hyperlex.sort_index(level=['model','template'], inplace=True)\n",
        "df_data_hyperlex"
      ],
      "metadata": {
        "id": "1gF2TlmAJpW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "e0c377e3-ad48-4b3e-88bf-aab3113dcb07"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      hyperlex-lexical                                     \\\n",
              "                       hyp-correlation logit-correlation prob-correlation   \n",
              "model        template                                                       \n",
              "Bert         T1               0.657046          0.686073         0.694415   \n",
              "             T2               0.450562          0.402034         0.416582   \n",
              "             T3               0.667674          0.746582         0.755574   \n",
              "             TM1              0.671808          0.766412         0.753963   \n",
              "             TM2              0.589105          0.656697         0.672861   \n",
              "             TM3              0.686035          0.741279         0.755081   \n",
              "Roberta      T1               0.738920          0.754927         0.771282   \n",
              "             T2               0.375061          0.286757         0.297980   \n",
              "             T3               0.636995          0.669128         0.671147   \n",
              "             TM1              0.736504          0.788514         0.796706   \n",
              "             TM2              0.600176          0.653700         0.669882   \n",
              "             TM3              0.721263          0.794234         0.783369   \n",
              "bert-base    T1               0.498647          0.471335         0.474081   \n",
              "             T2               0.406193          0.374065         0.378994   \n",
              "             T3               0.588330          0.614221         0.622301   \n",
              "             TM1              0.569934          0.597293         0.599501   \n",
              "             TM2              0.561259          0.574906         0.610851   \n",
              "             TM3              0.573250          0.583948         0.579535   \n",
              "roberta-base T1               0.651845          0.677077         0.725030   \n",
              "             T2               0.483763          0.406815         0.400610   \n",
              "             T3               0.641070          0.625525         0.664061   \n",
              "             TM1              0.690944          0.736169         0.776089   \n",
              "             TM2              0.641001          0.710913         0.725829   \n",
              "             TM3              0.697356          0.757284         0.770732   \n",
              "\n",
              "                                                                     \\\n",
              "                      noun-logit-correlation verb-logit-correlation   \n",
              "model        template                                                 \n",
              "Bert         T1                     0.737330               0.498642   \n",
              "             T2                     0.433256               0.285916   \n",
              "             T3                     0.781277               0.622566   \n",
              "             TM1                    0.807058               0.672470   \n",
              "             TM2                    0.716757               0.477823   \n",
              "             TM3                    0.781051               0.632722   \n",
              "Roberta      T1                     0.787916               0.531692   \n",
              "             T2                     0.350415               0.063102   \n",
              "             T3                     0.690485               0.515506   \n",
              "             TM1                    0.836832               0.612222   \n",
              "             TM2                    0.705038               0.417199   \n",
              "             TM3                    0.828178               0.656274   \n",
              "bert-base    T1                     0.556778               0.172960   \n",
              "             T2                     0.445601               0.116163   \n",
              "             T3                     0.690729               0.311820   \n",
              "             TM1                    0.680286               0.379775   \n",
              "             TM2                    0.656103               0.276512   \n",
              "             TM3                    0.664762               0.355509   \n",
              "roberta-base T1                     0.713328               0.542603   \n",
              "             T2                     0.482727               0.166631   \n",
              "             T3                     0.692522               0.391105   \n",
              "             TM1                    0.799714               0.552940   \n",
              "             TM2                    0.756986               0.525289   \n",
              "             TM3                    0.806950               0.633745   \n",
              "\n",
              "                                                                   \\\n",
              "                      noun-prob-correlation verb-prob-correlation   \n",
              "model        template                                               \n",
              "Bert         T1                    0.750145              0.459302   \n",
              "             T2                    0.456257              0.248895   \n",
              "             T3                    0.795455              0.597384   \n",
              "             TM1                   0.794629              0.609317   \n",
              "             TM2                   0.728138              0.481895   \n",
              "             TM3                   0.793492              0.609348   \n",
              "Roberta      T1                    0.808948              0.517498   \n",
              "             T2                    0.374889             -0.008955   \n",
              "             T3                    0.699184              0.485816   \n",
              "             TM1                   0.829112              0.618363   \n",
              "             TM2                   0.715934              0.410055   \n",
              "             TM3                   0.827253              0.600486   \n",
              "bert-base    T1                    0.551468              0.137220   \n",
              "             T2                    0.442557              0.126524   \n",
              "             T3                    0.713881              0.309381   \n",
              "             TM1                   0.673157              0.267178   \n",
              "             TM2                   0.685900              0.266175   \n",
              "             TM3                   0.647579              0.316415   \n",
              "roberta-base T1                    0.764511              0.547946   \n",
              "             T2                    0.475401              0.085916   \n",
              "             T3                    0.725598              0.441986   \n",
              "             TM1                   0.819748              0.549569   \n",
              "             TM2                   0.778836              0.442391   \n",
              "             TM3                   0.808939              0.639037   \n",
              "\n",
              "                      hyperlex-random                                     \\\n",
              "                      hyp-correlation logit-correlation prob-correlation   \n",
              "model        template                                                      \n",
              "Bert         T1              0.623897          0.643511         0.668539   \n",
              "             T2              0.540876          0.577157         0.603452   \n",
              "             T3              0.650930          0.727820         0.757310   \n",
              "             TM1             0.721062          0.800457         0.790645   \n",
              "             TM2             0.703471          0.778105         0.784275   \n",
              "             TM3             0.737478          0.794417         0.799238   \n",
              "Roberta      T1              0.708957          0.740859         0.757459   \n",
              "             T2              0.096328          0.152353         0.146603   \n",
              "             T3              0.712420          0.773941         0.794968   \n",
              "             TM1             0.779260          0.828275         0.828471   \n",
              "             TM2             0.627888          0.749403         0.743976   \n",
              "             TM3             0.774364          0.814334         0.791946   \n",
              "bert-base    T1              0.627959          0.642563         0.669358   \n",
              "             T2              0.609193          0.625754         0.626871   \n",
              "             T3              0.605028          0.638078         0.684352   \n",
              "             TM1             0.663666          0.718852         0.714413   \n",
              "             TM2             0.660960          0.706902         0.708351   \n",
              "             TM3             0.655755          0.685267         0.694070   \n",
              "roberta-base T1              0.683489          0.737337         0.743523   \n",
              "             T2              0.608671          0.651969         0.634775   \n",
              "             T3              0.705194          0.742221         0.772627   \n",
              "             TM1             0.719304          0.795759         0.792388   \n",
              "             TM2             0.717575          0.780942         0.786090   \n",
              "             TM3             0.726303          0.782770         0.776317   \n",
              "\n",
              "                                                                     \\\n",
              "                      noun-logit-correlation verb-logit-correlation   \n",
              "model        template                                                 \n",
              "Bert         T1                     0.653573               0.524790   \n",
              "             T2                     0.585964               0.431753   \n",
              "             T3                     0.741746               0.550663   \n",
              "             TM1                    0.822060               0.576625   \n",
              "             TM2                    0.803631               0.553331   \n",
              "             TM3                    0.816571               0.577981   \n",
              "Roberta      T1                     0.752877               0.583824   \n",
              "             T2                     0.169537               0.030114   \n",
              "             T3                     0.789695               0.630920   \n",
              "             TM1                    0.839351               0.716252   \n",
              "             TM2                    0.761168               0.645773   \n",
              "             TM3                    0.830364               0.682727   \n",
              "bert-base    T1                     0.666371               0.426226   \n",
              "             T2                     0.657132               0.305781   \n",
              "             T3                     0.669079               0.374907   \n",
              "             TM1                    0.746955               0.427838   \n",
              "             TM2                    0.743039               0.366064   \n",
              "             TM3                    0.716521               0.416953   \n",
              "roberta-base T1                     0.749482               0.594251   \n",
              "             T2                     0.682594               0.376505   \n",
              "             T3                     0.756931               0.636962   \n",
              "             TM1                    0.811107               0.639123   \n",
              "             TM2                    0.793191               0.663757   \n",
              "             TM3                    0.795358               0.635163   \n",
              "\n",
              "                                                                   \n",
              "                      noun-prob-correlation verb-prob-correlation  \n",
              "model        template                                              \n",
              "Bert         T1                    0.686033              0.490613  \n",
              "             T2                    0.623506              0.391794  \n",
              "             T3                    0.784944              0.482366  \n",
              "             TM1                   0.817262              0.527390  \n",
              "             TM2                   0.811488              0.538746  \n",
              "             TM3                   0.832978              0.525753  \n",
              "Roberta      T1                    0.779921              0.544862  \n",
              "             T2                    0.165167              0.009852  \n",
              "             T3                    0.815623              0.588373  \n",
              "             TM1                   0.843571              0.672157  \n",
              "             TM2                   0.757051              0.607138  \n",
              "             TM3                   0.808761              0.618838  \n",
              "bert-base    T1                    0.691809              0.427355  \n",
              "             T2                    0.658178              0.325007  \n",
              "             T3                    0.723588              0.347461  \n",
              "             TM1                   0.752636              0.356874  \n",
              "             TM2                   0.758504              0.230655  \n",
              "             TM3                   0.721807              0.436931  \n",
              "roberta-base T1                    0.757180              0.566824  \n",
              "             T2                    0.660551              0.391212  \n",
              "             T3                    0.787753              0.599196  \n",
              "             TM1                   0.820118              0.546569  \n",
              "             TM2                   0.808391              0.560856  \n",
              "             TM3                   0.796483              0.556385  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86cc4f49-9b9a-4a81-8995-91f0b503ee0c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"7\" halign=\"left\">hyperlex-lexical</th>\n",
              "      <th colspan=\"7\" halign=\"left\">hyperlex-random</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>hyp-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>prob-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>noun-prob-correlation</th>\n",
              "      <th>verb-prob-correlation</th>\n",
              "      <th>hyp-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>prob-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>noun-prob-correlation</th>\n",
              "      <th>verb-prob-correlation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.657046</td>\n",
              "      <td>0.686073</td>\n",
              "      <td>0.694415</td>\n",
              "      <td>0.737330</td>\n",
              "      <td>0.498642</td>\n",
              "      <td>0.750145</td>\n",
              "      <td>0.459302</td>\n",
              "      <td>0.623897</td>\n",
              "      <td>0.643511</td>\n",
              "      <td>0.668539</td>\n",
              "      <td>0.653573</td>\n",
              "      <td>0.524790</td>\n",
              "      <td>0.686033</td>\n",
              "      <td>0.490613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.450562</td>\n",
              "      <td>0.402034</td>\n",
              "      <td>0.416582</td>\n",
              "      <td>0.433256</td>\n",
              "      <td>0.285916</td>\n",
              "      <td>0.456257</td>\n",
              "      <td>0.248895</td>\n",
              "      <td>0.540876</td>\n",
              "      <td>0.577157</td>\n",
              "      <td>0.603452</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.431753</td>\n",
              "      <td>0.623506</td>\n",
              "      <td>0.391794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.667674</td>\n",
              "      <td>0.746582</td>\n",
              "      <td>0.755574</td>\n",
              "      <td>0.781277</td>\n",
              "      <td>0.622566</td>\n",
              "      <td>0.795455</td>\n",
              "      <td>0.597384</td>\n",
              "      <td>0.650930</td>\n",
              "      <td>0.727820</td>\n",
              "      <td>0.757310</td>\n",
              "      <td>0.741746</td>\n",
              "      <td>0.550663</td>\n",
              "      <td>0.784944</td>\n",
              "      <td>0.482366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.671808</td>\n",
              "      <td>0.766412</td>\n",
              "      <td>0.753963</td>\n",
              "      <td>0.807058</td>\n",
              "      <td>0.672470</td>\n",
              "      <td>0.794629</td>\n",
              "      <td>0.609317</td>\n",
              "      <td>0.721062</td>\n",
              "      <td>0.800457</td>\n",
              "      <td>0.790645</td>\n",
              "      <td>0.822060</td>\n",
              "      <td>0.576625</td>\n",
              "      <td>0.817262</td>\n",
              "      <td>0.527390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.589105</td>\n",
              "      <td>0.656697</td>\n",
              "      <td>0.672861</td>\n",
              "      <td>0.716757</td>\n",
              "      <td>0.477823</td>\n",
              "      <td>0.728138</td>\n",
              "      <td>0.481895</td>\n",
              "      <td>0.703471</td>\n",
              "      <td>0.778105</td>\n",
              "      <td>0.784275</td>\n",
              "      <td>0.803631</td>\n",
              "      <td>0.553331</td>\n",
              "      <td>0.811488</td>\n",
              "      <td>0.538746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.686035</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.755081</td>\n",
              "      <td>0.781051</td>\n",
              "      <td>0.632722</td>\n",
              "      <td>0.793492</td>\n",
              "      <td>0.609348</td>\n",
              "      <td>0.737478</td>\n",
              "      <td>0.794417</td>\n",
              "      <td>0.799238</td>\n",
              "      <td>0.816571</td>\n",
              "      <td>0.577981</td>\n",
              "      <td>0.832978</td>\n",
              "      <td>0.525753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.738920</td>\n",
              "      <td>0.754927</td>\n",
              "      <td>0.771282</td>\n",
              "      <td>0.787916</td>\n",
              "      <td>0.531692</td>\n",
              "      <td>0.808948</td>\n",
              "      <td>0.517498</td>\n",
              "      <td>0.708957</td>\n",
              "      <td>0.740859</td>\n",
              "      <td>0.757459</td>\n",
              "      <td>0.752877</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.779921</td>\n",
              "      <td>0.544862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.375061</td>\n",
              "      <td>0.286757</td>\n",
              "      <td>0.297980</td>\n",
              "      <td>0.350415</td>\n",
              "      <td>0.063102</td>\n",
              "      <td>0.374889</td>\n",
              "      <td>-0.008955</td>\n",
              "      <td>0.096328</td>\n",
              "      <td>0.152353</td>\n",
              "      <td>0.146603</td>\n",
              "      <td>0.169537</td>\n",
              "      <td>0.030114</td>\n",
              "      <td>0.165167</td>\n",
              "      <td>0.009852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.636995</td>\n",
              "      <td>0.669128</td>\n",
              "      <td>0.671147</td>\n",
              "      <td>0.690485</td>\n",
              "      <td>0.515506</td>\n",
              "      <td>0.699184</td>\n",
              "      <td>0.485816</td>\n",
              "      <td>0.712420</td>\n",
              "      <td>0.773941</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.789695</td>\n",
              "      <td>0.630920</td>\n",
              "      <td>0.815623</td>\n",
              "      <td>0.588373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.736504</td>\n",
              "      <td>0.788514</td>\n",
              "      <td>0.796706</td>\n",
              "      <td>0.836832</td>\n",
              "      <td>0.612222</td>\n",
              "      <td>0.829112</td>\n",
              "      <td>0.618363</td>\n",
              "      <td>0.779260</td>\n",
              "      <td>0.828275</td>\n",
              "      <td>0.828471</td>\n",
              "      <td>0.839351</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.843571</td>\n",
              "      <td>0.672157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.600176</td>\n",
              "      <td>0.653700</td>\n",
              "      <td>0.669882</td>\n",
              "      <td>0.705038</td>\n",
              "      <td>0.417199</td>\n",
              "      <td>0.715934</td>\n",
              "      <td>0.410055</td>\n",
              "      <td>0.627888</td>\n",
              "      <td>0.749403</td>\n",
              "      <td>0.743976</td>\n",
              "      <td>0.761168</td>\n",
              "      <td>0.645773</td>\n",
              "      <td>0.757051</td>\n",
              "      <td>0.607138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.721263</td>\n",
              "      <td>0.794234</td>\n",
              "      <td>0.783369</td>\n",
              "      <td>0.828178</td>\n",
              "      <td>0.656274</td>\n",
              "      <td>0.827253</td>\n",
              "      <td>0.600486</td>\n",
              "      <td>0.774364</td>\n",
              "      <td>0.814334</td>\n",
              "      <td>0.791946</td>\n",
              "      <td>0.830364</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>0.808761</td>\n",
              "      <td>0.618838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.498647</td>\n",
              "      <td>0.471335</td>\n",
              "      <td>0.474081</td>\n",
              "      <td>0.556778</td>\n",
              "      <td>0.172960</td>\n",
              "      <td>0.551468</td>\n",
              "      <td>0.137220</td>\n",
              "      <td>0.627959</td>\n",
              "      <td>0.642563</td>\n",
              "      <td>0.669358</td>\n",
              "      <td>0.666371</td>\n",
              "      <td>0.426226</td>\n",
              "      <td>0.691809</td>\n",
              "      <td>0.427355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.406193</td>\n",
              "      <td>0.374065</td>\n",
              "      <td>0.378994</td>\n",
              "      <td>0.445601</td>\n",
              "      <td>0.116163</td>\n",
              "      <td>0.442557</td>\n",
              "      <td>0.126524</td>\n",
              "      <td>0.609193</td>\n",
              "      <td>0.625754</td>\n",
              "      <td>0.626871</td>\n",
              "      <td>0.657132</td>\n",
              "      <td>0.305781</td>\n",
              "      <td>0.658178</td>\n",
              "      <td>0.325007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.588330</td>\n",
              "      <td>0.614221</td>\n",
              "      <td>0.622301</td>\n",
              "      <td>0.690729</td>\n",
              "      <td>0.311820</td>\n",
              "      <td>0.713881</td>\n",
              "      <td>0.309381</td>\n",
              "      <td>0.605028</td>\n",
              "      <td>0.638078</td>\n",
              "      <td>0.684352</td>\n",
              "      <td>0.669079</td>\n",
              "      <td>0.374907</td>\n",
              "      <td>0.723588</td>\n",
              "      <td>0.347461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.569934</td>\n",
              "      <td>0.597293</td>\n",
              "      <td>0.599501</td>\n",
              "      <td>0.680286</td>\n",
              "      <td>0.379775</td>\n",
              "      <td>0.673157</td>\n",
              "      <td>0.267178</td>\n",
              "      <td>0.663666</td>\n",
              "      <td>0.718852</td>\n",
              "      <td>0.714413</td>\n",
              "      <td>0.746955</td>\n",
              "      <td>0.427838</td>\n",
              "      <td>0.752636</td>\n",
              "      <td>0.356874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.561259</td>\n",
              "      <td>0.574906</td>\n",
              "      <td>0.610851</td>\n",
              "      <td>0.656103</td>\n",
              "      <td>0.276512</td>\n",
              "      <td>0.685900</td>\n",
              "      <td>0.266175</td>\n",
              "      <td>0.660960</td>\n",
              "      <td>0.706902</td>\n",
              "      <td>0.708351</td>\n",
              "      <td>0.743039</td>\n",
              "      <td>0.366064</td>\n",
              "      <td>0.758504</td>\n",
              "      <td>0.230655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.573250</td>\n",
              "      <td>0.583948</td>\n",
              "      <td>0.579535</td>\n",
              "      <td>0.664762</td>\n",
              "      <td>0.355509</td>\n",
              "      <td>0.647579</td>\n",
              "      <td>0.316415</td>\n",
              "      <td>0.655755</td>\n",
              "      <td>0.685267</td>\n",
              "      <td>0.694070</td>\n",
              "      <td>0.716521</td>\n",
              "      <td>0.416953</td>\n",
              "      <td>0.721807</td>\n",
              "      <td>0.436931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.651845</td>\n",
              "      <td>0.677077</td>\n",
              "      <td>0.725030</td>\n",
              "      <td>0.713328</td>\n",
              "      <td>0.542603</td>\n",
              "      <td>0.764511</td>\n",
              "      <td>0.547946</td>\n",
              "      <td>0.683489</td>\n",
              "      <td>0.737337</td>\n",
              "      <td>0.743523</td>\n",
              "      <td>0.749482</td>\n",
              "      <td>0.594251</td>\n",
              "      <td>0.757180</td>\n",
              "      <td>0.566824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.483763</td>\n",
              "      <td>0.406815</td>\n",
              "      <td>0.400610</td>\n",
              "      <td>0.482727</td>\n",
              "      <td>0.166631</td>\n",
              "      <td>0.475401</td>\n",
              "      <td>0.085916</td>\n",
              "      <td>0.608671</td>\n",
              "      <td>0.651969</td>\n",
              "      <td>0.634775</td>\n",
              "      <td>0.682594</td>\n",
              "      <td>0.376505</td>\n",
              "      <td>0.660551</td>\n",
              "      <td>0.391212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.641070</td>\n",
              "      <td>0.625525</td>\n",
              "      <td>0.664061</td>\n",
              "      <td>0.692522</td>\n",
              "      <td>0.391105</td>\n",
              "      <td>0.725598</td>\n",
              "      <td>0.441986</td>\n",
              "      <td>0.705194</td>\n",
              "      <td>0.742221</td>\n",
              "      <td>0.772627</td>\n",
              "      <td>0.756931</td>\n",
              "      <td>0.636962</td>\n",
              "      <td>0.787753</td>\n",
              "      <td>0.599196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.690944</td>\n",
              "      <td>0.736169</td>\n",
              "      <td>0.776089</td>\n",
              "      <td>0.799714</td>\n",
              "      <td>0.552940</td>\n",
              "      <td>0.819748</td>\n",
              "      <td>0.549569</td>\n",
              "      <td>0.719304</td>\n",
              "      <td>0.795759</td>\n",
              "      <td>0.792388</td>\n",
              "      <td>0.811107</td>\n",
              "      <td>0.639123</td>\n",
              "      <td>0.820118</td>\n",
              "      <td>0.546569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.641001</td>\n",
              "      <td>0.710913</td>\n",
              "      <td>0.725829</td>\n",
              "      <td>0.756986</td>\n",
              "      <td>0.525289</td>\n",
              "      <td>0.778836</td>\n",
              "      <td>0.442391</td>\n",
              "      <td>0.717575</td>\n",
              "      <td>0.780942</td>\n",
              "      <td>0.786090</td>\n",
              "      <td>0.793191</td>\n",
              "      <td>0.663757</td>\n",
              "      <td>0.808391</td>\n",
              "      <td>0.560856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.697356</td>\n",
              "      <td>0.757284</td>\n",
              "      <td>0.770732</td>\n",
              "      <td>0.806950</td>\n",
              "      <td>0.633745</td>\n",
              "      <td>0.808939</td>\n",
              "      <td>0.639037</td>\n",
              "      <td>0.726303</td>\n",
              "      <td>0.782770</td>\n",
              "      <td>0.776317</td>\n",
              "      <td>0.795358</td>\n",
              "      <td>0.635163</td>\n",
              "      <td>0.796483</td>\n",
              "      <td>0.556385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86cc4f49-9b9a-4a81-8995-91f0b503ee0c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-86cc4f49-9b9a-4a81-8995-91f0b503ee0c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-86cc4f49-9b9a-4a81-8995-91f0b503ee0c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperlex: Reported results in the paper**"
      ],
      "metadata": {
        "id": "f0xNZFsjH1vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_paper =[(\"hyperlex-random\", 'logit-correlation'),(\"hyperlex-random\", 'noun-logit-correlation'), (\"hyperlex-random\", 'verb-logit-correlation'),\n",
        "                (\"hyperlex-lexical\", 'logit-correlation'),(\"hyperlex-lexical\", 'noun-logit-correlation'), (\"hyperlex-lexical\", 'verb-logit-correlation')] \n",
        "df_res_hyperlex_paper = df_data_hyperlex[columns_paper]\n",
        "df_res_hyperlex_paper\n",
        "df_res_hyperlex_paper.to_csv(\"datos.csv\")"
      ],
      "metadata": {
        "id": "isicaq2yCP5n"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOTAS_FILE_REST = DIR_RESULTS + 'sotas_results_literature/hyperlex_Sotas.txt' \n",
        "df_hyperlex_sotas = pd.read_csv(SOTAS_FILE_REST, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "\n",
        "multi_row_tuples = list(zip(['Sota']*df_hyperlex_sotas.shape[0], list(df_hyperlex_sotas.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "df_hyperlex_sotas.index=multi_row\n",
        "df_res_sotas_hyperlex = pd.concat([df_res_hyperlex_paper,df_hyperlex_sotas])\n",
        "df_res_sotas_hyperlex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6M--RnvGEtN6",
        "outputId": "5b17ba9b-fbdc-44ef-de73-767791f71eb6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        hyperlex-random                         \\\n",
              "                      logit-correlation noun-logit-correlation   \n",
              "model        template                                            \n",
              "Bert         T1                0.643511               0.653573   \n",
              "             T2                0.577157               0.585964   \n",
              "             T3                0.727820               0.741746   \n",
              "             TM1               0.800457                0.82206   \n",
              "             TM2               0.778105               0.803631   \n",
              "             TM3               0.794417               0.816571   \n",
              "Roberta      T1                0.740859               0.752877   \n",
              "             T2                0.152353               0.169537   \n",
              "             T3                0.773941               0.789695   \n",
              "             TM1               0.828275               0.839351   \n",
              "             TM2               0.749403               0.761168   \n",
              "             TM3               0.814334               0.830364   \n",
              "bert-base    T1                0.642563               0.666371   \n",
              "             T2                0.625754               0.657132   \n",
              "             T3                0.638078               0.669079   \n",
              "             TM1               0.718852               0.746955   \n",
              "             TM2               0.706902               0.743039   \n",
              "             TM3               0.685267               0.716521   \n",
              "roberta-base T1                0.737337               0.749482   \n",
              "             T2                0.651969               0.682594   \n",
              "             T3                0.742221               0.756931   \n",
              "             TM1               0.795759               0.811107   \n",
              "             TM2               0.780942               0.793191   \n",
              "             TM3               0.782770               0.795358   \n",
              "Sota         LEAR              0.686000                   0.71   \n",
              "             SDNS              0.692000                     na   \n",
              "             GLEN              0.520000                     na   \n",
              "             POSTLE            0.686000                     na   \n",
              "             LexSub            0.533000                     na   \n",
              "             HF                0.690000                     na   \n",
              "\n",
              "                                              hyperlex-lexical  \\\n",
              "                      verb-logit-correlation logit-correlation   \n",
              "model        template                                            \n",
              "Bert         T1                      0.52479          0.686073   \n",
              "             T2                     0.431753          0.402034   \n",
              "             T3                     0.550663          0.746582   \n",
              "             TM1                    0.576625          0.766412   \n",
              "             TM2                    0.553331          0.656697   \n",
              "             TM3                    0.577981          0.741279   \n",
              "Roberta      T1                     0.583824          0.754927   \n",
              "             T2                     0.030114          0.286757   \n",
              "             T3                      0.63092          0.669128   \n",
              "             TM1                    0.716252          0.788514   \n",
              "             TM2                    0.645773            0.6537   \n",
              "             TM3                    0.682727          0.794234   \n",
              "bert-base    T1                     0.426226          0.471335   \n",
              "             T2                     0.305781          0.374065   \n",
              "             T3                     0.374907          0.614221   \n",
              "             TM1                    0.427838          0.597293   \n",
              "             TM2                    0.366064          0.574906   \n",
              "             TM3                    0.416953          0.583948   \n",
              "roberta-base T1                     0.594251          0.677077   \n",
              "             T2                     0.376505          0.406815   \n",
              "             T3                     0.636962          0.625525   \n",
              "             TM1                    0.639123          0.736169   \n",
              "             TM2                    0.663757          0.710913   \n",
              "             TM3                    0.635163          0.757284   \n",
              "Sota         LEAR                         na             0.174   \n",
              "             SDNS                         na             0.544   \n",
              "             GLEN                         na             0.481   \n",
              "             POSTLE                       na                na   \n",
              "             LexSub                       na                na   \n",
              "             HF                           na                na   \n",
              "\n",
              "                                                                     \n",
              "                      noun-logit-correlation verb-logit-correlation  \n",
              "model        template                                                \n",
              "Bert         T1                      0.73733               0.498642  \n",
              "             T2                     0.433256               0.285916  \n",
              "             T3                     0.781277               0.622566  \n",
              "             TM1                    0.807058                0.67247  \n",
              "             TM2                    0.716757               0.477823  \n",
              "             TM3                    0.781051               0.632722  \n",
              "Roberta      T1                     0.787916               0.531692  \n",
              "             T2                     0.350415               0.063102  \n",
              "             T3                     0.690485               0.515506  \n",
              "             TM1                    0.836832               0.612222  \n",
              "             TM2                    0.705038               0.417199  \n",
              "             TM3                    0.828178               0.656274  \n",
              "bert-base    T1                     0.556778                0.17296  \n",
              "             T2                     0.445601               0.116163  \n",
              "             T3                     0.690729                0.31182  \n",
              "             TM1                    0.680286               0.379775  \n",
              "             TM2                    0.656103               0.276512  \n",
              "             TM3                    0.664762               0.355509  \n",
              "roberta-base T1                     0.713328               0.542603  \n",
              "             T2                     0.482727               0.166631  \n",
              "             T3                     0.692522               0.391105  \n",
              "             TM1                    0.799714                0.55294  \n",
              "             TM2                    0.756986               0.525289  \n",
              "             TM3                     0.80695               0.633745  \n",
              "Sota         LEAR                         na                     na  \n",
              "             SDNS                         na                     na  \n",
              "             GLEN                         na                     na  \n",
              "             POSTLE                     0.60                     na  \n",
              "             LexSub                       na                     na  \n",
              "             HF                           na                     na  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c420100-6555-4c98-b9bd-18313eff7260\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyperlex-random</th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyperlex-lexical</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.643511</td>\n",
              "      <td>0.653573</td>\n",
              "      <td>0.52479</td>\n",
              "      <td>0.686073</td>\n",
              "      <td>0.73733</td>\n",
              "      <td>0.498642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.577157</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.431753</td>\n",
              "      <td>0.402034</td>\n",
              "      <td>0.433256</td>\n",
              "      <td>0.285916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.727820</td>\n",
              "      <td>0.741746</td>\n",
              "      <td>0.550663</td>\n",
              "      <td>0.746582</td>\n",
              "      <td>0.781277</td>\n",
              "      <td>0.622566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.800457</td>\n",
              "      <td>0.82206</td>\n",
              "      <td>0.576625</td>\n",
              "      <td>0.766412</td>\n",
              "      <td>0.807058</td>\n",
              "      <td>0.67247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.778105</td>\n",
              "      <td>0.803631</td>\n",
              "      <td>0.553331</td>\n",
              "      <td>0.656697</td>\n",
              "      <td>0.716757</td>\n",
              "      <td>0.477823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.794417</td>\n",
              "      <td>0.816571</td>\n",
              "      <td>0.577981</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.781051</td>\n",
              "      <td>0.632722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.740859</td>\n",
              "      <td>0.752877</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.754927</td>\n",
              "      <td>0.787916</td>\n",
              "      <td>0.531692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.152353</td>\n",
              "      <td>0.169537</td>\n",
              "      <td>0.030114</td>\n",
              "      <td>0.286757</td>\n",
              "      <td>0.350415</td>\n",
              "      <td>0.063102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.773941</td>\n",
              "      <td>0.789695</td>\n",
              "      <td>0.63092</td>\n",
              "      <td>0.669128</td>\n",
              "      <td>0.690485</td>\n",
              "      <td>0.515506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.828275</td>\n",
              "      <td>0.839351</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.788514</td>\n",
              "      <td>0.836832</td>\n",
              "      <td>0.612222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.749403</td>\n",
              "      <td>0.761168</td>\n",
              "      <td>0.645773</td>\n",
              "      <td>0.6537</td>\n",
              "      <td>0.705038</td>\n",
              "      <td>0.417199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.814334</td>\n",
              "      <td>0.830364</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>0.794234</td>\n",
              "      <td>0.828178</td>\n",
              "      <td>0.656274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.642563</td>\n",
              "      <td>0.666371</td>\n",
              "      <td>0.426226</td>\n",
              "      <td>0.471335</td>\n",
              "      <td>0.556778</td>\n",
              "      <td>0.17296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.625754</td>\n",
              "      <td>0.657132</td>\n",
              "      <td>0.305781</td>\n",
              "      <td>0.374065</td>\n",
              "      <td>0.445601</td>\n",
              "      <td>0.116163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.638078</td>\n",
              "      <td>0.669079</td>\n",
              "      <td>0.374907</td>\n",
              "      <td>0.614221</td>\n",
              "      <td>0.690729</td>\n",
              "      <td>0.31182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.718852</td>\n",
              "      <td>0.746955</td>\n",
              "      <td>0.427838</td>\n",
              "      <td>0.597293</td>\n",
              "      <td>0.680286</td>\n",
              "      <td>0.379775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.706902</td>\n",
              "      <td>0.743039</td>\n",
              "      <td>0.366064</td>\n",
              "      <td>0.574906</td>\n",
              "      <td>0.656103</td>\n",
              "      <td>0.276512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.685267</td>\n",
              "      <td>0.716521</td>\n",
              "      <td>0.416953</td>\n",
              "      <td>0.583948</td>\n",
              "      <td>0.664762</td>\n",
              "      <td>0.355509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.737337</td>\n",
              "      <td>0.749482</td>\n",
              "      <td>0.594251</td>\n",
              "      <td>0.677077</td>\n",
              "      <td>0.713328</td>\n",
              "      <td>0.542603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.651969</td>\n",
              "      <td>0.682594</td>\n",
              "      <td>0.376505</td>\n",
              "      <td>0.406815</td>\n",
              "      <td>0.482727</td>\n",
              "      <td>0.166631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.742221</td>\n",
              "      <td>0.756931</td>\n",
              "      <td>0.636962</td>\n",
              "      <td>0.625525</td>\n",
              "      <td>0.692522</td>\n",
              "      <td>0.391105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.795759</td>\n",
              "      <td>0.811107</td>\n",
              "      <td>0.639123</td>\n",
              "      <td>0.736169</td>\n",
              "      <td>0.799714</td>\n",
              "      <td>0.55294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.780942</td>\n",
              "      <td>0.793191</td>\n",
              "      <td>0.663757</td>\n",
              "      <td>0.710913</td>\n",
              "      <td>0.756986</td>\n",
              "      <td>0.525289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.782770</td>\n",
              "      <td>0.795358</td>\n",
              "      <td>0.635163</td>\n",
              "      <td>0.757284</td>\n",
              "      <td>0.80695</td>\n",
              "      <td>0.633745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Sota</th>\n",
              "      <th>LEAR</th>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.71</td>\n",
              "      <td>na</td>\n",
              "      <td>0.174</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SDNS</th>\n",
              "      <td>0.692000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>0.544</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GLEN</th>\n",
              "      <td>0.520000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>0.481</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>POSTLE</th>\n",
              "      <td>0.686000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>0.60</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LexSub</th>\n",
              "      <td>0.533000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HF</th>\n",
              "      <td>0.690000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c420100-6555-4c98-b9bd-18313eff7260')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4c420100-6555-4c98-b9bd-18313eff7260 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4c420100-6555-4c98-b9bd-18313eff7260');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graphs"
      ],
      "metadata": {
        "id": "qF_nqf4495GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First some plots to visualize the best performing model and template per dataset. The worse performing ones are excluded to obtain better visualization. Used boxplots for visualization to also observe the variability of the 5 iterations conducted per experiment. "
      ],
      "metadata": {
        "id": "864nTZEN4p2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_plot = []\n",
        "model_plot = []\n",
        "template_plot = []\n",
        "dataset_plot = []\n",
        "\n",
        "for dataset in dict_res:\n",
        "  if dataset == 'cogalexv':\n",
        "    for model in dict_res['cogalexv']:\n",
        "      for template in dict_res['cogalexv'][model]:\n",
        "        for i in dict_res['cogalexv'][model][template]['report']:\n",
        "          preds_plot.append(i['weighted f1-score not random'])\n",
        "          template_plot.append(templates2abrev[template]) \n",
        "          model_plot.append(models2abrev[model])\n",
        "          dataset_plot.append(dataset)\n",
        "  else:\n",
        "    for model in dict_res[dataset]:\n",
        "      for template in dict_res[dataset][model]:\n",
        "        for i in dict_res[dataset][model][template]['report']:\n",
        "          preds_plot.append(i['weighted avg']['f1-score'])\n",
        "          template_plot.append(templates2abrev[template]) \n",
        "          model_plot.append(models2abrev[model])\n",
        "          dataset_plot.append(dataset)\n",
        "\n",
        "df_plot = plot = pd.DataFrame([preds_plot, model_plot, template_plot, dataset_plot])\n",
        "df_plot = df_plot.T\n",
        "df_plot.columns = ['preds', 'model', 'template', 'dataset']\n",
        "df_plot['label'] = df_plot.model.str.cat(df_plot.template)"
      ],
      "metadata": {
        "id": "o90jSwSoTXpe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot_prov = df_plot[df_plot['preds']>0.7] #just take weight averages higher than 0.7 \n",
        "#to avoid the ones in which data didnt converge and visualize better. \n",
        "#to see al data, comment above line, and change data below to 'df_plot'\n",
        "sns.boxplot(data = df_plot_prov, x=\"template\", y=\"preds\", hue=\"dataset\")"
      ],
      "metadata": {
        "id": "FG0N6zZCSGDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "23192bb1-e76b-4818-c321-1ac3db2e6ee9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d66faf940>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_plot_prov = df_plot[df_plot['preds']>0.7]\n",
        "sns.boxplot(data = df_plot, x=\"model\", y=\"preds\", hue=\"dataset\")"
      ],
      "metadata": {
        "id": "j9xRT9EUXyhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "e4165199-46e0-4857-df10-d9e7b6185027"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d66065f10>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cogalexv weighed f1 of all categories compared to SOTA"
      ],
      "metadata": {
        "id": "-RapkEHxa6YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfm_cogalexv = df_res_sotas_cogalexv.T\n",
        "dfm_cogalexv = pd.melt(dfm_cogalexv)\n",
        "dfm_cogalexv"
      ],
      "metadata": {
        "id": "jmG3HwxTcTE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "77325a5a-0b40-4fc0-e3b9-7c5daf042940"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    model template     value\n",
              "0    Bert       T1  0.770474\n",
              "1    Bert       T1  0.680452\n",
              "2    Bert       T1  0.715095\n",
              "3    Bert       T1  0.563975\n",
              "4    Bert       T1  0.690274\n",
              "..    ...      ...       ...\n",
              "155  Sota  RelBert  0.794000\n",
              "156  Sota  RelBert  0.616000\n",
              "157  Sota  RelBert  0.702000\n",
              "158  Sota  RelBert  0.505000\n",
              "159  Sota  RelBert  0.664000\n",
              "\n",
              "[160 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ef11144-cab0-4713-b7f4-0208a893513d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.770474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.680452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.715095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.563975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.690274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.505000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ef11144-cab0-4713-b7f4-0208a893513d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0ef11144-cab0-4713-b7f4-0208a893513d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0ef11144-cab0-4713-b7f4-0208a893513d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=dfm_cogalexv, x= 'template', y='value', hue='model')"
      ],
      "metadata": {
        "id": "dbpZZl5CbbtG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "8db02572-685c-49e1-caf6-513384f1cebe"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d69f28f10>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=dfm_cogalexv, x= 'model', y='value', hue='template')"
      ],
      "metadata": {
        "id": "RvWF7kIQfhf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b4ab8bfd-f230-40ed-df8e-f598ceaa8e35"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d69c974c0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Large experiments (the ones with better results and less variability on first sight), with T1(simplest template) compared to SOTA with bar plots. Here variability is not taken into account, just overall f1 weighed score."
      ],
      "metadata": {
        "id": "WGdhfizEf4Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_f1_sota = pd.DataFrame()\n",
        "for dataset in ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']:\n",
        "  df_prov = pd.melt(df_res_sotas[dataset].T)\n",
        "  df_prov['dataset'] = dataset\n",
        "  compare_f1_sota = pd.concat([df_prov, compare_f1_sota])\n",
        "\n"
      ],
      "metadata": {
        "id": "KFE_jBFmoPzN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_f1_sota"
      ],
      "metadata": {
        "id": "9x3ylhKjvODr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "6213b5b4-e164-4b9b-cf73-2ac695261d81"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   model  template     value dataset\n",
              "0   Bert        T1  0.926534  ROOT09\n",
              "1   Bert        T1  0.926418  ROOT09\n",
              "2   Bert        T1  0.926258  ROOT09\n",
              "3   Bert        T2  0.930032  ROOT09\n",
              "4   Bert        T2  0.928549  ROOT09\n",
              "..   ...       ...       ...     ...\n",
              "91  Sota  SphereRE  0.989000   K&H+N\n",
              "92  Sota  SphereRE  0.990000   K&H+N\n",
              "93  Sota   RelBERT       NaN   K&H+N\n",
              "94  Sota   RelBERT       NaN   K&H+N\n",
              "95  Sota   RelBERT  0.949000   K&H+N\n",
              "\n",
              "[384 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c3b241d-d349-4b25-b96f-ca4494bef930\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th>value</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926534</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926418</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926258</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T2</td>\n",
              "      <td>0.930032</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T2</td>\n",
              "      <td>0.928549</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Sota</td>\n",
              "      <td>SphereRE</td>\n",
              "      <td>0.989000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Sota</td>\n",
              "      <td>SphereRE</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>0.949000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>384 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c3b241d-d349-4b25-b96f-ca4494bef930')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c3b241d-d349-4b25-b96f-ca4494bef930 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c3b241d-d349-4b25-b96f-ca4494bef930');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=compare_f1_sota, x= 'model', y='value', hue='dataset')"
      ],
      "metadata": {
        "id": "QAo2ZVWEvVG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "fa397075-e182-48ad-ca37-edb035fd5ec5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d65eab0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=compare_f1_sota, x= 'template', y='value', hue='dataset')"
      ],
      "metadata": {
        "id": "pJDFG80rvkpr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "a733e365-6154-4765-9c5d-d9c7983290e2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f8d65d79370>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}