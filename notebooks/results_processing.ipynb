{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# to download a anonymous github\n",
        "!git clone https://github.com/fedebotu/clone-anonymous-github\n",
        "!pip install -r clone-anonymous-github/requirements.txt\n",
        "\n",
        "# clone our anonymous github\n",
        "%cd /content/clone-anonymous-github/\n",
        "from src.download import download_repo \n",
        "import os\n",
        "\n",
        "config = {'url': 'https://anonymous.4open.science/r/LRC-0C0F/',\n",
        "                    'save_dir': '/content/',\n",
        "                    'max_conns': 256,\n",
        "                    'max_retry': 5}\n",
        "download_repo(config)\n",
        "\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "KRZ1VjIG0qFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9av5dK1mVLTm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Process results LRC**\n",
        "This notebook is devoted to process the report files generated by the scripts used for Lexical Relation Classification (LRC) and graded Lexical Entailment (LE). The intend is to generate a dataframe to visualize the results for our models/templates, and compare with SoTA results. \n",
        "\n",
        "To reproduct the results in the paper with other result files (not in anonymous github), change the variables in a bellow cell that contains:\n",
        "\n",
        "```\n",
        "# folder with all datasets\n",
        "DIR_DATASETS = '/content/LRC-0C0F/datasets/'\n",
        "# folder with the results\n",
        "DIR_RESULTS = '/content/LRC-0C0F/results/'\n",
        "LIST_DIR_RES = [DIR_RESULTS + 'K&H+N/', \n",
        "                DIR_RESULTS + 'BLESS/',\n",
        "                DIR_RESULTS + 'EVALution/', \n",
        "                DIR_RESULTS + 'ROOT09/',\n",
        "                DIR_RESULTS + 'CogALexV/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_lexical_split/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_random_split/'\n",
        "                ]\n",
        "```\n",
        "\n",
        "###**LRC task: BLESS, K&H+N, ROOT9, EVALution and CogALexV datasets**\n",
        "\n",
        "Lexical relation classification is the task of predict the lexical relation among two words, such as *(horse,animal) - hyponymy*.\n",
        "\n",
        "####**Models in the literature: baselines**\n",
        "The reported results in the literature for BLESS, K&H+N, ROOT9 and EVALution datasets are precision, recall and f1-score, weighted by the support of the labels. We have collected the results for the following models:\n",
        " - [LexNET](https://aclanthology.org/W16-5304/) (2016)\n",
        " - [SphereRE](https://aclanthology.org/P19-1169/) (2019)\n",
        " - [KEML](https://arxiv.org/abs/2002.10903) (2020)\n",
        " - [RelBERT](https://arxiv.org/abs/2110.15705) (2021)\n",
        "\n",
        "As far as our knowneldge, these are the last SoTA models. \n",
        "\n",
        "The reported results for CogALexV dataset are different. This dataset comes from the [CogALexV shared task (subtask 2)](https://sites.google.com/site/cogalex2016/home/shared-task) and it is well defined what results must be reported. In this dataset, there are five labels, including \"RANDOM\". The report results are the f1-scores for the four labels that are not the \"RANDOM\" one, and the weighted f1-score by the support of these four labels. \n",
        "\n",
        "####**How the baseline results are collected**\n",
        "We collect the results from the regarding papers.\n",
        "The reported results in RelBERT paper do not contain precision and recall values, only micro and macro f1 average, i.e., the accuracy (micro) and the average of the f1-scores (macro) for all labels, repectively.  We have calculated the weighted f1-scores for RelBERT with the data (table 4) in the paper. \n",
        "\n",
        "\n",
        "####**Our proposal for LRC**\n",
        "We fine-tune a pretrained language model (as BERT or RoBERTa) feeding the model with a verbalization of the two words by means of a template (see in a bellow cell the variables `models2abrev` and `templates2abrev` for the chosen models and templates). We need the following components: \n",
        "1. A pretrained language model (PTLM) $M$ and its token vocabulary $V_M$;\n",
        "2. A training set ${\\cal T}=\\{(\\boldsymbol{w}_i, y_i)\\mid i=1,\\dots n\\}$, where $\\boldsymbol{w}_i = (w^1_i, w^2_i)$ is a pair of words and $y_i\\in Y$ is the label of a lexical relation ($|Y|=K$);\n",
        "3. An injective function from the set of labels to the vocabulary of tokens $V_M$, $v\\colon Y\\to V_M$, called the *mask verbalizer* function;\n",
        "4. A training and a testing template, $T_t$ and $T_e$, used to verbalize $\\boldsymbol{w}_i$. \n",
        "In this context, a template $T$ is a function, $T\\colon V\\times V \\to {\\cal S}$, from pairs of the word vocabulary to the set of sentences where the `CLS`, `SEP` and `MASK` special tokens of the PTLM can appear in the sentence. We denote by $T(\\boldsymbol{w})_{C}$ and $T(\\boldsymbol{w})_{M}$ to the `CLS` and `MASK` tokens in the sentence $T(\\boldsymbol{w})$, respectively.\n",
        "\n",
        "Depending on the template used, we adopt one of the following two training objectives: \n",
        "1. It the train template does not contain the `MASK` token, the classification objective estimates the probability $P(Y=y_j| T_t(\\boldsymbol{w}_i)_C)$; \n",
        "2. It the train template contains the `MASK` token, a mask prediction objective is used to estimate $P(T_t(\\boldsymbol{w}_i)_{M}=t_j| T_t(\\boldsymbol{w}_i) )$, where $t_j \\in V_M$ is any token in the vocabulary of the PTLM.\n",
        "\n",
        "At inference time, for a model trained with a classification objective, we use the testing template $T_e$ to predict the label with \n",
        "$argmax_{y_i\\in Y} \\{P(Y=y_i| T_e(\\boldsymbol{w})_C)\\}$, and for the mask objective, $argmax_{y_i\\in Y} \\{P(T_e(\\boldsymbol{w})_{M}=v(y_j)|T_e(\\boldsymbol{w}))\\}$. For this latter case, note that at inference time, we only use the tokens given by the mask verbalizer function $v$.\n",
        "\n",
        "###**Graded lexical entailment task: Hyperlex dataset**\n",
        "Graded Lexical Entailment (graded LE) is a regression task. It consists of giving a score for the hyperonym relationship between two words $(w_1,w_2)$ expressing a degree that \"*$w_1$ is a hyponym of $w_2$*\". [Hyperlex dataset](https://arxiv.org/pdf/1608.02117v2.pdf) contains pairs of words with a score between 0-6. The score is the median of the ratings given by at least $10$ human annotators to the question \"*To what degree is $w1$ a type of $w2$?*\". The inter-annotator agreement (IAA) was calculated with the average Spearman correlation $\\rho$ of a human rater with the average of all the other raters. It was obtained IAA $\\rho = 0.864$. This is considered an \"*upper bound*\" for the performance of automatic systems. Thus, the task is to give a score for all pairs in the dataset and report the Spearman correlation between the median human annotator scores and the calculated ones.\n",
        "\n",
        "The pairs in the dataset were obtained from WordNet. It also annotated the lexical relation in WordNet and the POS. The pairs are nouns and verbs. The annotated lexical relations are:\n",
        "- `hyp-i` where $1\\le$ `i` $\\le 4$: Using the `isA` hierarchy, `hyp-i` means that the word $w_1$ is an hyponym of degree `i` of the word $w_2$. If `i`$=4$, means a degree greater or equal than $4$.\n",
        "- `r-hyp-i` where $1\\le$ `i` $\\le 4$: Same as `hyp-i`, but $w_1$ is a hyperonym of degree `i` of $w_2$.\n",
        "- `syn`, `ant`, `mero`, `cohyp` and `no-rel`: Words are synonyms, antonyms, meronyms, cohyponyms or non the above relations, respectively.\n",
        "\n",
        "\n",
        "The dataset is provided in three configurations:\n",
        "- *all pairs*: It contains all pairs (2616 pairs). It is usually used by unsupervised models.\n",
        "- *random split*: All pairs are randomly divided in train (1831 pairs), validation (130) and test (655) datasets. To train supervised models.\n",
        "- *lexical split*: It is also splitted into train/val/test datasets, but to avoid lexical memorization, there are not words in common between the train+val and test splits. To force this lexical split, the total pairs are reduced to 1133/85/269.\n",
        "\n",
        "####**Models in the literature: baselines**\n",
        "\n",
        "The visited models are: [HyperVec](https://arxiv.org/pdf/1707.07273.pdf) (2017), [Poincar√© embeddings](https://arxiv.org/pdf/1705.08039v2.pdf) (2017), [LEAR](https://aclanthology.org/N18-1103.pdf) (2018), [SDNS](https://arxiv.org/pdf/1805.09355v1.pdf) (2018), [GLEN](https://aclanthology.org/P19-1476.pdf) (2019), [POSTLE](https://aclanthology.org/W19-4310.pdf) (2019), [LexSub](https://aclanthology.org/2020.tacl-1.21.pdf) (2020), [Hierchical-fitting](https://www.sciencedirect.com/science/article/abs/pii/S0950705122006517?via%3Dihub) (2022). All papers can be consulted in the paperswithcode web page about Hyperlex (https://paperswithcode.com/dataset/hyperlex).\n",
        "\n",
        "In a nutshell, all the following models start with a set of non-contextual embeddings, such as Skip-gram, Glove, fastText embeddings (except for HyperVec and Poincar√© embeddings that starts to train the embeddings from scratch), and using WordNet, ConceptNet or any other resource, they obtain a set of synonyms, antonyms, hyponyms/hyperonyms pairs of words. The set of pairs is used to train new embeddings by means of a specialized loss function that separate the synonyms/hyponyms from antonyms/hyperonyms. The above models differ in the chosen loss function. Once the model is trained, it is defined a distance function to give the hyponymy degree. The distance functions somehow use the Euclidean distance.\n",
        "\n",
        "- HyperVec: For a vocabulary $V$, it is trained from scratch a word embedding $v_w$, for all $w \\in V$, in a similar fashion as Skip-gramm archictecture with negative sampling training objective (SKNS). The usual loss function in SKNS is modified adding some terms to learn *hierarchical embeddings* based on hyperonym/hyperonym hierarchy. It is needed to know the hyponyms/hyeronyms of the words (or at least of some words) during the training. The pairs of training hyponyms/hyperonyms come from WordNet. The final score is calculated with a distance function, *HyperScore*, that mixes the cosine distance and the norms of the calculated embeddings. The authors claim in the paper that all pairs in the evaluation datasets have been removed from the training dataset.\n",
        "\n",
        "- Poincar√© embeddings: Similar to HyerVec, embeddings for words in a vocabulary $V$ are trained from scratch. The embeddings are learned optimizing a loss function depending on an hyperbolic distance. Given a set of unordered pairs $D= \\{\\{u,v\\}\\}$, where one of $u$ and $v$ is an hyperonym of the other, the loss function uses as positive examples the pairs of $D$ and, similar to negative sampling, uses as negative examples $10$ pairs that are not in $D$. The unordered pairs of $D$ are obtained from WordNet. The main goal of the hyperbolic embeddings are to reconstruct the complete hierarchy of a network knowing its hyperonym relationships. For graded LE, the authors define a score function based on the norms of the vectors and the hiperbolic distance. \n",
        "\n",
        "- LEAR: The starting point of this method is a set of non-contextual trained word embeddings. In particular, the authors test with SKNS, CBOW, Glove and fastText embeddings. Inspired by the *Attract-Repel* framework, these embeddings are retrained to capture the hypernoym structure of the concepts giving higher and lower norms to word embeddings situated in higher and lower positions of the hierarchy, while trying that the cosine distance mantains its properties about the similiraty of the words. The embeddings are retrained with sets of Attract ($A$), Repel($R$) and Lexical Entailment ($L$) examples. These examples are pairs of words obtained from WordNet and Roget's thesaurus, where the $A$, $R$ and $L$ pairs are $(u,v)$ where $u$ is a synonym, antonym and hyponym of $v$, respectively. The authors use $1,023,082$ synonyms pair, $380,873$ antonym pairs and $ 1,545,630$ hyponym pairs. To solve the graded LE task, a distance over the trained embeddings is defined based on the difference of the norms of the embeddings. Since the hyperonym degree for a pair of words is calculated with a combination of the cosine and the Euclidean distances of the embeddings, and the loss function during training also contains the distance of the embeddings, it should not be trained with pairs that appear in the hyperlex dataset. But this point it is not discussed in the paper. There are only two papers that mention this problem with the training: GLEN and POSTLE. Both papers show that if the LEAR model don't see any word of the hyperlex dataset while the model is retraining, the results are worse. Note that \"*don't see any word*\" is a stronger condition that \"*don't see any pair*\".\n",
        "\n",
        "- SDNS: This paper is entirely devoted to graded LE. As in LEAR, the inputs of the model are non-contextual embeddings (dependency-based word embeddings, a generalization of the SKNS embeddings). A NN is defined that is trained with the scores in the train dataset, so it is a pure regression model. To obtain better results, the authors add to the NN some weights to incorporate: sparse distributional features, model called SDNS+SDF; and some  additional information (SDNS+SDF+AD) of some positive/negatives examples that are pairs of hyponyms/synomys (positive) and hyperonyms/antonyms (negative). These examples are again obtained from WordNet and are used in pretrain stage to push the vectors to the correct side of the decision boundary. They use $102,586$ positive pairs and $42,958$ negative ones. The authors do not inform if these pairs intersects with the Hyperlex pairs. These pairs are used only to push the embedding to the correct side by means of a hinge loss function in a preliminary training.  \n",
        "\n",
        "- GLEN: This model is inspired by LEAR. The difference is the loss function. It also uses sets of pairs of synonyms, antonyms and hyponyms in the training obtained from WordNet. But in this case the authors carry out tests with models that have seen during the training from the 0% to 100% of the words in Hyperlex. Moreover, they conducted the same experiments for LEAR. As it can be appreciated in the bellow table, the performance of LEAR without seeing the Hyperlex pairs is quite poor. The 0% setup will be similar to train a model in the lexical split configuration.\n",
        "\n",
        "<center>\n",
        "\n",
        "|Setup |0% |10% |30% |50% |70% |90% |100%|\n",
        "|-|-|-|-|-|-|-|-|\n",
        "|LEAR |.174 |.188 |.273 |.438 |.548 |.634 |.682\n",
        "|GLEN |.481 |.485 |.478 |.474 |.506 |.504 |.520\n",
        "\n",
        "</center>\n",
        "\n",
        "- POSTLE: This model is a post-specialization of the LEAR embeddings. Since many words has not been seen during training in LEAR model, only the words in the constraints pairs, the original embedding and the embedding produces by LEAR serves as training examples in a post-training, that learns how the embeddings have changed for seen words and try to mimic these changes in not seen words. Again, the authors make controlled experiments with the number of words seen by LEAR during its training. They used two types of NN to do the post-specialization: Deep feed-forward network (DFFN) and Adversarial network (ADV). \n",
        "\n"
      ],
      "metadata": {
        "id": "Sg9HP-HNVQh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "</center>\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "FfuuTlJjOI9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- LexSub: Like other previous models, the inputs in LexSub are non-contextual embeddings (Glove embeddings) and by means of a loss function that takes into account hyponyms, antonymns, synomys, the embeddings are retrained. The authors train LEAR and LexSub with: firstly, less lexical resources than in LEAR's paper; secondly, with the same resources. The latter reported Spearman correlation for LEAR should similar than the correlation reported in LEAR's paper, but it doesn't match at all : $0.533$ vs. $0.686$ (may be due to the initial embeddings?). They report the following results:\n",
        "\n",
        "<center>\n",
        "\n",
        "|     | less resorc. | more resor |\n",
        "|----|----|----|\n",
        "|LEAR|0.1384 | 0.5024|\n",
        "|LexSub| 0.2615 | 0.5327|\n",
        "\n",
        "</center>\n",
        " \n",
        "\n",
        "- Hierarchical-fitting: Finally, HF trains again non-contextual embeddings using lexical pairs from WordNet, but with a novel loss function called by the authors *Hierarchical-fitting*.\n",
        "\n",
        "####**How baseline results are obtained**\n",
        "Except for SDNS model, all of the rest models are unsurpervised. But, as we disccuss above, the unsupervised models see the words in the Hyperlex dataset during training in the form of pair constraints. Thus, these models can be compared with our models trained with the datasets under the Hyperlex random configuration, since $100\\%$ of the words in train/val datasets are in the test dataset. GLEN and POSTLE papers also perform controlled experiments in such a way that during their training, it is not seen any word of the Hyperlex dataset, and they also do the same for the LEAR model. In this case, these models are compared with our models trained with the lexical configuration of Hyperlex. Thus, we get the comparable results with the random split from the regarding papers of all models, and the comparable results with the lexical split from GLEN, POSTLE and SDNS papers.\n",
        "\n",
        "####**Our proposal**\n",
        "An elemental form to give a LE grade is to use the probability $p_{hyp}$ for the the `hyp` label calculated by one of our masked/non-masked models: Greater/lower $p_{hyp}$, more/less sure we are that the pair represent an hyponym. But, let's say that one pair gets the probabilities $p_{hyp}=0.8$ and $p_{syn}=0.15$, an another pair obtains $p_{hyp}=0.8$ and $p_{syn}=0.05$. If we have two pairs with similar probabilities to be an hyponym, but one of them with higher probability to be a synonym, as in the above example, this could be an indication that the first pair should be given a higher *hyponym* degree. We can argue the same, but in the reverse sense, with other labels such as $p_{mero}$ or $p_{norel}$. Thus, if the calculated probabilities really represented the certainty about the lexical relations of two words, we could find a linear combination of the probabilities to obtain a grade for LE, that is, we could give some weights $\\beta_{hyp}, \\beta_{syn},\\beta_{norel}, \\dots$, such that knowing the probabilities of a pair $p_{hyp}, p_{syn},p_{norel}, \\dots$, we would get the hyperonym degree as the value, \n",
        "\n",
        " $$grade = \\beta_{hyp}p_{hyp}+\\beta_{syn}p_{syn}+\\beta_{norel}p_{norel} + \\dots$$\n",
        "Similarly, we can also use the logits instead of the probabilities to find the weights $\\beta_i$.\n",
        "\n",
        "One simple way to obtain the weights is to use the validation set to fit a linear regression model where the response variable is the `grade` given by the human annotators and the predictors are the logtis produce by the fine-tuned model (we also tried to use the probabilities insted of logits, and the results were quite similar). The estimated regresion coefficients will be our weights. \n",
        "\n",
        "Our method follows:\n",
        "1. We collapse all `hyp-i` and `r-hpy-i` labels to `hyp` and `r-hyp`, respectively. Thus, we train the model/template with $7$ classes: `ant`, `syn`, `hyp`, `r-hyp`, `cohyp`, `mero` and `no_rel`. In early testing, it seemed that having too many labels and too little data was hurting performance. \n",
        "2. A model/template is trained with the train/val datasets as it is described in the paper. It is run $10$ epochs and the final model is the best one on the val dataset regarding the metrics: For non-masked trained models, the metric is the macro average; and for masked models, it is the cross-entropy loss for the masked tokens. \n",
        "3. Post-processing: Once the model/template is trained, it is calculated the predicted logit of the labels for each pair in the val dataset. Thus, we get a matrix $A = [l_i^j]$, where $l_i^j$ is the logit of the $j$-th label for the $i$-th pair in the val dataset. A linear regression model is fitted to predict the vector of the median human ratings $\\boldsymbol{r}_{val}=(r_1,\\dots,r_n)$. So, we obtain a vector of $7$ weights, $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_7)$, such that,\n",
        "$$\\boldsymbol{r}_{val}\\approx \\boldsymbol{\\beta} \\cdot M$$\n",
        "4. The vector $\\boldsymbol{\\beta}$ is used to predict our rating: Given a pair in the test dataset, it is calculated its vector of logits  $\\boldsymbol{l}=(l_1,\\dots, l_7)$ for the labels; our final rating is $\\boldsymbol{\\beta} \\cdot \\boldsymbol{l}$.\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "iN2gDfbUOShQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import glob\n",
        "import ast\n",
        "import re\n",
        "import copy\n",
        "import math\n",
        "from scipy import stats\n",
        "from scipy.stats import spearmanr\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "Ymw0Mwigmlxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder with all datasets\n",
        "DIR_DATASETS = '/content/LRC-0C0F/datasets/'\n",
        "# folder with the results\n",
        "DIR_RESULTS = '/content/LRC-0C0F/results/'\n",
        "LIST_DIR_RES = [DIR_RESULTS + 'K&H+N/', \n",
        "                DIR_RESULTS + 'BLESS/',\n",
        "                DIR_RESULTS + 'EVALution/', \n",
        "                DIR_RESULTS + 'ROOT09/',\n",
        "                DIR_RESULTS + 'CogALexV/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_lexical_split/',\n",
        "                DIR_RESULTS + 'hyperlex/results_hyperlex_random_split/'\n",
        "                ]\n"
      ],
      "metadata": {
        "id": "EYI5qYdwpJ4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models2abrev = {'bert-large-uncased-whole-word-masking'.lower():'Bert',\n",
        "\t\t\t\t   'roberta-large'.lower():'Roberta',\n",
        "                   'roberta-base'.lower(): 'roberta-base',\n",
        "                   'bert-base-uncased'.lower(): \"bert-base\"\n",
        "\t\t\t\t  }\n",
        "templates2abrev = {\"' <W1> ' <SEP> ' <W2> '\".lower(): 'T1', \n",
        "                  \" <W1> <SEP> <W2> \".lower(): 'T2',\n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>.\".lower() : 'T3',\n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>: <W1> is the <LABEL> of <W2>.\".lower() : 'T4',\n",
        "                  \"' <W1> ' <MASK> ' <W2> '\".lower(): 'TM1', \n",
        "                  \" <W1> <MASK> <W2> \".lower(): 'TM2',                     \n",
        "                  \"Today, I finally discovered the relation between <W1> and <W2>: <W1> is the <MASK> of <W2>.\".lower() : 'TM3', \n",
        "                  }\n",
        "reverse_models2abrev = {v:k for k, v in models2abrev.items()}\n",
        "reverse_templates2abrev = {v:k for k, v in templates2abrev.items()}               "
      ],
      "metadata": {
        "id": "ziFTtdyEmm81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Read txt and csv result files and save results in `dict_res`**\n",
        "`dict_res` is a dictionary of dictionaries with the following structure:\n",
        "- `dict_res[dataset]`: dictionary with all results for a dataset. One from:\n",
        " ```\n",
        " ['bless', 'k&h+n','cogalexv','evalution','root09']\n",
        " ```\n",
        "-`dict_res[dataset][model]`: dictionary with all results for a dataset and model\n",
        "-`dict_res[dataset][model][template]`: dictionary with all results for a dataset, model and template\n",
        "-`dict_res[dataset][model][template]['res']`: list of dataframes from csv files. It should contain $5$ dataframes\n",
        "-`dict_res[dataset][model][template]['report']`:  list of classification reports in txt files. It should contain $5$ reports. The reports are the output dictionaries of the function `classification_report` in the `sklearn.metrics` package, adding: \n",
        "  1. For CogALexV dataset, the weighted f1-score taking into account all labels except the `random` label, if such label exists. Otherwise, the  weighted f1-score is set to $-1$; \n",
        "  2. For Hyperlex dataset, the Spearman correlations with the median of the human annotators and the following calculated scores: the logit of one label; the learned combination of all label logits; the learned combination of all label probs; it is also added these correlations rstricted to nouns and verbs.\n",
        "-`dict_res[dataset][model][template]['mean_report']`: classification report with the means and stds of the $5$ classification reports. \n",
        "\n",
        "Classification reports ( the outputs of `classification_report` function) are also dictionaries of dictionaries:\n",
        " \n",
        " ```\n",
        " { 'label1': {'precison':xxx, 'recall':yyy, 'f1-score':zzz, 'correlation':ccc (correlation only for hyperlex dataset)}\n",
        "   ....\n",
        "   'labeln': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'accuracy': xxx, \n",
        "   'macro avg': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'weighted avg': {'precison':xxx, 'recall':yyy, 'f1-score':zzz}\n",
        "   'weigthed f1-score not random':xxxxx},\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_nouns_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_nouns_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_verbs_logit': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'spearman_verbs_prob': {'correlation':cccc, 'pvalue':pppp},\n",
        "   'coefs_logit':{'l0':lll,...        ,'l7':lll, 'intercept':iii},\n",
        "   'coefs_prob':{'l0':lll,...,'l6':lll, 'intercept':iii},\n",
        "```\n",
        "The `dict_res[dataset][model][template]['mean_report']` classification report is created using the $5$ classification reports of one experiment for a dataset/model/template. It contains the same structure that the above classification report adding the std for precision, recall, f1-score values and the accuracy:\n",
        " ```\n",
        "{ 'label1': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz, 'correlation':meancccc, (correlation only for hyperlex dataset)\n",
        "             'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz, 'std_correlation':ssc}\n",
        "   ....\n",
        "   'labeln': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,'correlation':meancccc,\n",
        "              'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz, 'std_correlation':ssc}\n",
        "   'accuracy': meanxxx, \n",
        "   'std_accuracy':sss\n",
        "   'macro avg': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,\n",
        "                 'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz}\n",
        "   'weighted avg': {'precison':meanxxx, 'recall':meanyyy, 'f1-score':meanzzz,\n",
        "                    'std_precison':ssx, 'std_recall':ssy, 'std_f1-score':ssz},\n",
        "   'weigthed f1-score not random':meanxxx,\n",
        "   'std_weigthed f1-score not random':ssxxx,}\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_nouns_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_verbs_logit': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_nouns_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'spearman_verbs_prob': {'correlation':meancc, 'pvalue':meanpp,'std_correlation':sscc, 'std_pvalue':sspp},\n",
        "   'coefs_logit':{'l0':meanl,...        ,'l7':meanl, 'intercept':meani,\n",
        "                 'std_l0':meanl,...      ,'std_l7':meanl, 'std_intercept':meani},\n",
        "   'coefs_prob':{'l0':meanl,...   ,'l6':meanl, 'intercept':meani,\n",
        "                 'std_l0':meanl,... ,'std_l6':meanl, 'std_intercept':meani}\n",
        "```"
      ],
      "metadata": {
        "id": "gKjAjyP-6Doj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read txt and csv result files from our scripts**. \n",
        "\n",
        "They are read from a list of folders in variable `LIST_DIR_RES`"
      ],
      "metadata": {
        "id": "7GMv5n05_Qvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weighted_f1_no_random(dict_report):\n",
        "    ''' \n",
        "    Function to calculate the weigthed f1-score by support of all labels\n",
        "    except the random label, if the random label exists. If it not exists\n",
        "    return -1\n",
        "    '''\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'random']\n",
        "    weighted_f1_no_random = -1\n",
        "    if 'random' in dict_report.keys():\n",
        "        total_support_no_random = 0\n",
        "        for k in dict_report:\n",
        "            if k.lower() not in except_list:\n",
        "                weighted_f1_no_random += dict_report[k]['support']*dict_report[k]['f1-score']\n",
        "                total_support_no_random += dict_report[k]['support']\n",
        "        weighted_f1_no_random = weighted_f1_no_random/total_support_no_random\n",
        "    \n",
        "    return weighted_f1_no_random"
      ],
      "metadata": {
        "id": "-bCbRBKOWzPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def from_spearman_string_to_dict(spearman_string):\n",
        "    spearman_string = re.sub(\"SpearmanrResult\", \"\", spearman_string)\n",
        "    spearman_string = re.sub(\"correlation=\",\"'correlation':\",spearman_string)\n",
        "    spearman_string = re.sub(\"pvalue=\",\"'pvalue':\",spearman_string)\n",
        "    spearman_string = re.sub(\"\\(\", \"{\", spearman_string)\n",
        "    spearman_string = re.sub(\"\\)\", \"}\", spearman_string)\n",
        "    spearman = ast.literal_eval(spearman_string)\n",
        "    return spearman\n",
        "\n",
        "dict_res = {}\n",
        "LABEL_WEIGHTED_NOT_RANDOM = 'weighted f1-score not random'\n",
        "\n",
        "for dir in LIST_DIR_RES:\n",
        "    list_txt_files = glob.glob(dir + \"*.txt\")\n",
        "    list_csv_files = [re.sub(\"txt$\", \"csv\", f) for f in list_txt_files]\n",
        "    for txt_file, csv_file in zip(list_txt_files, list_csv_files):\n",
        "        # read and process txt results file\n",
        "        with open(txt_file) as ftxt:\n",
        "            # line 1: arguments\n",
        "            par = ast.literal_eval(ftxt.readline())\n",
        "            if par['dataset'].lower() == 'hyperlex':\n",
        "                if 'hyperlex/random' in par['train_file']:\n",
        "                    par['dataset'] = 'hyperlex-random'\n",
        "                else:\n",
        "                    par['dataset'] = 'hyperlex-lexical'\n",
        "            # line 2: date\n",
        "            ftxt.readline()\n",
        "            # line 3: report\n",
        "            report = ast.literal_eval(ftxt.readline().lower())\n",
        "            # line 4: hyperlex correlation by label\n",
        "            spearman_string = ftxt.readline()\n",
        "            if spearman_string != '':\n",
        "                spearman = from_spearman_string_to_dict(spearman_string)\n",
        "                # add spearman correlations to report\n",
        "                for label in list(spearman.keys()):\n",
        "                    report[label]['correlation'] = spearman[label]['correlation']\n",
        "            # line 5: overall hyperlex correlation logits\n",
        "            all_spearman_logit_string = ftxt.readline()\n",
        "            # line 6: checkpoint overall hyperlex correlation logits\n",
        "            all_spearman_check_string = ftxt.readline()\n",
        "            # line 7: overall hyperlex correlation probs\n",
        "            all_spearman_prob_string = ftxt.readline()\n",
        "            if all_spearman_logit_string != '':\n",
        "                all_spearman_logit = from_spearman_string_to_dict(all_spearman_logit_string)\n",
        "                all_spearman_check = from_spearman_string_to_dict(all_spearman_check_string)\n",
        "                if (all_spearman_logit['correlation'] != all_spearman_check['correlation']):\n",
        "                    raise Exception('Error checkpoint: Different Spearman correlations in ' + txt_file)\n",
        "                all_spearman_prob = from_spearman_string_to_dict(all_spearman_prob_string)\n",
        "                report['spearman_logit'] = all_spearman_logit\n",
        "                report['spearman_prob'] = all_spearman_prob\n",
        "            # line 8: hyperlex logit coefs\n",
        "            coefs_logit_string = ftxt.readline()\n",
        "            # line 9: hyperlex probs coefs\n",
        "            coefs_prob_string = ftxt.readline()\n",
        "            if coefs_logit_string != '':\n",
        "                coefs_logit = ast.literal_eval(coefs_logit_string)\n",
        "                report['coefs_logit'] = {'l'+str(i):c for i,c in enumerate(coefs_logit['coefs'])}\n",
        "                report['coefs_logit']['intercept'] = coefs_logit['intercept']\n",
        "                coefs_prob = ast.literal_eval(coefs_prob_string)\n",
        "                report['coefs_prob'] = {'l'+str(i):c for i,c in enumerate(coefs_prob['coefs'])}\n",
        "                report['coefs_prob']['intercept'] = coefs_prob['intercept']\n",
        "        \n",
        "        # read csv results file      \n",
        "        res_csv = pd.read_csv(csv_file, quotechar='\"',keep_default_na=False)\n",
        "        # save csv and txt files to dict_res\n",
        "        a = dict_res.setdefault(par['dataset'].lower(), {})\n",
        "        b = a.setdefault(par['model'].lower(), {})\n",
        "        if \"RandomMask\" in dir:\n",
        "            par['train_templates'][0] = re.sub(\"<mask>\", \"<maskr>\", par['train_templates'][0].lower())\n",
        "        c = b.setdefault(par['train_templates'][0].lower(), {})\n",
        "        d1 = c.setdefault('res',[])\n",
        "        d2 = c.setdefault('report',[])\n",
        "        d1.append(res_csv)\n",
        "        report[LABEL_WEIGHTED_NOT_RANDOM] = calculate_weighted_f1_no_random(report)\n",
        "        if 'spearman_logit' in list(report.keys()):\n",
        "            #distinguir non-masked/masked\n",
        "            col_names = [nc for nc in res_csv.columns]\n",
        "            logit_col_names = filter(lambda x: re.match('.*logit$',x), col_names)\n",
        "            prob_col_names = [re.sub(\"_logit$\", \"\", c) for c in logit_col_names]\n",
        "            X_logit = res_csv.filter(regex=('.*logit$')).to_numpy(copy=True)\n",
        "            X_prob = res_csv[prob_col_names].to_numpy(copy=True)\n",
        "            X_prob = np.delete(X_prob, 5, axis=1)\n",
        "            coefs_l = np.array(coefs_logit['coefs'])\n",
        "            coefs_p = np.array(coefs_prob['coefs'])\n",
        "            grades = res_csv['grade']\n",
        "            if 'hyperlex/random' in par['test_file']:\n",
        "                test_data = pd.read_csv(DIR_DATASETS + 'hyperlex/random/test.tsv',\n",
        "                                        sep = '\\t', header=None)\n",
        "            else:\n",
        "                test_data = pd.read_csv(DIR_DATASETS + 'hyperlex/lexical/test.tsv',\n",
        "                                        sep = '\\t', header=None)\n",
        "\n",
        "            check_spearman_logit = spearmanr(np.dot(X_logit,coefs_l), grades)\n",
        "            check_spearman_prob =  spearmanr(np.dot(X_prob,coefs_p), grades)\n",
        "            # checkpoint: It must be equal (upto thousandths) the Spearman correlations\n",
        "            # calculated in the experiments and the ones calculated in this test\n",
        "            if abs(check_spearman_logit[0] - report['spearman_logit']['correlation']) > 0.001:\n",
        "                raise Exception('Check: Different logit prob from experiments and test: ' + str(check_spearman_logit[0]) + '  ' + str(report['spearman_logit']['correlation']) )\n",
        "            if abs(check_spearman_prob[0] - report['spearman_prob']['correlation']) > 0.001:\n",
        "                raise Exception('Check: Different spearman prob from experiments and test: ' + str(check_spearman_prob[0]) + '  ' + str(report['spearman_prob']['correlation']) )\n",
        "               \n",
        "            nouns_idx = test_data.iloc[:,2] == 'N'\n",
        "            verbs_idx = test_data.iloc[:,2] == 'V'\n",
        "            spearman_nouns_logit = spearmanr(np.dot(X_logit[nouns_idx,:],coefs_l), grades[nouns_idx])\n",
        "            spearman_verbs_logit = spearmanr(np.dot(X_logit[verbs_idx,:],coefs_l), grades[verbs_idx])\n",
        "            spearman_nouns_prob = spearmanr(np.dot(X_prob[nouns_idx,:],coefs_p), grades[nouns_idx])\n",
        "            spearman_verbs_prob = spearmanr(np.dot(X_prob[verbs_idx,:],coefs_p), grades[verbs_idx])\n",
        "            report['spearman_nouns_logit'] = from_spearman_string_to_dict(str(spearman_nouns_logit))\n",
        "            report['spearman_verbs_logit'] = from_spearman_string_to_dict(str(spearman_verbs_logit))\n",
        "            report['spearman_nouns_prob'] = from_spearman_string_to_dict(str(spearman_nouns_prob))\n",
        "            report['spearman_verbs_prob'] = from_spearman_string_to_dict(str(spearman_verbs_prob))\n",
        "        d2.append(report)"
      ],
      "metadata": {
        "id": "U7EIgN6fmpS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add the mean report**. \n",
        "\n"
      ],
      "metadata": {
        "id": "IJo1begf_tRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_value(one_report, keys):\n",
        "    val = one_report\n",
        "    for k in keys:\n",
        "        val = val[k]\n",
        "    return val\n",
        "\n",
        "def get_values(list_reports, keys):\n",
        "    values = np.array([get_value(one_report, keys) for one_report in list_reports])\n",
        "    return values\n",
        "\n",
        "def calculateMeansRec(list_reports, one_dict, past_keys, exclude_keys):   \n",
        "    for k in list(one_dict.keys()):\n",
        "        if k not in exclude_keys:\n",
        "            copy_past_keys = copy.deepcopy(past_keys)\n",
        "            copy_past_keys.append(k)\n",
        "            if not isinstance(one_dict[k], dict):\n",
        "                values = get_values(list_reports, copy_past_keys)\n",
        "                one_dict[k]= values.mean()\n",
        "                one_dict['std_'+k] = values.std()\n",
        "            else:\n",
        "                calculateMeansRec(list_reports, one_dict[k], copy_past_keys, exclude_keys)\n",
        "\n",
        "def flat_list_reportsRec(list_reports, one_dict, past_keys, exclude_keys):   \n",
        "    for k in list(one_dict.keys()):\n",
        "        if k not in exclude_keys:\n",
        "            copy_past_keys = copy.deepcopy(past_keys)\n",
        "            copy_past_keys.append(k)\n",
        "            if not isinstance(one_dict[k], dict):\n",
        "                values = get_values(list_reports, copy_past_keys)\n",
        "                one_dict[k]= values.tolist()\n",
        "            else:\n",
        "                flat_list_reportsRec(list_reports, one_dict[k], copy_past_keys, exclude_keys)\n",
        "\n",
        "def calculateMeans(list_reports):\n",
        "    '''\n",
        "    Given a list of structurally equal reports, the function\n",
        "    returns a report with the means and stds. A report is a dictionary\n",
        "    whose values are either a dictionary or a real number.\n",
        "    '''\n",
        "    means_report = copy.deepcopy(list_reports[0])\n",
        "    calculateMeansRec(list_reports, means_report, past_keys=[], exclude_keys=['support'])\n",
        "\n",
        "    return means_report\n",
        "\n",
        "def flat_list_reports(list_reports):\n",
        "    '''\n",
        "    Given a list of structurally equal reports, the function returns\n",
        "    a structurally equal report join in a list all values that are real numbers.\n",
        "    A report is a dictionary whose values are either a dictionary or a real number.\n",
        "    '''\n",
        "    flat_report = copy.deepcopy(list_reports[0])\n",
        "    flat_list_reportsRec(list_reports, flat_report, past_keys=[], exclude_keys=['support'])\n",
        "\n",
        "    return flat_report"
      ],
      "metadata": {
        "id": "Nz9IJTVwL5Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for d in dict_res:\n",
        "    print(d.upper())\n",
        "    for m in dict_res[d]:\n",
        "        print(\" -\".join(['Calculating:', d, m]))\n",
        "        for t in dict_res[d][m]:\n",
        "            list_reports = dict_res[d][m][t]['report']\n",
        "            print(\" -\".join([\"    \", t, templates2abrev[t] ]))\n",
        "            dict_res[d][m][t]['mean_report'] = calculateMeans(list_reports)\n",
        "            dict_res[d][m][t]['flat_reports'] = flat_list_reports(list_reports)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkbbrrSy-Ekb",
        "outputId": "bd6ff788-dcad-4086-f157-b183ab640920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K&H+N\n",
            "Calculating: -k&h+n -roberta-base\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -k&h+n -roberta-large\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -k&h+n -bert-base-uncased\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -k&h+n -bert-large-uncased-whole-word-masking\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "BLESS\n",
            "Calculating: -bless -roberta-large\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -bless -roberta-base\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -bless -bert-base-uncased\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -bless -bert-large-uncased-whole-word-masking\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "EVALUTION\n",
            "Calculating: -evalution -bert-base-uncased\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -evalution -roberta-base\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -evalution -bert-large-uncased-whole-word-masking\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -evalution -roberta-large\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "ROOT09\n",
            "Calculating: -root09 -bert-large-uncased-whole-word-masking\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -root09 -roberta-large\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -root09 -roberta-base\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -root09 -bert-base-uncased\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "COGALEXV\n",
            "Calculating: -cogalexv -bert-large-uncased-whole-word-masking\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -cogalexv -roberta-large\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -cogalexv -roberta-base\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -cogalexv -bert-base-uncased\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <label> of <w2>. -T4\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "HYPERLEX-LEXICAL\n",
            "Calculating: -hyperlex-lexical -bert-large-uncased-whole-word-masking\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-lexical -bert-base-uncased\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-lexical -roberta-large\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-lexical -roberta-base\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "HYPERLEX-RANDOM\n",
            "Calculating: -hyperlex-random -bert-large-uncased-whole-word-masking\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-random -bert-base-uncased\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-random -roberta-large\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n",
            "Calculating: -hyperlex-random -roberta-base\n",
            "     -' <w1> ' <sep> ' <w2> ' -T1\n",
            "     -today, i finally discovered the relation between <w1> and <w2>. -T3\n",
            "     - <w1> <mask> <w2>  -TM2\n",
            "     - <w1> <sep> <w2>  -T2\n",
            "     -today, i finally discovered the relation between <w1> and <w2>: <w1> is the <mask> of <w2>. -TM3\n",
            "     -' <w1> ' <mask> ' <w2> ' -TM1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Dictionary with best results**\n",
        "Given the dictionary of results `dict_res`, the following functions create `dict_best`, a python dictionary of dictionaries with the best results for any measure and model/template. It has the structure:\n",
        " - `dict_best[dataset]`: It is a dictionary with the best results for a dataset. One from:\n",
        " ```\n",
        " ['bless', 'k&h+n','cogalexv','evalution','root09']\n",
        " ```\n",
        " - `dict_best[dataset]`: It is a dictionary with the best results for a dataset:\n",
        " \n",
        " \n",
        " ```\n",
        " {label1: { 'precision': {'best_val': xxxx,\n",
        "                          'best_model':mmmm,\n",
        "                          'best_template':tttt,\n",
        "                          'p-vals': {model: { template_1: pppp,\n",
        "                                              ....\n",
        "                                              template_k:pppp}}},\n",
        "              'recall': {'best_val': xxxx,\n",
        "                         'best_model':mmmm,\n",
        "                         'best_template':tttt,\n",
        "                         'p-vals': {model: { template_1: pppp,\n",
        "                                             ....\n",
        "                                             template_k:pppp}}},\n",
        "              'f1-score': {'best_val': xxxx,\n",
        "                           'best_model':mmmm,\n",
        "                           'best_template':tttt,\n",
        "                           'p-vals': {model: { template_1: pppp,\n",
        "                                                ....\n",
        "                                                template_k:pppp}}},\n",
        "           //only for hyperlex dataset\n",
        "           'correlation': {'best_val': xxxx,\n",
        "                           'best_model':mmmm,\n",
        "                           'best_template':tttt,\n",
        "                           'p-vals': {model: { template_1: pppp,\n",
        "                                                ....\n",
        "                                                template_k:pppp}}}}}                                                \n",
        "  ....\n",
        "  labeln: {same label 1 ...}\n",
        "  'accuracy': {'best_val': xxxx,\n",
        "               'best_model':mmmm,\n",
        "               'best_template':tttt,\n",
        "               'p-vals': {model: { template_1: pppp,\n",
        "                                    ....\n",
        "                                    template_k:pppp}}},\n",
        "  'macro avg':{similar to label 1...},\n",
        "  'weighted avg' :{similar to label 1...},\n",
        "  'weigthed f1-score not random': {similar to accuracy...},\n",
        "   //only for hyperlex dataset\n",
        "   'spearman_logit': {...},\n",
        "   'spearman_prob': {...},\n",
        "   'spearman_nouns_logit': {...},\n",
        "   'spearman_verbs_logit': {...},\n",
        "   'spearman_nouns_prob': {...},\n",
        "   'spearman_verbs_prob': {...},\n",
        "   'coefs_logit':{...},\n",
        "   'coefs_prob':{...}\n",
        " }\n",
        "```                                      \n",
        "\n",
        "\n",
        "For example, \n",
        "- `dict_best['k&h+n']['ant']['precision']['best_val']` contains the best **mean** precision value among all models and templates for the antonyms label, \n",
        "- `dict_best['k&h+n']['ant']['precision']['best_model']` is the model for which the best precision has been obtained,\n",
        "- and  `dict_best['k&h+n']['ant']['precision']['best_template']` is the template for which the best precision has been obtained.\n",
        "\n",
        "The `dict_best` dictionary also contains the p-values comparing the mean of the best model/template with any other mean for a model/template. It is used a Welch's test to check if there is statistical evidence that the means are different. So, for instance the value in\n",
        "```\n",
        "dict_best['k&h+n']['ant']['precision']['p-vals']['roberta-large'][' <w1> <sep> <w2> ']\n",
        "```\n",
        "contains the p-value to check if the mean of the $5$ precision values of our experiments for antonyms in K&H+N dataset trained with the RoBERTa large model and the template ` <W1> <SEP> <W2> ` is different from the mean of the $5$ precision values obtained with the model/template that has got the best mean precision value."
      ],
      "metadata": {
        "id": "Qmxgt2_auQuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_empty(one_dict, empty_best_dict, suffix=''):\n",
        "    for label in one_dict:\n",
        "        empty_best_dict[label+suffix]={}\n",
        "        if isinstance(one_dict[label], dict):\n",
        "            for sublabel in one_dict[label]:\n",
        "                if sublabel != 'support':\n",
        "                    empty_best_dict[label+suffix][sublabel] = {}\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_val'] = -1.0\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_model'] = ''\n",
        "                    empty_best_dict[label+suffix][sublabel]['best_template'] = ''\n",
        "        else:\n",
        "            empty_best_dict[label+suffix]['best_val'] = -1.0\n",
        "            empty_best_dict[label+suffix]['best_model'] = ''\n",
        "            empty_best_dict[label+suffix]['best_template'] = ''\n",
        "\n",
        "def create_empty_best_dict(dict_res, dataset):\n",
        "    m0 = list(dict_res[dataset].keys())[0]\n",
        "    t0 = list(dict_res[dataset][m0].keys())[0]\n",
        "    empty_best_dict = {}\n",
        "    one_dict = dict_res[dataset][m0][t0]['report'][0]\n",
        "    create_empty(one_dict, empty_best_dict)\n",
        "    return empty_best_dict\n",
        "\n",
        "def create (dict_mean_rep, m, t, best_dict_dataset):\n",
        "    for label in dict_mean_rep:\n",
        "        if label in list(best_dict_dataset.keys()):\n",
        "            if isinstance(dict_mean_rep[label], dict):\n",
        "                for sublabel in dict_mean_rep[label]:\n",
        "                    if sublabel in list(best_dict_dataset[label].keys()):\n",
        "                        if dict_mean_rep[label][sublabel] > best_dict_dataset[label][sublabel]['best_val']:\n",
        "                            best_dict_dataset[label][sublabel]['best_val'] = dict_mean_rep[label][sublabel]\n",
        "                            best_dict_dataset[label][sublabel]['best_model'] = m\n",
        "                            best_dict_dataset[label][sublabel]['best_template'] = t\n",
        "            else:\n",
        "                if dict_mean_rep[label] >= best_dict_dataset[label]['best_val']:\n",
        "                    best_dict_dataset[label]['best_val'] = dict_mean_rep[label]\n",
        "                    best_dict_dataset[label]['best_model'] = m\n",
        "                    best_dict_dataset[label]['best_template'] = t\n",
        "\n",
        "def create_best_dict_dataset(dict_res, d):\n",
        "    '''\n",
        "    For a dataset d, create the best dictionary without p-values\n",
        "    '''\n",
        "    best_dict_dataset = create_empty_best_dict(dict_res, d)\n",
        "    for m in dict_res[d]:\n",
        "        for t in dict_res[d][m]:\n",
        "            create(dict_res[d][m][t]['mean_report'], m, t, best_dict_dataset)\n",
        "    return best_dict_dataset\n",
        "\n",
        "def create_pvals_dataset(best_dict, dict_res, dataset, label, sublabel=None):\n",
        "    '''\n",
        "    Given a best_dict without p-values and dictionary of results, label and sublabel\n",
        "    create a dictionary containing the p-values for label and sublabel.\n",
        "    '''\n",
        "    dict_pvals = {}\n",
        "    if sublabel is not None:\n",
        "        mref = best_dict[dataset][label][sublabel]['best_model']\n",
        "        tref = best_dict[dataset][label][sublabel]['best_template']\n",
        "        list_reports = dict_res[dataset][mref][tref]['report']\n",
        "        list_ref_max = [rep[label][sublabel] for rep in list_reports]\n",
        "    else:\n",
        "        mref = best_dict[dataset][label]['best_model']\n",
        "        tref = best_dict[dataset][label]['best_template']\n",
        "        list_reports = dict_res[dataset][mref][tref]['report']\n",
        "        list_ref_max = [rep[label] for rep in list_reports]\n",
        "    \n",
        "    for m in dict_res[dataset]:\n",
        "        for t in dict_res[dataset][m]:\n",
        "            if not (m == mref and t == tref):\n",
        "                if sublabel is not None:\n",
        "                    list_reports_others = dict_res[dataset][m][t]['report']\n",
        "                    list_vals = [rep[label][sublabel] for rep in list_reports_others]\n",
        "                else:\n",
        "                    list_reports_others = dict_res[dataset][m][t]['report']\n",
        "                    list_vals = [rep[label] for rep in list_reports_others]\n",
        "                \n",
        "                pval = stats.ttest_ind(list_ref_max, list_vals, equal_var=False)[1]\n",
        "                dict_1 = dict_pvals.setdefault(m,{})\n",
        "                dict_1[t] = pval\n",
        "    return dict_pvals\n"
      ],
      "metadata": {
        "id": "BkqDiBWlbwau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = create_best_dict_dataset(dict_res, 'hyperlex-lexical')\n",
        "h.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WoH9yPj4Okp",
        "outputId": "a979d5e1-fd98-4e91-9182-640521ecd9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['ant', 'cohyp', 'hyp', 'mero', 'no-rel', 'r-hyp', 'syn', 'accuracy', 'macro avg', 'weighted avg', 'spearman_logit', 'spearman_prob', 'coefs_logit', 'coefs_prob', 'weighted f1-score not random', 'spearman_nouns_logit', 'spearman_verbs_logit', 'spearman_nouns_prob', 'spearman_verbs_prob'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dict_best(dict_res):\n",
        "    ''' \n",
        "    Given a dictionary of results, it returns a dictionary of best results\n",
        "    '''\n",
        "    # create best dictionary without p-vals\n",
        "    dict_best_1={}\n",
        "    for d in dict_res:\n",
        "        dict_best_1[d] = create_best_dict_dataset(dict_res, d)\n",
        "    \n",
        "    # create best dictionary adding p-vals\n",
        "    dict_best = copy.deepcopy(dict_best_1)    \n",
        "    for d in dict_best_1:\n",
        "        for label in dict_best_1[d]:\n",
        "            if 'best_val' in list(dict_best_1[d][label]):\n",
        "                dict_best[d][label]['p-vals'] = create_pvals_dataset(dict_best_1, dict_res, d, label) \n",
        "            else:\n",
        "                for sublabel in dict_best_1[d][label]:\n",
        "                    dict_best[d][label][sublabel]['p-vals'] = create_pvals_dataset(dict_best_1, dict_res, d, label,sublabel)\n",
        "    return dict_best\n",
        "    \n",
        "dict_best = create_dict_best(dict_res)"
      ],
      "metadata": {
        "id": "DhD5FCWPf_lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CogALexV**\n",
        "To process the CogALexV results. Note that the results for templates `T1`-`T4` correspond to the non-masked models, and templates `TM1`-`TM3`for the masked ones. See the dictionary `templates2abrev` in a cell above."
      ],
      "metadata": {
        "id": "74KSBwjbExz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframe_results_cogalexv(dict_res, dataset):\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'std_'+LABEL_WEIGHTED_NOT_RANDOM, 'random']\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]}\n",
        "    col_tuples=[]\n",
        "    create_tuples = True\n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            mean_report = dict_res[dataset.lower()][m][t]['mean_report']\n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "            for k in mean_report: \n",
        "                if k not in except_list:\n",
        "                    l = dict_df.setdefault(k, [])\n",
        "                    if k != LABEL_WEIGHTED_NOT_RANDOM:\n",
        "                        l.append(mean_report[k]['f1-score'])\n",
        "                        if create_tuples:\n",
        "                            col_tuples.append((k,'f1-score'))\n",
        "                    else:\n",
        "                        l.append(mean_report[k])\n",
        "            create_tuples = False       \n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    col_tuples.append(('all','weighted f1-score'))\n",
        "    col_index = pd.MultiIndex.from_tuples(col_tuples)\n",
        "    res_df = res_df.iloc[:, 3:].copy()\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = col_index\n",
        "    return res_df\n",
        "\n",
        "def get_dataframe_results_max_min_cogalexv(dict_res, dataset):\n",
        "    except_list = ['accuracy', 'macro avg', 'weighted avg', 'std_accuracy', 'std_'+LABEL_WEIGHTED_NOT_RANDOM, 'random']\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]}\n",
        "    col_tuples=[]\n",
        "    create_tuples_labels = True\n",
        "    create_tuples_all = True\n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            flat_reports = dict_res[dataset.lower()][m][t]['flat_reports']\n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "            for k in flat_reports: \n",
        "                if k not in except_list:\n",
        "                    l_max = dict_df.setdefault('max-'+ k, [])\n",
        "                    l_min = dict_df.setdefault('min-'+ k, [])\n",
        "                    l_std = dict_df.setdefault('std-'+ k, [])\n",
        "                    if k != LABEL_WEIGHTED_NOT_RANDOM:\n",
        "                        l_max.append(np.array(flat_reports[k]['f1-score']).max())\n",
        "                        l_min.append(np.array(flat_reports[k]['f1-score']).min())\n",
        "                        l_std.append(np.array(flat_reports[k]['f1-score']).std())\n",
        "                        if create_tuples_labels:\n",
        "                            col_tuples.append((k,'max-f1-score'))\n",
        "                            col_tuples.append((k,'min-f1-score'))\n",
        "                            col_tuples.append((k,'std-f1-score'))\n",
        "                    else:\n",
        "                        l_max.append(np.array(flat_reports[k]).max())\n",
        "                        l_min.append(np.array(flat_reports[k]).min())\n",
        "                        l_std.append(np.array(flat_reports[k]).std())\n",
        "                        if create_tuples_all:\n",
        "                            col_tuples.append((k,'max-all'))\n",
        "                            col_tuples.append((k,'min-all'))\n",
        "                            col_tuples.append((k,'std-all'))\n",
        "            create_tuples_labels = False   \n",
        "            create_tuples_all = False  \n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    #col_tuples.append(('all','weighted f1-score'))\n",
        "    col_index = pd.MultiIndex.from_tuples(col_tuples)\n",
        "    res_df = res_df.iloc[:, 3:].copy()\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = col_index\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "moVbJ7meDFOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cogalexv_final = get_dataframe_results_cogalexv(dict_res, 'CogALexV')\n",
        "df_cogalexv_final = df_cogalexv_final.sort_index(level=['model','template'])"
      ],
      "metadata": {
        "id": "bcUBUbN10gve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_SOTAS_COGALEXV = DIR_RESULTS + 'sotas_results_literature/cogalexv_Sotas_RC.txt'\n",
        "df_sotas_cogalexv = pd.read_csv(FILE_SOTAS_COGALEXV, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "multi_row_tuples = list(zip(['Sota']*df_sotas_cogalexv.shape[0], list(df_sotas_cogalexv.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "df_sotas_cogalexv.index=multi_row\n",
        "df_res_sotas_cogalexv = pd.concat([df_cogalexv_final,df_sotas_cogalexv])\n",
        "\n",
        "#change column order\n",
        "order1 = {'ant':0,'hyper':1,'part_of':2, 'syn':3, 'all':4}\n",
        "multi_col_list = list(df_res_sotas_cogalexv.columns)\n",
        "multi_col_list.sort(key=lambda x: order1[x[0].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "df_res_sotas_cogalexv = pd.DataFrame(df_res_sotas_cogalexv, columns=multi_col)\n",
        "df_res_sotas_cogalexv"
      ],
      "metadata": {
        "id": "k6REijakbq8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "272cfbfc-5515-401b-b5fc-520f7752e7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                            ant     hyper   part_of       syn  \\\n",
              "                       f1-score  f1-score  f1-score  f1-score   \n",
              "model        template                                           \n",
              "Bert         T1        0.770474  0.680452  0.715095  0.563975   \n",
              "             T2        0.768676  0.675201  0.727876  0.528010   \n",
              "             T3        0.788689  0.681299  0.735502  0.565756   \n",
              "             T4        0.119323  0.044062  0.077908  0.000000   \n",
              "             TM1       0.798389  0.682221  0.745577  0.584719   \n",
              "             TM2       0.781551  0.687806  0.742477  0.560105   \n",
              "             TM3       0.778578  0.682159  0.742217  0.562916   \n",
              "Roberta      T1        0.872557  0.703143  0.752050  0.603664   \n",
              "             T2        0.863005  0.681854  0.744814  0.583635   \n",
              "             T3        0.883666  0.718214  0.784419  0.628822   \n",
              "             T4        0.236519  0.003704  0.164753  0.084852   \n",
              "             TM1       0.879658  0.708541  0.772999  0.598834   \n",
              "             TM2       0.870704  0.723055  0.787251  0.621032   \n",
              "             TM3       0.870720  0.717585  0.787410  0.615927   \n",
              "bert-base    T1        0.553600  0.590749  0.657085  0.361485   \n",
              "             T2        0.528956  0.544102  0.610356  0.277698   \n",
              "             T3        0.565381  0.605407  0.683618  0.374989   \n",
              "             T4        0.081221  0.000000  0.101444  0.006059   \n",
              "             TM1       0.644556  0.625010  0.707385  0.431301   \n",
              "             TM2       0.570093  0.621784  0.685384  0.393129   \n",
              "             TM3       0.635968  0.648195  0.720730  0.430058   \n",
              "roberta-base T1        0.805550  0.677129  0.732487  0.569636   \n",
              "             T2        0.782981  0.651829  0.692575  0.536492   \n",
              "             T3        0.819614  0.675679  0.731475  0.576918   \n",
              "             T4        0.026575  0.000000  0.102297  0.092189   \n",
              "             TM1       0.809081  0.678229  0.743181  0.560552   \n",
              "             TM2       0.801448  0.673224  0.742456  0.555826   \n",
              "             TM3       0.814963  0.679439  0.729843  0.560999   \n",
              "Sota         LexNET    0.425000  0.526000  0.493000  0.297000   \n",
              "             SphereRE  0.479000  0.538000  0.539000  0.286000   \n",
              "             KEML      0.492000  0.547000  0.652000  0.292000   \n",
              "             RelBert   0.794000  0.616000  0.702000  0.505000   \n",
              "\n",
              "                                    all  \n",
              "                      weighted f1-score  \n",
              "model        template                    \n",
              "Bert         T1                0.690274  \n",
              "             T2                0.683411  \n",
              "             T3                0.700158  \n",
              "             T4                0.063480  \n",
              "             TM1               0.708948  \n",
              "             TM2               0.700283  \n",
              "             TM3               0.698097  \n",
              "Roberta      T1                0.742749  \n",
              "             T2                0.727845  \n",
              "             T3                0.761832  \n",
              "             T4                0.118573  \n",
              "             TM1               0.749557  \n",
              "             TM2               0.758490  \n",
              "             TM3               0.755786  \n",
              "bert-base    T1                0.546293  \n",
              "             T2                0.498959  \n",
              "             T3                0.562078  \n",
              "             T4                0.043619  \n",
              "             TM1               0.607497  \n",
              "             TM2               0.572578  \n",
              "             TM3               0.614543  \n",
              "roberta-base T1                0.704082  \n",
              "             T2                0.675341  \n",
              "             T3                0.709073  \n",
              "             T4                0.044251  \n",
              "             TM1               0.705707  \n",
              "             TM2               0.700768  \n",
              "             TM3               0.705455  \n",
              "Sota         LexNET            0.445000  \n",
              "             SphereRE          0.471000  \n",
              "             KEML              0.500000  \n",
              "             RelBert           0.664000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3db4a90a-909b-4ad3-a792-a8b8ee3e4f7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ant</th>\n",
              "      <th>hyper</th>\n",
              "      <th>part_of</th>\n",
              "      <th>syn</th>\n",
              "      <th>all</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>weighted f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.770474</td>\n",
              "      <td>0.680452</td>\n",
              "      <td>0.715095</td>\n",
              "      <td>0.563975</td>\n",
              "      <td>0.690274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.768676</td>\n",
              "      <td>0.675201</td>\n",
              "      <td>0.727876</td>\n",
              "      <td>0.528010</td>\n",
              "      <td>0.683411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.788689</td>\n",
              "      <td>0.681299</td>\n",
              "      <td>0.735502</td>\n",
              "      <td>0.565756</td>\n",
              "      <td>0.700158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.119323</td>\n",
              "      <td>0.044062</td>\n",
              "      <td>0.077908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.063480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.798389</td>\n",
              "      <td>0.682221</td>\n",
              "      <td>0.745577</td>\n",
              "      <td>0.584719</td>\n",
              "      <td>0.708948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.781551</td>\n",
              "      <td>0.687806</td>\n",
              "      <td>0.742477</td>\n",
              "      <td>0.560105</td>\n",
              "      <td>0.700283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.778578</td>\n",
              "      <td>0.682159</td>\n",
              "      <td>0.742217</td>\n",
              "      <td>0.562916</td>\n",
              "      <td>0.698097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.872557</td>\n",
              "      <td>0.703143</td>\n",
              "      <td>0.752050</td>\n",
              "      <td>0.603664</td>\n",
              "      <td>0.742749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.863005</td>\n",
              "      <td>0.681854</td>\n",
              "      <td>0.744814</td>\n",
              "      <td>0.583635</td>\n",
              "      <td>0.727845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.883666</td>\n",
              "      <td>0.718214</td>\n",
              "      <td>0.784419</td>\n",
              "      <td>0.628822</td>\n",
              "      <td>0.761832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.236519</td>\n",
              "      <td>0.003704</td>\n",
              "      <td>0.164753</td>\n",
              "      <td>0.084852</td>\n",
              "      <td>0.118573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.879658</td>\n",
              "      <td>0.708541</td>\n",
              "      <td>0.772999</td>\n",
              "      <td>0.598834</td>\n",
              "      <td>0.749557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.870704</td>\n",
              "      <td>0.723055</td>\n",
              "      <td>0.787251</td>\n",
              "      <td>0.621032</td>\n",
              "      <td>0.758490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.870720</td>\n",
              "      <td>0.717585</td>\n",
              "      <td>0.787410</td>\n",
              "      <td>0.615927</td>\n",
              "      <td>0.755786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.553600</td>\n",
              "      <td>0.590749</td>\n",
              "      <td>0.657085</td>\n",
              "      <td>0.361485</td>\n",
              "      <td>0.546293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.528956</td>\n",
              "      <td>0.544102</td>\n",
              "      <td>0.610356</td>\n",
              "      <td>0.277698</td>\n",
              "      <td>0.498959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.565381</td>\n",
              "      <td>0.605407</td>\n",
              "      <td>0.683618</td>\n",
              "      <td>0.374989</td>\n",
              "      <td>0.562078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.081221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.101444</td>\n",
              "      <td>0.006059</td>\n",
              "      <td>0.043619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.644556</td>\n",
              "      <td>0.625010</td>\n",
              "      <td>0.707385</td>\n",
              "      <td>0.431301</td>\n",
              "      <td>0.607497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.570093</td>\n",
              "      <td>0.621784</td>\n",
              "      <td>0.685384</td>\n",
              "      <td>0.393129</td>\n",
              "      <td>0.572578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.635968</td>\n",
              "      <td>0.648195</td>\n",
              "      <td>0.720730</td>\n",
              "      <td>0.430058</td>\n",
              "      <td>0.614543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.805550</td>\n",
              "      <td>0.677129</td>\n",
              "      <td>0.732487</td>\n",
              "      <td>0.569636</td>\n",
              "      <td>0.704082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.782981</td>\n",
              "      <td>0.651829</td>\n",
              "      <td>0.692575</td>\n",
              "      <td>0.536492</td>\n",
              "      <td>0.675341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.819614</td>\n",
              "      <td>0.675679</td>\n",
              "      <td>0.731475</td>\n",
              "      <td>0.576918</td>\n",
              "      <td>0.709073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.026575</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102297</td>\n",
              "      <td>0.092189</td>\n",
              "      <td>0.044251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.809081</td>\n",
              "      <td>0.678229</td>\n",
              "      <td>0.743181</td>\n",
              "      <td>0.560552</td>\n",
              "      <td>0.705707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.801448</td>\n",
              "      <td>0.673224</td>\n",
              "      <td>0.742456</td>\n",
              "      <td>0.555826</td>\n",
              "      <td>0.700768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.814963</td>\n",
              "      <td>0.679439</td>\n",
              "      <td>0.729843</td>\n",
              "      <td>0.560999</td>\n",
              "      <td>0.705455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Sota</th>\n",
              "      <th>LexNET</th>\n",
              "      <td>0.425000</td>\n",
              "      <td>0.526000</td>\n",
              "      <td>0.493000</td>\n",
              "      <td>0.297000</td>\n",
              "      <td>0.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SphereRE</th>\n",
              "      <td>0.479000</td>\n",
              "      <td>0.538000</td>\n",
              "      <td>0.539000</td>\n",
              "      <td>0.286000</td>\n",
              "      <td>0.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KEML</th>\n",
              "      <td>0.492000</td>\n",
              "      <td>0.547000</td>\n",
              "      <td>0.652000</td>\n",
              "      <td>0.292000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RelBert</th>\n",
              "      <td>0.794000</td>\n",
              "      <td>0.616000</td>\n",
              "      <td>0.702000</td>\n",
              "      <td>0.505000</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3db4a90a-909b-4ad3-a792-a8b8ee3e4f7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3db4a90a-909b-4ad3-a792-a8b8ee3e4f7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3db4a90a-909b-4ad3-a792-a8b8ee3e4f7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cogalexv_max_min_final = get_dataframe_results_max_min_cogalexv(dict_res, 'CogALexV')\n",
        "df_cogalexv_max_min_final = df_cogalexv_max_min_final.sort_index(level=['model','template'])\n",
        "df_cogalexv_max_min_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g5mp3eKfVGC_",
        "outputId": "b4b44082-8119-4628-8a91-73c3c95a3d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                               ant                                  hyper  \\\n",
              "                      max-f1-score min-f1-score std-f1-score max-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.788540     0.754986     0.011111     0.693042   \n",
              "             T2           0.784203     0.752044     0.012690     0.697095   \n",
              "             T3           0.797122     0.776406     0.007539     0.687879   \n",
              "             T4           0.220661     0.000000     0.099786     0.138219   \n",
              "             TM1          0.805556     0.789250     0.006764     0.694891   \n",
              "             TM2          0.797768     0.765537     0.011892     0.696296   \n",
              "             TM3          0.812589     0.763948     0.017595     0.691771   \n",
              "Roberta      T1           0.890469     0.836524     0.019903     0.735724   \n",
              "             T2           0.905109     0.830137     0.024201     0.725067   \n",
              "             T3           0.896067     0.874644     0.007488     0.734211   \n",
              "             T4           0.523985     0.000000     0.181011     0.018519   \n",
              "             TM1          0.888252     0.863572     0.008898     0.714286   \n",
              "             TM2          0.886657     0.862119     0.008853     0.732432   \n",
              "             TM3          0.879536     0.861671     0.007292     0.724965   \n",
              "bert-base    T1           0.563415     0.543478     0.006968     0.598854   \n",
              "             T2           0.550336     0.508906     0.015003     0.560647   \n",
              "             T3           0.580132     0.539642     0.015095     0.615385   \n",
              "             T4           0.189547     0.000000     0.077721     0.000000   \n",
              "             TM1          0.655172     0.631136     0.008916     0.634538   \n",
              "             TM2          0.593123     0.550512     0.016477     0.631436   \n",
              "             TM3          0.653061     0.615836     0.012420     0.656381   \n",
              "roberta-base T1           0.821683     0.789400     0.011938     0.703601   \n",
              "             T2           0.802837     0.761134     0.013577     0.673130   \n",
              "             T3           0.832386     0.810512     0.008108     0.701513   \n",
              "             T4           0.080925     0.000000     0.033812     0.000000   \n",
              "             TM1          0.826896     0.792614     0.012819     0.690667   \n",
              "             TM2          0.813097     0.787535     0.008702     0.686192   \n",
              "             TM3          0.827586     0.803443     0.009157     0.698727   \n",
              "\n",
              "                                                     part_of               \\\n",
              "                      min-f1-score std-f1-score max-f1-score min-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.662953     0.011157     0.744292     0.676724   \n",
              "             T2           0.642336     0.021950     0.749474     0.702820   \n",
              "             T3           0.675250     0.005329     0.740566     0.729670   \n",
              "             T4           0.000000     0.056808     0.289157     0.000000   \n",
              "             TM1          0.670537     0.008846     0.758772     0.726437   \n",
              "             TM2          0.671576     0.008973     0.748837     0.740088   \n",
              "             TM3          0.668648     0.007784     0.766520     0.724221   \n",
              "Roberta      T1           0.662050     0.024285     0.764835     0.730361   \n",
              "             T2           0.643836     0.030330     0.777778     0.718182   \n",
              "             T3           0.699454     0.013026     0.798144     0.769231   \n",
              "             T4           0.000000     0.007407     0.321878     0.100045   \n",
              "             TM1          0.697274     0.006072     0.789238     0.749415   \n",
              "             TM2          0.713092     0.007152     0.828054     0.757991   \n",
              "             TM3          0.709497     0.005322     0.823799     0.771930   \n",
              "bert-base    T1           0.574386     0.008526     0.684564     0.634573   \n",
              "             T2           0.522667     0.012943     0.628450     0.586207   \n",
              "             T3           0.598930     0.005760     0.711111     0.648188   \n",
              "             T4           0.000000     0.000000     0.138322     0.031250   \n",
              "             TM1          0.620321     0.005249     0.716895     0.694690   \n",
              "             TM2          0.616231     0.006214     0.692810     0.672566   \n",
              "             TM3          0.642150     0.006246     0.734066     0.703540   \n",
              "roberta-base T1           0.651748     0.016891     0.746137     0.707158   \n",
              "             T2           0.623881     0.018566     0.723769     0.669663   \n",
              "             T3           0.657895     0.014903     0.745098     0.720358   \n",
              "             T4           0.000000     0.000000     0.173318     0.013363   \n",
              "             TM1          0.658263     0.012111     0.751131     0.738197   \n",
              "             TM2          0.660140     0.009889     0.747204     0.733485   \n",
              "             TM3          0.658192     0.013215     0.745098     0.714588   \n",
              "\n",
              "                                            syn                            \\\n",
              "                      std-f1-score max-f1-score min-f1-score std-f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1           0.022261     0.597895     0.526807     0.023576   \n",
              "             T2           0.016871     0.545024     0.502392     0.016426   \n",
              "             T3           0.003565     0.603306     0.505593     0.033071   \n",
              "             T4           0.112552     0.000000     0.000000     0.000000   \n",
              "             TM1          0.011805     0.616034     0.569593     0.016333   \n",
              "             TM2          0.003289     0.586777     0.541761     0.015535   \n",
              "             TM3          0.014963     0.587196     0.552995     0.012320   \n",
              "Roberta      T1           0.012531     0.622407     0.579775     0.015731   \n",
              "             T2           0.020200     0.600877     0.546535     0.021163   \n",
              "             T3           0.009349     0.641822     0.600000     0.014810   \n",
              "             T4           0.080385     0.222222     0.008299     0.090397   \n",
              "             TM1          0.015826     0.646091     0.574380     0.025951   \n",
              "             TM2          0.023326     0.639004     0.605932     0.013809   \n",
              "             TM3          0.018926     0.639130     0.596950     0.015386   \n",
              "bert-base    T1           0.018373     0.419048     0.309859     0.040520   \n",
              "             T2           0.016889     0.310273     0.250000     0.022780   \n",
              "             T3           0.023506     0.402464     0.343249     0.023584   \n",
              "             T4           0.036721     0.022388     0.000000     0.008720   \n",
              "             TM1          0.009327     0.446352     0.410596     0.014221   \n",
              "             TM2          0.008676     0.409186     0.370203     0.013042   \n",
              "             TM3          0.010298     0.437500     0.424116     0.004361   \n",
              "roberta-base T1           0.013355     0.597849     0.545842     0.017984   \n",
              "             T2           0.023624     0.561404     0.513131     0.015437   \n",
              "             T3           0.010589     0.605578     0.564756     0.015188   \n",
              "             T4           0.051620     0.181244     0.000000     0.059557   \n",
              "             TM1          0.004568     0.593220     0.528455     0.026487   \n",
              "             TM2          0.004731     0.570815     0.543568     0.009878   \n",
              "             TM3          0.011037     0.581443     0.533613     0.016158   \n",
              "\n",
              "                      weighted f1-score not random                      \n",
              "                                           max-all   min-all   std-all  \n",
              "model        template                                                   \n",
              "Bert         T1                           0.704759  0.677656  0.008870  \n",
              "             T2                           0.700384  0.662954  0.014347  \n",
              "             T3                           0.712255  0.687849  0.008922  \n",
              "             T4                           0.117960 -0.000833  0.047692  \n",
              "             TM1                          0.717220  0.702957  0.004713  \n",
              "             TM2                          0.706387  0.692681  0.004527  \n",
              "             TM3                          0.714033  0.687597  0.009353  \n",
              "Roberta      T1                           0.760516  0.712373  0.017246  \n",
              "             T2                           0.763227  0.701886  0.021401  \n",
              "             T3                           0.771118  0.754640  0.005476  \n",
              "             T4                           0.259748  0.019451  0.080573  \n",
              "             TM1                          0.764715  0.742481  0.007857  \n",
              "             TM2                          0.766471  0.750697  0.005891  \n",
              "             TM3                          0.766335  0.746589  0.007618  \n",
              "bert-base    T1                           0.563546  0.530574  0.011261  \n",
              "             T2                           0.515928  0.476910  0.015441  \n",
              "             T3                           0.575422  0.551220  0.007997  \n",
              "             T4                           0.072364  0.018762  0.020278  \n",
              "             TM1                          0.616324  0.596140  0.006960  \n",
              "             TM2                          0.577598  0.564060  0.004835  \n",
              "             TM3                          0.620461  0.607989  0.005315  \n",
              "roberta-base T1                           0.723223  0.698204  0.009664  \n",
              "             T2                           0.694423  0.662422  0.015018  \n",
              "             T3                           0.721743  0.700958  0.007519  \n",
              "             T4                           0.069518  0.017802  0.020213  \n",
              "             TM1                          0.722879  0.690554  0.011276  \n",
              "             TM2                          0.710174  0.693918  0.005279  \n",
              "             TM3                          0.712762  0.692731  0.006741  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c9030b7-fd27-4355-9499-f0ff84001de7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">ant</th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyper</th>\n",
              "      <th colspan=\"3\" halign=\"left\">part_of</th>\n",
              "      <th colspan=\"3\" halign=\"left\">syn</th>\n",
              "      <th colspan=\"3\" halign=\"left\">weighted f1-score not random</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-f1-score</th>\n",
              "      <th>min-f1-score</th>\n",
              "      <th>std-f1-score</th>\n",
              "      <th>max-all</th>\n",
              "      <th>min-all</th>\n",
              "      <th>std-all</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.788540</td>\n",
              "      <td>0.754986</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>0.693042</td>\n",
              "      <td>0.662953</td>\n",
              "      <td>0.011157</td>\n",
              "      <td>0.744292</td>\n",
              "      <td>0.676724</td>\n",
              "      <td>0.022261</td>\n",
              "      <td>0.597895</td>\n",
              "      <td>0.526807</td>\n",
              "      <td>0.023576</td>\n",
              "      <td>0.704759</td>\n",
              "      <td>0.677656</td>\n",
              "      <td>0.008870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.784203</td>\n",
              "      <td>0.752044</td>\n",
              "      <td>0.012690</td>\n",
              "      <td>0.697095</td>\n",
              "      <td>0.642336</td>\n",
              "      <td>0.021950</td>\n",
              "      <td>0.749474</td>\n",
              "      <td>0.702820</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.545024</td>\n",
              "      <td>0.502392</td>\n",
              "      <td>0.016426</td>\n",
              "      <td>0.700384</td>\n",
              "      <td>0.662954</td>\n",
              "      <td>0.014347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.797122</td>\n",
              "      <td>0.776406</td>\n",
              "      <td>0.007539</td>\n",
              "      <td>0.687879</td>\n",
              "      <td>0.675250</td>\n",
              "      <td>0.005329</td>\n",
              "      <td>0.740566</td>\n",
              "      <td>0.729670</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>0.603306</td>\n",
              "      <td>0.505593</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>0.712255</td>\n",
              "      <td>0.687849</td>\n",
              "      <td>0.008922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.220661</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099786</td>\n",
              "      <td>0.138219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.056808</td>\n",
              "      <td>0.289157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.112552</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117960</td>\n",
              "      <td>-0.000833</td>\n",
              "      <td>0.047692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.789250</td>\n",
              "      <td>0.006764</td>\n",
              "      <td>0.694891</td>\n",
              "      <td>0.670537</td>\n",
              "      <td>0.008846</td>\n",
              "      <td>0.758772</td>\n",
              "      <td>0.726437</td>\n",
              "      <td>0.011805</td>\n",
              "      <td>0.616034</td>\n",
              "      <td>0.569593</td>\n",
              "      <td>0.016333</td>\n",
              "      <td>0.717220</td>\n",
              "      <td>0.702957</td>\n",
              "      <td>0.004713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.797768</td>\n",
              "      <td>0.765537</td>\n",
              "      <td>0.011892</td>\n",
              "      <td>0.696296</td>\n",
              "      <td>0.671576</td>\n",
              "      <td>0.008973</td>\n",
              "      <td>0.748837</td>\n",
              "      <td>0.740088</td>\n",
              "      <td>0.003289</td>\n",
              "      <td>0.586777</td>\n",
              "      <td>0.541761</td>\n",
              "      <td>0.015535</td>\n",
              "      <td>0.706387</td>\n",
              "      <td>0.692681</td>\n",
              "      <td>0.004527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.812589</td>\n",
              "      <td>0.763948</td>\n",
              "      <td>0.017595</td>\n",
              "      <td>0.691771</td>\n",
              "      <td>0.668648</td>\n",
              "      <td>0.007784</td>\n",
              "      <td>0.766520</td>\n",
              "      <td>0.724221</td>\n",
              "      <td>0.014963</td>\n",
              "      <td>0.587196</td>\n",
              "      <td>0.552995</td>\n",
              "      <td>0.012320</td>\n",
              "      <td>0.714033</td>\n",
              "      <td>0.687597</td>\n",
              "      <td>0.009353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.890469</td>\n",
              "      <td>0.836524</td>\n",
              "      <td>0.019903</td>\n",
              "      <td>0.735724</td>\n",
              "      <td>0.662050</td>\n",
              "      <td>0.024285</td>\n",
              "      <td>0.764835</td>\n",
              "      <td>0.730361</td>\n",
              "      <td>0.012531</td>\n",
              "      <td>0.622407</td>\n",
              "      <td>0.579775</td>\n",
              "      <td>0.015731</td>\n",
              "      <td>0.760516</td>\n",
              "      <td>0.712373</td>\n",
              "      <td>0.017246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.905109</td>\n",
              "      <td>0.830137</td>\n",
              "      <td>0.024201</td>\n",
              "      <td>0.725067</td>\n",
              "      <td>0.643836</td>\n",
              "      <td>0.030330</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.718182</td>\n",
              "      <td>0.020200</td>\n",
              "      <td>0.600877</td>\n",
              "      <td>0.546535</td>\n",
              "      <td>0.021163</td>\n",
              "      <td>0.763227</td>\n",
              "      <td>0.701886</td>\n",
              "      <td>0.021401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.896067</td>\n",
              "      <td>0.874644</td>\n",
              "      <td>0.007488</td>\n",
              "      <td>0.734211</td>\n",
              "      <td>0.699454</td>\n",
              "      <td>0.013026</td>\n",
              "      <td>0.798144</td>\n",
              "      <td>0.769231</td>\n",
              "      <td>0.009349</td>\n",
              "      <td>0.641822</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.014810</td>\n",
              "      <td>0.771118</td>\n",
              "      <td>0.754640</td>\n",
              "      <td>0.005476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.523985</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181011</td>\n",
              "      <td>0.018519</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007407</td>\n",
              "      <td>0.321878</td>\n",
              "      <td>0.100045</td>\n",
              "      <td>0.080385</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.008299</td>\n",
              "      <td>0.090397</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.019451</td>\n",
              "      <td>0.080573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.888252</td>\n",
              "      <td>0.863572</td>\n",
              "      <td>0.008898</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.697274</td>\n",
              "      <td>0.006072</td>\n",
              "      <td>0.789238</td>\n",
              "      <td>0.749415</td>\n",
              "      <td>0.015826</td>\n",
              "      <td>0.646091</td>\n",
              "      <td>0.574380</td>\n",
              "      <td>0.025951</td>\n",
              "      <td>0.764715</td>\n",
              "      <td>0.742481</td>\n",
              "      <td>0.007857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.886657</td>\n",
              "      <td>0.862119</td>\n",
              "      <td>0.008853</td>\n",
              "      <td>0.732432</td>\n",
              "      <td>0.713092</td>\n",
              "      <td>0.007152</td>\n",
              "      <td>0.828054</td>\n",
              "      <td>0.757991</td>\n",
              "      <td>0.023326</td>\n",
              "      <td>0.639004</td>\n",
              "      <td>0.605932</td>\n",
              "      <td>0.013809</td>\n",
              "      <td>0.766471</td>\n",
              "      <td>0.750697</td>\n",
              "      <td>0.005891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.879536</td>\n",
              "      <td>0.861671</td>\n",
              "      <td>0.007292</td>\n",
              "      <td>0.724965</td>\n",
              "      <td>0.709497</td>\n",
              "      <td>0.005322</td>\n",
              "      <td>0.823799</td>\n",
              "      <td>0.771930</td>\n",
              "      <td>0.018926</td>\n",
              "      <td>0.639130</td>\n",
              "      <td>0.596950</td>\n",
              "      <td>0.015386</td>\n",
              "      <td>0.766335</td>\n",
              "      <td>0.746589</td>\n",
              "      <td>0.007618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.563415</td>\n",
              "      <td>0.543478</td>\n",
              "      <td>0.006968</td>\n",
              "      <td>0.598854</td>\n",
              "      <td>0.574386</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>0.684564</td>\n",
              "      <td>0.634573</td>\n",
              "      <td>0.018373</td>\n",
              "      <td>0.419048</td>\n",
              "      <td>0.309859</td>\n",
              "      <td>0.040520</td>\n",
              "      <td>0.563546</td>\n",
              "      <td>0.530574</td>\n",
              "      <td>0.011261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.550336</td>\n",
              "      <td>0.508906</td>\n",
              "      <td>0.015003</td>\n",
              "      <td>0.560647</td>\n",
              "      <td>0.522667</td>\n",
              "      <td>0.012943</td>\n",
              "      <td>0.628450</td>\n",
              "      <td>0.586207</td>\n",
              "      <td>0.016889</td>\n",
              "      <td>0.310273</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.022780</td>\n",
              "      <td>0.515928</td>\n",
              "      <td>0.476910</td>\n",
              "      <td>0.015441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.580132</td>\n",
              "      <td>0.539642</td>\n",
              "      <td>0.015095</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.598930</td>\n",
              "      <td>0.005760</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.648188</td>\n",
              "      <td>0.023506</td>\n",
              "      <td>0.402464</td>\n",
              "      <td>0.343249</td>\n",
              "      <td>0.023584</td>\n",
              "      <td>0.575422</td>\n",
              "      <td>0.551220</td>\n",
              "      <td>0.007997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.189547</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.077721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138322</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.036721</td>\n",
              "      <td>0.022388</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008720</td>\n",
              "      <td>0.072364</td>\n",
              "      <td>0.018762</td>\n",
              "      <td>0.020278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.631136</td>\n",
              "      <td>0.008916</td>\n",
              "      <td>0.634538</td>\n",
              "      <td>0.620321</td>\n",
              "      <td>0.005249</td>\n",
              "      <td>0.716895</td>\n",
              "      <td>0.694690</td>\n",
              "      <td>0.009327</td>\n",
              "      <td>0.446352</td>\n",
              "      <td>0.410596</td>\n",
              "      <td>0.014221</td>\n",
              "      <td>0.616324</td>\n",
              "      <td>0.596140</td>\n",
              "      <td>0.006960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.593123</td>\n",
              "      <td>0.550512</td>\n",
              "      <td>0.016477</td>\n",
              "      <td>0.631436</td>\n",
              "      <td>0.616231</td>\n",
              "      <td>0.006214</td>\n",
              "      <td>0.692810</td>\n",
              "      <td>0.672566</td>\n",
              "      <td>0.008676</td>\n",
              "      <td>0.409186</td>\n",
              "      <td>0.370203</td>\n",
              "      <td>0.013042</td>\n",
              "      <td>0.577598</td>\n",
              "      <td>0.564060</td>\n",
              "      <td>0.004835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.653061</td>\n",
              "      <td>0.615836</td>\n",
              "      <td>0.012420</td>\n",
              "      <td>0.656381</td>\n",
              "      <td>0.642150</td>\n",
              "      <td>0.006246</td>\n",
              "      <td>0.734066</td>\n",
              "      <td>0.703540</td>\n",
              "      <td>0.010298</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>0.424116</td>\n",
              "      <td>0.004361</td>\n",
              "      <td>0.620461</td>\n",
              "      <td>0.607989</td>\n",
              "      <td>0.005315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.821683</td>\n",
              "      <td>0.789400</td>\n",
              "      <td>0.011938</td>\n",
              "      <td>0.703601</td>\n",
              "      <td>0.651748</td>\n",
              "      <td>0.016891</td>\n",
              "      <td>0.746137</td>\n",
              "      <td>0.707158</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.597849</td>\n",
              "      <td>0.545842</td>\n",
              "      <td>0.017984</td>\n",
              "      <td>0.723223</td>\n",
              "      <td>0.698204</td>\n",
              "      <td>0.009664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.802837</td>\n",
              "      <td>0.761134</td>\n",
              "      <td>0.013577</td>\n",
              "      <td>0.673130</td>\n",
              "      <td>0.623881</td>\n",
              "      <td>0.018566</td>\n",
              "      <td>0.723769</td>\n",
              "      <td>0.669663</td>\n",
              "      <td>0.023624</td>\n",
              "      <td>0.561404</td>\n",
              "      <td>0.513131</td>\n",
              "      <td>0.015437</td>\n",
              "      <td>0.694423</td>\n",
              "      <td>0.662422</td>\n",
              "      <td>0.015018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.832386</td>\n",
              "      <td>0.810512</td>\n",
              "      <td>0.008108</td>\n",
              "      <td>0.701513</td>\n",
              "      <td>0.657895</td>\n",
              "      <td>0.014903</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.720358</td>\n",
              "      <td>0.010589</td>\n",
              "      <td>0.605578</td>\n",
              "      <td>0.564756</td>\n",
              "      <td>0.015188</td>\n",
              "      <td>0.721743</td>\n",
              "      <td>0.700958</td>\n",
              "      <td>0.007519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.080925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173318</td>\n",
              "      <td>0.013363</td>\n",
              "      <td>0.051620</td>\n",
              "      <td>0.181244</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.059557</td>\n",
              "      <td>0.069518</td>\n",
              "      <td>0.017802</td>\n",
              "      <td>0.020213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.826896</td>\n",
              "      <td>0.792614</td>\n",
              "      <td>0.012819</td>\n",
              "      <td>0.690667</td>\n",
              "      <td>0.658263</td>\n",
              "      <td>0.012111</td>\n",
              "      <td>0.751131</td>\n",
              "      <td>0.738197</td>\n",
              "      <td>0.004568</td>\n",
              "      <td>0.593220</td>\n",
              "      <td>0.528455</td>\n",
              "      <td>0.026487</td>\n",
              "      <td>0.722879</td>\n",
              "      <td>0.690554</td>\n",
              "      <td>0.011276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.813097</td>\n",
              "      <td>0.787535</td>\n",
              "      <td>0.008702</td>\n",
              "      <td>0.686192</td>\n",
              "      <td>0.660140</td>\n",
              "      <td>0.009889</td>\n",
              "      <td>0.747204</td>\n",
              "      <td>0.733485</td>\n",
              "      <td>0.004731</td>\n",
              "      <td>0.570815</td>\n",
              "      <td>0.543568</td>\n",
              "      <td>0.009878</td>\n",
              "      <td>0.710174</td>\n",
              "      <td>0.693918</td>\n",
              "      <td>0.005279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.827586</td>\n",
              "      <td>0.803443</td>\n",
              "      <td>0.009157</td>\n",
              "      <td>0.698727</td>\n",
              "      <td>0.658192</td>\n",
              "      <td>0.013215</td>\n",
              "      <td>0.745098</td>\n",
              "      <td>0.714588</td>\n",
              "      <td>0.011037</td>\n",
              "      <td>0.581443</td>\n",
              "      <td>0.533613</td>\n",
              "      <td>0.016158</td>\n",
              "      <td>0.712762</td>\n",
              "      <td>0.692731</td>\n",
              "      <td>0.006741</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c9030b7-fd27-4355-9499-f0ff84001de7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0c9030b7-fd27-4355-9499-f0ff84001de7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0c9030b7-fd27-4355-9499-f0ff84001de7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**K&H+N, BLESS, EVALution, ROOT9**\n",
        "To process the results of these datasets. Note that the results for templates `T1`-`T4` correspond to the non-masked models, and templates `TM1`-`TM3`for the masked ones. See the dictionary `templates2abrev` in a cell above."
      ],
      "metadata": {
        "id": "Tdub9432I4PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataframe_results_max_min_measure(dict_res, dataset, measure, list_sub_measures=['precision', 'recall', 'f1-score']):\n",
        "    inc_index = 3*len(list_sub_measures)\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]} \n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            d_measures = dict_res[dataset.lower()][m][t]['flat_reports'][measure]\n",
        "            \n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "\n",
        "            for meas in d_measures: \n",
        "                if meas in list_sub_measures:               \n",
        "                    l_max = dict_df.setdefault('max-'+meas, [])\n",
        "                    l_max.append(np.array(d_measures[meas]).max())\n",
        "                    l_min = dict_df.setdefault('min-'+meas, [])\n",
        "                    l_min.append(np.array(d_measures[meas]).min())\n",
        "                    l_std = dict_df.setdefault('std-'+meas, [])\n",
        "                    l_std.append(np.array(d_measures[meas]).std())\n",
        "\n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    multi_col = pd.MultiIndex.from_tuples(zip([dataset]*inc_index, res_df.columns[3:(3+inc_index)]),sortorder=None)\n",
        "    res_df = res_df.iloc[:,3:(3+inc_index)]\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = multi_col\n",
        "    return res_df\n",
        "\n",
        "def get_dataframe_results_measure(dict_res, dataset, measure, list_sub_measures=['precision', 'recall', 'f1-score']):\n",
        "    inc_index = len(list_sub_measures)\n",
        "    dict_df = {'dataset': [], 'model':[], 'template':[]} \n",
        "    for m in dict_res[dataset.lower()]:\n",
        "        for t in dict_res[dataset.lower()][m]:\n",
        "            d_measures = dict_res[dataset.lower()][m][t]['mean_report'][measure]\n",
        "            \n",
        "            dict_df['dataset'].append(dataset)\n",
        "            dict_df['model'].append(m)\n",
        "            dict_df['template'].append(t)\n",
        "\n",
        "            for meas in d_measures: \n",
        "                if meas in list_sub_measures:               \n",
        "                    l = dict_df.setdefault(meas, [])\n",
        "                    l.append(d_measures[meas])\n",
        "\n",
        "    res_df = pd.DataFrame.from_dict(dict_df)\n",
        "    res_df['model'] = res_df['model'].apply(lambda x : models2abrev[x])\n",
        "    res_df['template'] = res_df['template'].apply(lambda x : templates2abrev[x])\n",
        "    multi_row = pd.MultiIndex.from_frame(res_df.iloc[:,1:3],sortorder=None)\n",
        "    multi_col = pd.MultiIndex.from_tuples(zip([dataset]*inc_index, res_df.columns[3:(3+inc_index)]),sortorder=None)\n",
        "    res_df = res_df.iloc[:,3:(3+inc_index)]\n",
        "    res_df.index = multi_row\n",
        "    res_df.columns = multi_col\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "35lg0QNuJJlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']\n",
        "list_complete_res = []\n",
        "\n",
        "for one_dataset in all_datasets:\n",
        "    if one_dataset.lower() in list(dict_res.keys()):\n",
        "        df_res = get_dataframe_results_measure(dict_res, one_dataset, 'weighted avg') # one of 'weighted avg' and 'macro avg'\n",
        "        list_complete_res.append(df_res)\n",
        "    else:\n",
        "        print(\"WARNING: There are not results for dataset \" + one_dataset)\n",
        "df_datasets_final = pd.concat(list_complete_res, axis = 1)\n",
        "\n",
        "#change column order\n",
        "order1 = {'k&h+n':0,'bless':1,'evalution':2, 'root09':3}\n",
        "order2 = {'precision':0,'recall':1,'f1-score':2}\n",
        "multi_col_list = list(df_datasets_final.columns)\n",
        "multi_col_list.sort(key=lambda x: 10*order1[x[0].lower()] + order2[x[1].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "df_datasets_final = pd.DataFrame(df_datasets_final, columns=multi_col)\n",
        "df_datasets_final.sort_index(level=['model','template'], inplace=True)"
      ],
      "metadata": {
        "id": "ajsY7HIUJPl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOTAS_FILE_REST = DIR_RESULTS + 'sotas_results_literature/khnBlessEvalRoot_Sotas_RC.txt'\n",
        "sotas_res = pd.read_csv(SOTAS_FILE_REST, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "multi_row_tuples = list(zip(['Sota']*sotas_res.shape[0], list(sotas_res.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "sotas_res.index=multi_row\n",
        "df_res_sotas = pd.concat([df_datasets_final,sotas_res])\n",
        "\n",
        "#change column order\n",
        "order1 = {'k&h+n':0,'bless':1,'evalution':2, 'root09':3}\n",
        "order2 = {'precision':0,'recall':1,'f1-score':2}\n",
        "multi_col_list = list(df_res_sotas.columns)\n",
        "multi_col_list.sort(key=lambda x: 10*order1[x[0].lower()] + order2[x[1].lower()])\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "\n",
        "df_res_sotas = pd.DataFrame(df_res_sotas, columns=multi_col)\n",
        "df_res_sotas.apply(round, ndigits=30)\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "aDAQ_3tS3mMb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "070bab1a-17d6-4638-e23b-6ead0b02a750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          K&H+N                         BLESS            \\\n",
              "                      precision    recall  f1-score precision    recall   \n",
              "model        template                                                     \n",
              "Bert         T1        0.989279  0.989372  0.989283  0.952062  0.950972   \n",
              "             T2        0.988852  0.988955  0.988862  0.949638  0.948079   \n",
              "             T3        0.989652  0.989761  0.989652  0.952650  0.951996   \n",
              "             T4        0.740641  0.587883  0.510361  0.243637  0.200090   \n",
              "             TM1       0.987078  0.987216  0.987016  0.942206  0.940726   \n",
              "             TM2       0.987278  0.987438  0.987288  0.945898  0.944463   \n",
              "             TM3       0.985504  0.985755  0.985431  0.947861  0.946723   \n",
              "Roberta      T1        0.988899  0.988968  0.988911  0.954583  0.953895   \n",
              "             T2        0.988876  0.988955  0.988888  0.955340  0.954347   \n",
              "             T3        0.988944  0.989024  0.988954  0.956224  0.955492   \n",
              "             T4        0.603157  0.325923  0.311507  0.511042  0.194275   \n",
              "             TM1       0.988515  0.988593  0.988496  0.948353  0.946241   \n",
              "             TM2       0.987667  0.987772  0.987678  0.947049  0.945482   \n",
              "             TM3       0.985518  0.985185  0.985282  0.951303  0.950460   \n",
              "bert-base    T1        0.987638  0.987689  0.987644  0.943748  0.941871   \n",
              "             T2        0.986912  0.986993  0.986923  0.943035  0.941058   \n",
              "             T3        0.986869  0.986771  0.986793  0.943626  0.942203   \n",
              "             T4        0.547507  0.428935  0.316018  0.369564  0.228386   \n",
              "             TM1       0.986135  0.986256  0.986119  0.938743  0.936055   \n",
              "             TM2       0.985471  0.985672  0.985414  0.940231  0.939491   \n",
              "             TM3       0.985178  0.985324  0.985048  0.940717  0.939220   \n",
              "roberta-base T1        0.983463  0.983724  0.983494  0.950049  0.948893   \n",
              "             T2        0.987663  0.987703  0.987669  0.948488  0.947235   \n",
              "             T3        0.987439  0.987480  0.987445  0.949862  0.948772   \n",
              "             T4        0.660224  0.455352  0.299090  0.504303  0.138557   \n",
              "             TM1       0.986559  0.986311  0.986338  0.941440  0.939882   \n",
              "             TM2       0.982519  0.982848  0.982503  0.945688  0.944252   \n",
              "             TM3       0.986242  0.986298  0.986152  0.945586  0.943890   \n",
              "Sota         LexNET    0.985000  0.986000  0.985000  0.894000  0.893000   \n",
              "             KEML      0.993000  0.993000  0.993000  0.944000  0.943000   \n",
              "             SphereRE  0.990000  0.989000  0.990000  0.938000  0.938000   \n",
              "             RelBERT        NaN       NaN  0.949000       NaN       NaN   \n",
              "\n",
              "                                EVALution                        ROOT09  \\\n",
              "                       f1-score precision    recall  f1-score precision   \n",
              "model        template                                                     \n",
              "Bert         T1        0.951223  0.747669  0.747562  0.747197  0.926534   \n",
              "             T2        0.948395  0.738647  0.739003  0.736617  0.930032   \n",
              "             T3        0.952156  0.753333  0.750271  0.750660  0.930945   \n",
              "             T4        0.087649  0.115621  0.148537  0.052673  0.929053   \n",
              "             TM1       0.940908  0.754933  0.743987  0.744853  0.926502   \n",
              "             TM2       0.944739  0.738177  0.728711  0.722481  0.925350   \n",
              "             TM3       0.946968  0.729665  0.726327  0.723992  0.926887   \n",
              "Roberta      T1        0.953960  0.768848  0.764789  0.764326  0.937188   \n",
              "             T2        0.954563  0.758895  0.759372  0.757473  0.936314   \n",
              "             T3        0.955660  0.773299  0.771181  0.770558  0.938197   \n",
              "             T4        0.132698  0.229777  0.190683  0.087452  0.935851   \n",
              "             TM1       0.946602  0.771562  0.761863  0.760673  0.936317   \n",
              "             TM2       0.945714  0.771351  0.764680  0.764198  0.929912   \n",
              "             TM3       0.950598  0.774145  0.754171  0.746288  0.926417   \n",
              "bert-base    T1        0.942220  0.690299  0.690574  0.689041  0.925621   \n",
              "             T2        0.941427  0.674987  0.672264  0.672354  0.919290   \n",
              "             T3        0.942477  0.696403  0.693716  0.694414  0.922468   \n",
              "             T4        0.165348  0.213162  0.218202  0.119044  0.920844   \n",
              "             TM1       0.936486  0.706863  0.699675  0.698001  0.917368   \n",
              "             TM2       0.939514  0.689736  0.686457  0.683546  0.917999   \n",
              "             TM3       0.939404  0.696653  0.691874  0.686207  0.917967   \n",
              "roberta-base T1        0.949115  0.748904  0.744204  0.745112  0.932277   \n",
              "             T2        0.947487  0.746132  0.744420  0.744220  0.931237   \n",
              "             T3        0.949017  0.756320  0.753304  0.753849  0.934262   \n",
              "             T4        0.042571  0.120698  0.094908  0.023306  0.923711   \n",
              "             TM1       0.940137  0.758330  0.745179  0.747326  0.927174   \n",
              "             TM2       0.944434  0.740216  0.723835  0.727344  0.925764   \n",
              "             TM3       0.944177  0.740151  0.737486  0.729413  0.924481   \n",
              "Sota         LexNET    0.893000  0.601000  0.607000  0.600000  0.813000   \n",
              "             KEML      0.944000  0.663000  0.660000  0.660000  0.878000   \n",
              "             SphereRE  0.938000  0.620000  0.621000  0.620000  0.860000   \n",
              "             RelBERT   0.921000       NaN       NaN  0.701000       NaN   \n",
              "\n",
              "                                           \n",
              "                         recall  f1-score  \n",
              "model        template                      \n",
              "Bert         T1        0.926418  0.926258  \n",
              "             T2        0.928549  0.928900  \n",
              "             T3        0.931056  0.930907  \n",
              "             T4        0.927922  0.928181  \n",
              "             TM1       0.925227  0.925272  \n",
              "             TM2       0.924726  0.924586  \n",
              "             TM3       0.923660  0.923911  \n",
              "Roberta      T1        0.936133  0.936310  \n",
              "             T2        0.935945  0.936030  \n",
              "             T3        0.937073  0.937334  \n",
              "             T4        0.933626  0.934019  \n",
              "             TM1       0.935569  0.935642  \n",
              "             TM2       0.928612  0.928193  \n",
              "             TM3       0.926042  0.925744  \n",
              "bert-base    T1        0.924036  0.924396  \n",
              "             T2        0.917706  0.918047  \n",
              "             T3        0.921216  0.921417  \n",
              "             T4        0.919210  0.919461  \n",
              "             TM1       0.917330  0.917135  \n",
              "             TM2       0.916578  0.916741  \n",
              "             TM3       0.914886  0.915287  \n",
              "roberta-base T1        0.931181  0.931330  \n",
              "             T2        0.930743  0.930811  \n",
              "             T3        0.933124  0.933323  \n",
              "             T4        0.923347  0.923347  \n",
              "             TM1       0.925729  0.925664  \n",
              "             TM2       0.925541  0.925317  \n",
              "             TM3       0.924287  0.923989  \n",
              "Sota         LexNET    0.814000  0.813000  \n",
              "             KEML      0.877000  0.878000  \n",
              "             SphereRE  0.862000  0.861000  \n",
              "             RelBERT        NaN  0.910000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-27216607-9d11-4365-8dca-69ed59327071\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">K&amp;H+N</th>\n",
              "      <th colspan=\"3\" halign=\"left\">BLESS</th>\n",
              "      <th colspan=\"3\" halign=\"left\">EVALution</th>\n",
              "      <th colspan=\"3\" halign=\"left\">ROOT09</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.989279</td>\n",
              "      <td>0.989372</td>\n",
              "      <td>0.989283</td>\n",
              "      <td>0.952062</td>\n",
              "      <td>0.950972</td>\n",
              "      <td>0.951223</td>\n",
              "      <td>0.747669</td>\n",
              "      <td>0.747562</td>\n",
              "      <td>0.747197</td>\n",
              "      <td>0.926534</td>\n",
              "      <td>0.926418</td>\n",
              "      <td>0.926258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988852</td>\n",
              "      <td>0.988955</td>\n",
              "      <td>0.988862</td>\n",
              "      <td>0.949638</td>\n",
              "      <td>0.948079</td>\n",
              "      <td>0.948395</td>\n",
              "      <td>0.738647</td>\n",
              "      <td>0.739003</td>\n",
              "      <td>0.736617</td>\n",
              "      <td>0.930032</td>\n",
              "      <td>0.928549</td>\n",
              "      <td>0.928900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.989652</td>\n",
              "      <td>0.989761</td>\n",
              "      <td>0.989652</td>\n",
              "      <td>0.952650</td>\n",
              "      <td>0.951996</td>\n",
              "      <td>0.952156</td>\n",
              "      <td>0.753333</td>\n",
              "      <td>0.750271</td>\n",
              "      <td>0.750660</td>\n",
              "      <td>0.930945</td>\n",
              "      <td>0.931056</td>\n",
              "      <td>0.930907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.740641</td>\n",
              "      <td>0.587883</td>\n",
              "      <td>0.510361</td>\n",
              "      <td>0.243637</td>\n",
              "      <td>0.200090</td>\n",
              "      <td>0.087649</td>\n",
              "      <td>0.115621</td>\n",
              "      <td>0.148537</td>\n",
              "      <td>0.052673</td>\n",
              "      <td>0.929053</td>\n",
              "      <td>0.927922</td>\n",
              "      <td>0.928181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.987078</td>\n",
              "      <td>0.987216</td>\n",
              "      <td>0.987016</td>\n",
              "      <td>0.942206</td>\n",
              "      <td>0.940726</td>\n",
              "      <td>0.940908</td>\n",
              "      <td>0.754933</td>\n",
              "      <td>0.743987</td>\n",
              "      <td>0.744853</td>\n",
              "      <td>0.926502</td>\n",
              "      <td>0.925227</td>\n",
              "      <td>0.925272</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.987278</td>\n",
              "      <td>0.987438</td>\n",
              "      <td>0.987288</td>\n",
              "      <td>0.945898</td>\n",
              "      <td>0.944463</td>\n",
              "      <td>0.944739</td>\n",
              "      <td>0.738177</td>\n",
              "      <td>0.728711</td>\n",
              "      <td>0.722481</td>\n",
              "      <td>0.925350</td>\n",
              "      <td>0.924726</td>\n",
              "      <td>0.924586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985504</td>\n",
              "      <td>0.985755</td>\n",
              "      <td>0.985431</td>\n",
              "      <td>0.947861</td>\n",
              "      <td>0.946723</td>\n",
              "      <td>0.946968</td>\n",
              "      <td>0.729665</td>\n",
              "      <td>0.726327</td>\n",
              "      <td>0.723992</td>\n",
              "      <td>0.926887</td>\n",
              "      <td>0.923660</td>\n",
              "      <td>0.923911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.988899</td>\n",
              "      <td>0.988968</td>\n",
              "      <td>0.988911</td>\n",
              "      <td>0.954583</td>\n",
              "      <td>0.953895</td>\n",
              "      <td>0.953960</td>\n",
              "      <td>0.768848</td>\n",
              "      <td>0.764789</td>\n",
              "      <td>0.764326</td>\n",
              "      <td>0.937188</td>\n",
              "      <td>0.936133</td>\n",
              "      <td>0.936310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.988876</td>\n",
              "      <td>0.988955</td>\n",
              "      <td>0.988888</td>\n",
              "      <td>0.955340</td>\n",
              "      <td>0.954347</td>\n",
              "      <td>0.954563</td>\n",
              "      <td>0.758895</td>\n",
              "      <td>0.759372</td>\n",
              "      <td>0.757473</td>\n",
              "      <td>0.936314</td>\n",
              "      <td>0.935945</td>\n",
              "      <td>0.936030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.988944</td>\n",
              "      <td>0.989024</td>\n",
              "      <td>0.988954</td>\n",
              "      <td>0.956224</td>\n",
              "      <td>0.955492</td>\n",
              "      <td>0.955660</td>\n",
              "      <td>0.773299</td>\n",
              "      <td>0.771181</td>\n",
              "      <td>0.770558</td>\n",
              "      <td>0.938197</td>\n",
              "      <td>0.937073</td>\n",
              "      <td>0.937334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.603157</td>\n",
              "      <td>0.325923</td>\n",
              "      <td>0.311507</td>\n",
              "      <td>0.511042</td>\n",
              "      <td>0.194275</td>\n",
              "      <td>0.132698</td>\n",
              "      <td>0.229777</td>\n",
              "      <td>0.190683</td>\n",
              "      <td>0.087452</td>\n",
              "      <td>0.935851</td>\n",
              "      <td>0.933626</td>\n",
              "      <td>0.934019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.988515</td>\n",
              "      <td>0.988593</td>\n",
              "      <td>0.988496</td>\n",
              "      <td>0.948353</td>\n",
              "      <td>0.946241</td>\n",
              "      <td>0.946602</td>\n",
              "      <td>0.771562</td>\n",
              "      <td>0.761863</td>\n",
              "      <td>0.760673</td>\n",
              "      <td>0.936317</td>\n",
              "      <td>0.935569</td>\n",
              "      <td>0.935642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.987667</td>\n",
              "      <td>0.987772</td>\n",
              "      <td>0.987678</td>\n",
              "      <td>0.947049</td>\n",
              "      <td>0.945482</td>\n",
              "      <td>0.945714</td>\n",
              "      <td>0.771351</td>\n",
              "      <td>0.764680</td>\n",
              "      <td>0.764198</td>\n",
              "      <td>0.929912</td>\n",
              "      <td>0.928612</td>\n",
              "      <td>0.928193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985518</td>\n",
              "      <td>0.985185</td>\n",
              "      <td>0.985282</td>\n",
              "      <td>0.951303</td>\n",
              "      <td>0.950460</td>\n",
              "      <td>0.950598</td>\n",
              "      <td>0.774145</td>\n",
              "      <td>0.754171</td>\n",
              "      <td>0.746288</td>\n",
              "      <td>0.926417</td>\n",
              "      <td>0.926042</td>\n",
              "      <td>0.925744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.987638</td>\n",
              "      <td>0.987689</td>\n",
              "      <td>0.987644</td>\n",
              "      <td>0.943748</td>\n",
              "      <td>0.941871</td>\n",
              "      <td>0.942220</td>\n",
              "      <td>0.690299</td>\n",
              "      <td>0.690574</td>\n",
              "      <td>0.689041</td>\n",
              "      <td>0.925621</td>\n",
              "      <td>0.924036</td>\n",
              "      <td>0.924396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.986912</td>\n",
              "      <td>0.986993</td>\n",
              "      <td>0.986923</td>\n",
              "      <td>0.943035</td>\n",
              "      <td>0.941058</td>\n",
              "      <td>0.941427</td>\n",
              "      <td>0.674987</td>\n",
              "      <td>0.672264</td>\n",
              "      <td>0.672354</td>\n",
              "      <td>0.919290</td>\n",
              "      <td>0.917706</td>\n",
              "      <td>0.918047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.986869</td>\n",
              "      <td>0.986771</td>\n",
              "      <td>0.986793</td>\n",
              "      <td>0.943626</td>\n",
              "      <td>0.942203</td>\n",
              "      <td>0.942477</td>\n",
              "      <td>0.696403</td>\n",
              "      <td>0.693716</td>\n",
              "      <td>0.694414</td>\n",
              "      <td>0.922468</td>\n",
              "      <td>0.921216</td>\n",
              "      <td>0.921417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.547507</td>\n",
              "      <td>0.428935</td>\n",
              "      <td>0.316018</td>\n",
              "      <td>0.369564</td>\n",
              "      <td>0.228386</td>\n",
              "      <td>0.165348</td>\n",
              "      <td>0.213162</td>\n",
              "      <td>0.218202</td>\n",
              "      <td>0.119044</td>\n",
              "      <td>0.920844</td>\n",
              "      <td>0.919210</td>\n",
              "      <td>0.919461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.986135</td>\n",
              "      <td>0.986256</td>\n",
              "      <td>0.986119</td>\n",
              "      <td>0.938743</td>\n",
              "      <td>0.936055</td>\n",
              "      <td>0.936486</td>\n",
              "      <td>0.706863</td>\n",
              "      <td>0.699675</td>\n",
              "      <td>0.698001</td>\n",
              "      <td>0.917368</td>\n",
              "      <td>0.917330</td>\n",
              "      <td>0.917135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.985471</td>\n",
              "      <td>0.985672</td>\n",
              "      <td>0.985414</td>\n",
              "      <td>0.940231</td>\n",
              "      <td>0.939491</td>\n",
              "      <td>0.939514</td>\n",
              "      <td>0.689736</td>\n",
              "      <td>0.686457</td>\n",
              "      <td>0.683546</td>\n",
              "      <td>0.917999</td>\n",
              "      <td>0.916578</td>\n",
              "      <td>0.916741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.985178</td>\n",
              "      <td>0.985324</td>\n",
              "      <td>0.985048</td>\n",
              "      <td>0.940717</td>\n",
              "      <td>0.939220</td>\n",
              "      <td>0.939404</td>\n",
              "      <td>0.696653</td>\n",
              "      <td>0.691874</td>\n",
              "      <td>0.686207</td>\n",
              "      <td>0.917967</td>\n",
              "      <td>0.914886</td>\n",
              "      <td>0.915287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.983463</td>\n",
              "      <td>0.983724</td>\n",
              "      <td>0.983494</td>\n",
              "      <td>0.950049</td>\n",
              "      <td>0.948893</td>\n",
              "      <td>0.949115</td>\n",
              "      <td>0.748904</td>\n",
              "      <td>0.744204</td>\n",
              "      <td>0.745112</td>\n",
              "      <td>0.932277</td>\n",
              "      <td>0.931181</td>\n",
              "      <td>0.931330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.987663</td>\n",
              "      <td>0.987703</td>\n",
              "      <td>0.987669</td>\n",
              "      <td>0.948488</td>\n",
              "      <td>0.947235</td>\n",
              "      <td>0.947487</td>\n",
              "      <td>0.746132</td>\n",
              "      <td>0.744420</td>\n",
              "      <td>0.744220</td>\n",
              "      <td>0.931237</td>\n",
              "      <td>0.930743</td>\n",
              "      <td>0.930811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.987439</td>\n",
              "      <td>0.987480</td>\n",
              "      <td>0.987445</td>\n",
              "      <td>0.949862</td>\n",
              "      <td>0.948772</td>\n",
              "      <td>0.949017</td>\n",
              "      <td>0.756320</td>\n",
              "      <td>0.753304</td>\n",
              "      <td>0.753849</td>\n",
              "      <td>0.934262</td>\n",
              "      <td>0.933124</td>\n",
              "      <td>0.933323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>0.660224</td>\n",
              "      <td>0.455352</td>\n",
              "      <td>0.299090</td>\n",
              "      <td>0.504303</td>\n",
              "      <td>0.138557</td>\n",
              "      <td>0.042571</td>\n",
              "      <td>0.120698</td>\n",
              "      <td>0.094908</td>\n",
              "      <td>0.023306</td>\n",
              "      <td>0.923711</td>\n",
              "      <td>0.923347</td>\n",
              "      <td>0.923347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.986559</td>\n",
              "      <td>0.986311</td>\n",
              "      <td>0.986338</td>\n",
              "      <td>0.941440</td>\n",
              "      <td>0.939882</td>\n",
              "      <td>0.940137</td>\n",
              "      <td>0.758330</td>\n",
              "      <td>0.745179</td>\n",
              "      <td>0.747326</td>\n",
              "      <td>0.927174</td>\n",
              "      <td>0.925729</td>\n",
              "      <td>0.925664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.982519</td>\n",
              "      <td>0.982848</td>\n",
              "      <td>0.982503</td>\n",
              "      <td>0.945688</td>\n",
              "      <td>0.944252</td>\n",
              "      <td>0.944434</td>\n",
              "      <td>0.740216</td>\n",
              "      <td>0.723835</td>\n",
              "      <td>0.727344</td>\n",
              "      <td>0.925764</td>\n",
              "      <td>0.925541</td>\n",
              "      <td>0.925317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.986242</td>\n",
              "      <td>0.986298</td>\n",
              "      <td>0.986152</td>\n",
              "      <td>0.945586</td>\n",
              "      <td>0.943890</td>\n",
              "      <td>0.944177</td>\n",
              "      <td>0.740151</td>\n",
              "      <td>0.737486</td>\n",
              "      <td>0.729413</td>\n",
              "      <td>0.924481</td>\n",
              "      <td>0.924287</td>\n",
              "      <td>0.923989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"4\" valign=\"top\">Sota</th>\n",
              "      <th>LexNET</th>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.986000</td>\n",
              "      <td>0.985000</td>\n",
              "      <td>0.894000</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.893000</td>\n",
              "      <td>0.601000</td>\n",
              "      <td>0.607000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.813000</td>\n",
              "      <td>0.814000</td>\n",
              "      <td>0.813000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KEML</th>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.993000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.943000</td>\n",
              "      <td>0.944000</td>\n",
              "      <td>0.663000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>0.878000</td>\n",
              "      <td>0.877000</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SphereRE</th>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.989000</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.938000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.621000</td>\n",
              "      <td>0.620000</td>\n",
              "      <td>0.860000</td>\n",
              "      <td>0.862000</td>\n",
              "      <td>0.861000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RelBERT</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.949000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.921000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.701000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.910000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27216607-9d11-4365-8dca-69ed59327071')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-27216607-9d11-4365-8dca-69ed59327071 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-27216607-9d11-4365-8dca-69ed59327071');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_datasets = ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']\n",
        "list_complete_res = []\n",
        "\n",
        "for one_dataset in all_datasets:\n",
        "    if one_dataset.lower() in list(dict_res.keys()):\n",
        "        df_res = get_dataframe_results_max_min_measure(dict_res, one_dataset, 'weighted avg') # one of 'weighted avg' and 'macro avg'\n",
        "        list_complete_res.append(df_res)\n",
        "    else:\n",
        "        print(\"WARNING: There are not results for dataset \" + one_dataset)\n",
        "df_datasets_max_min_final = pd.concat(list_complete_res, axis = 1)\n",
        "\n",
        "multi_col = pd.MultiIndex.from_tuples(multi_col_list)\n",
        "df_datasets_max_min_final = pd.DataFrame(df_datasets_max_min_final, columns=multi_col)\n",
        "df_datasets_max_min_final.sort_index(level=['model','template'], inplace=True)\n",
        "df_datasets_max_min_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "tf_86X5VSyPJ",
        "outputId": "859c7fa0-10c2-4583-e243-6f81cf8e3b45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          K&H+N                     BLESS                  \\\n",
              "                      precision recall f1-score precision recall f1-score   \n",
              "model        template                                                       \n",
              "Bert         T1             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "Roberta      T1             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "bert-base    T1             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "roberta-base T1             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN   \n",
              "\n",
              "                      EVALution                    ROOT09                  \n",
              "                      precision recall f1-score precision recall f1-score  \n",
              "model        template                                                      \n",
              "Bert         T1             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "Roberta      T1             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "bert-base    T1             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "roberta-base T1             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T2             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T3             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             T4             NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM1            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM2            NaN    NaN      NaN       NaN    NaN      NaN  \n",
              "             TM3            NaN    NaN      NaN       NaN    NaN      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-602def96-6b8c-455c-9cac-196e6518e5e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">K&amp;H+N</th>\n",
              "      <th colspan=\"3\" halign=\"left\">BLESS</th>\n",
              "      <th colspan=\"3\" halign=\"left\">EVALution</th>\n",
              "      <th colspan=\"3\" halign=\"left\">ROOT09</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1-score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"7\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-602def96-6b8c-455c-9cac-196e6518e5e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-602def96-6b8c-455c-9cac-196e6518e5e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-602def96-6b8c-455c-9cac-196e6518e5e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Graded LE: Hyperlex**\n",
        "It is reported the Spearman correlation between the median human rates contained in Hyperlex dataset and:\n",
        " - The calculated score by means of the logits (reported results in the paper).\n",
        " - The score considering only the probability to be an hyponym.\n",
        " - Instead of using the logits, the calculated probabilities for each label are used, except one of them (take into account that the sum of all probabilities is $1$, thus one of the values is useless for fitting a linear model)."
      ],
      "metadata": {
        "id": "WtuJP0UsJjgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hyperlex_list = []\n",
        "for d in ['hyperlex-lexical', 'hyperlex-random']:\n",
        "    df_data_hyperlex_hyp = get_dataframe_results_measure(dict_res, d, 'hyp', ['correlation'])\n",
        "    df_data_hyperlex_hyp.columns = pd.MultiIndex.from_tuples([(d,'hyp-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_hyp)\n",
        "\n",
        "    df_data_hyperlex_logit = get_dataframe_results_measure(dict_res, d, 'spearman_logit', ['correlation'])\n",
        "    df_data_hyperlex_logit.columns = pd.MultiIndex.from_tuples([(d,'logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_logit)\n",
        "\n",
        "    df_data_hyperlex_prob = get_dataframe_results_measure(dict_res, d, 'spearman_prob', ['correlation'])\n",
        "    df_data_hyperlex_prob.columns = pd.MultiIndex.from_tuples([(d,'prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_prob)\n",
        "\n",
        "    df_data_hyperlex_nouns_logit = get_dataframe_results_measure(dict_res, d, 'spearman_nouns_logit', ['correlation'])\n",
        "    df_data_hyperlex_nouns_logit.columns = pd.MultiIndex.from_tuples([(d,'noun-logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_nouns_logit)\n",
        "\n",
        "    df_data_hyperlex_verbs_logit = get_dataframe_results_measure(dict_res, d, 'spearman_verbs_logit', ['correlation'])\n",
        "    df_data_hyperlex_verbs_logit.columns = pd.MultiIndex.from_tuples([(d,'verb-logit-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_verbs_logit)\n",
        "\n",
        "    df_data_hyperlex_nouns_prob = get_dataframe_results_measure(dict_res, d, 'spearman_nouns_prob', ['correlation'])\n",
        "    df_data_hyperlex_nouns_prob.columns = pd.MultiIndex.from_tuples([(d,'noun-prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_nouns_prob)\n",
        "\n",
        "    df_data_hyperlex_verbs_prob = get_dataframe_results_measure(dict_res, d, 'spearman_verbs_prob', ['correlation'])\n",
        "    df_data_hyperlex_verbs_prob.columns = pd.MultiIndex.from_tuples([(d,'verb-prob-correlation')])\n",
        "    df_hyperlex_list.append(df_data_hyperlex_verbs_prob)\n",
        "    \n",
        "\n",
        "df_data_hyperlex = pd.concat(df_hyperlex_list, axis=1)\n",
        "df_data_hyperlex.sort_index(level=['model','template'], inplace=True)\n",
        "df_data_hyperlex"
      ],
      "metadata": {
        "id": "1gF2TlmAJpW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "outputId": "2c043ccc-3f75-4ff5-dc67-ba29626aa462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      hyperlex-lexical                                     \\\n",
              "                       hyp-correlation logit-correlation prob-correlation   \n",
              "model        template                                                       \n",
              "Bert         T1               0.657046          0.686073         0.694415   \n",
              "             T2               0.450562          0.402034         0.416582   \n",
              "             T3               0.667674          0.746582         0.755574   \n",
              "             TM1              0.671808          0.766412         0.753963   \n",
              "             TM2              0.589105          0.656697         0.672861   \n",
              "             TM3              0.686035          0.741279         0.755081   \n",
              "Roberta      T1               0.738920          0.754927         0.771282   \n",
              "             T2               0.375061          0.286757         0.297980   \n",
              "             T3               0.636995          0.669128         0.671147   \n",
              "             TM1              0.736504          0.788514         0.796706   \n",
              "             TM2              0.600176          0.653700         0.669882   \n",
              "             TM3              0.721263          0.794234         0.783369   \n",
              "bert-base    T1               0.498647          0.471335         0.474081   \n",
              "             T2               0.406193          0.374065         0.378994   \n",
              "             T3               0.588330          0.614221         0.622301   \n",
              "             TM1              0.569934          0.597293         0.599501   \n",
              "             TM2              0.561259          0.574906         0.610851   \n",
              "             TM3              0.573250          0.583948         0.579535   \n",
              "roberta-base T1               0.651845          0.677077         0.725030   \n",
              "             T2               0.483763          0.406815         0.400610   \n",
              "             T3               0.641070          0.625525         0.664061   \n",
              "             TM1              0.690944          0.736169         0.776089   \n",
              "             TM2              0.641001          0.710913         0.725829   \n",
              "             TM3              0.697356          0.757284         0.770732   \n",
              "\n",
              "                                                                     \\\n",
              "                      noun-logit-correlation verb-logit-correlation   \n",
              "model        template                                                 \n",
              "Bert         T1                     0.737330               0.498642   \n",
              "             T2                     0.433256               0.285916   \n",
              "             T3                     0.781277               0.622566   \n",
              "             TM1                    0.807058               0.672470   \n",
              "             TM2                    0.716757               0.477823   \n",
              "             TM3                    0.781051               0.632722   \n",
              "Roberta      T1                     0.787916               0.531692   \n",
              "             T2                     0.350415               0.063102   \n",
              "             T3                     0.690485               0.515506   \n",
              "             TM1                    0.836832               0.612222   \n",
              "             TM2                    0.705038               0.417199   \n",
              "             TM3                    0.828178               0.656274   \n",
              "bert-base    T1                     0.556778               0.172960   \n",
              "             T2                     0.445601               0.116163   \n",
              "             T3                     0.690729               0.311820   \n",
              "             TM1                    0.680286               0.379775   \n",
              "             TM2                    0.656103               0.276512   \n",
              "             TM3                    0.664762               0.355509   \n",
              "roberta-base T1                     0.713328               0.542603   \n",
              "             T2                     0.482727               0.166631   \n",
              "             T3                     0.692522               0.391105   \n",
              "             TM1                    0.799714               0.552940   \n",
              "             TM2                    0.756986               0.525289   \n",
              "             TM3                    0.806950               0.633745   \n",
              "\n",
              "                                                                   \\\n",
              "                      noun-prob-correlation verb-prob-correlation   \n",
              "model        template                                               \n",
              "Bert         T1                    0.750145              0.459302   \n",
              "             T2                    0.456257              0.248895   \n",
              "             T3                    0.795455              0.597384   \n",
              "             TM1                   0.794629              0.609317   \n",
              "             TM2                   0.728138              0.481895   \n",
              "             TM3                   0.793492              0.609348   \n",
              "Roberta      T1                    0.808948              0.517498   \n",
              "             T2                    0.374889             -0.008955   \n",
              "             T3                    0.699184              0.485816   \n",
              "             TM1                   0.829112              0.618363   \n",
              "             TM2                   0.715934              0.410055   \n",
              "             TM3                   0.827253              0.600486   \n",
              "bert-base    T1                    0.551468              0.137220   \n",
              "             T2                    0.442557              0.126524   \n",
              "             T3                    0.713881              0.309381   \n",
              "             TM1                   0.673157              0.267178   \n",
              "             TM2                   0.685900              0.266175   \n",
              "             TM3                   0.647579              0.316415   \n",
              "roberta-base T1                    0.764511              0.547946   \n",
              "             T2                    0.475401              0.085916   \n",
              "             T3                    0.725598              0.441986   \n",
              "             TM1                   0.819748              0.549569   \n",
              "             TM2                   0.778836              0.442391   \n",
              "             TM3                   0.808939              0.639037   \n",
              "\n",
              "                      hyperlex-random                                     \\\n",
              "                      hyp-correlation logit-correlation prob-correlation   \n",
              "model        template                                                      \n",
              "Bert         T1              0.623897          0.643511         0.668539   \n",
              "             T2              0.540876          0.577157         0.603452   \n",
              "             T3              0.650930          0.727820         0.757310   \n",
              "             TM1             0.721062          0.800457         0.790645   \n",
              "             TM2             0.703471          0.778105         0.784275   \n",
              "             TM3             0.737478          0.794417         0.799238   \n",
              "Roberta      T1              0.708957          0.740859         0.757459   \n",
              "             T2              0.096328          0.152353         0.146603   \n",
              "             T3              0.712420          0.773941         0.794968   \n",
              "             TM1             0.779260          0.828275         0.828471   \n",
              "             TM2             0.627888          0.749403         0.743976   \n",
              "             TM3             0.774364          0.814334         0.791946   \n",
              "bert-base    T1              0.627959          0.642563         0.669358   \n",
              "             T2              0.609193          0.625754         0.626871   \n",
              "             T3              0.605028          0.638078         0.684352   \n",
              "             TM1             0.663666          0.718852         0.714413   \n",
              "             TM2             0.660960          0.706902         0.708351   \n",
              "             TM3             0.655755          0.685267         0.694070   \n",
              "roberta-base T1              0.683489          0.737337         0.743523   \n",
              "             T2              0.608671          0.651969         0.634775   \n",
              "             T3              0.705194          0.742221         0.772627   \n",
              "             TM1             0.719304          0.795759         0.792388   \n",
              "             TM2             0.717575          0.780942         0.786090   \n",
              "             TM3             0.726303          0.782770         0.776317   \n",
              "\n",
              "                                                                     \\\n",
              "                      noun-logit-correlation verb-logit-correlation   \n",
              "model        template                                                 \n",
              "Bert         T1                     0.653573               0.524790   \n",
              "             T2                     0.585964               0.431753   \n",
              "             T3                     0.741746               0.550663   \n",
              "             TM1                    0.822060               0.576625   \n",
              "             TM2                    0.803631               0.553331   \n",
              "             TM3                    0.816571               0.577981   \n",
              "Roberta      T1                     0.752877               0.583824   \n",
              "             T2                     0.169537               0.030114   \n",
              "             T3                     0.789695               0.630920   \n",
              "             TM1                    0.839351               0.716252   \n",
              "             TM2                    0.761168               0.645773   \n",
              "             TM3                    0.830364               0.682727   \n",
              "bert-base    T1                     0.666371               0.426226   \n",
              "             T2                     0.657132               0.305781   \n",
              "             T3                     0.669079               0.374907   \n",
              "             TM1                    0.746955               0.427838   \n",
              "             TM2                    0.743039               0.366064   \n",
              "             TM3                    0.716521               0.416953   \n",
              "roberta-base T1                     0.749482               0.594251   \n",
              "             T2                     0.682594               0.376505   \n",
              "             T3                     0.756931               0.636962   \n",
              "             TM1                    0.811107               0.639123   \n",
              "             TM2                    0.793191               0.663757   \n",
              "             TM3                    0.795358               0.635163   \n",
              "\n",
              "                                                                   \n",
              "                      noun-prob-correlation verb-prob-correlation  \n",
              "model        template                                              \n",
              "Bert         T1                    0.686033              0.490613  \n",
              "             T2                    0.623506              0.391794  \n",
              "             T3                    0.784944              0.482366  \n",
              "             TM1                   0.817262              0.527390  \n",
              "             TM2                   0.811488              0.538746  \n",
              "             TM3                   0.832978              0.525753  \n",
              "Roberta      T1                    0.779921              0.544862  \n",
              "             T2                    0.165167              0.009852  \n",
              "             T3                    0.815623              0.588373  \n",
              "             TM1                   0.843571              0.672157  \n",
              "             TM2                   0.757051              0.607138  \n",
              "             TM3                   0.808761              0.618838  \n",
              "bert-base    T1                    0.691809              0.427355  \n",
              "             T2                    0.658178              0.325007  \n",
              "             T3                    0.723588              0.347461  \n",
              "             TM1                   0.752636              0.356874  \n",
              "             TM2                   0.758504              0.230655  \n",
              "             TM3                   0.721807              0.436931  \n",
              "roberta-base T1                    0.757180              0.566824  \n",
              "             T2                    0.660551              0.391212  \n",
              "             T3                    0.787753              0.599196  \n",
              "             TM1                   0.820118              0.546569  \n",
              "             TM2                   0.808391              0.560856  \n",
              "             TM3                   0.796483              0.556385  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ca4a327-1508-4f21-a7d5-7bb5c0f46706\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"7\" halign=\"left\">hyperlex-lexical</th>\n",
              "      <th colspan=\"7\" halign=\"left\">hyperlex-random</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>hyp-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>prob-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>noun-prob-correlation</th>\n",
              "      <th>verb-prob-correlation</th>\n",
              "      <th>hyp-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>prob-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>noun-prob-correlation</th>\n",
              "      <th>verb-prob-correlation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.657046</td>\n",
              "      <td>0.686073</td>\n",
              "      <td>0.694415</td>\n",
              "      <td>0.737330</td>\n",
              "      <td>0.498642</td>\n",
              "      <td>0.750145</td>\n",
              "      <td>0.459302</td>\n",
              "      <td>0.623897</td>\n",
              "      <td>0.643511</td>\n",
              "      <td>0.668539</td>\n",
              "      <td>0.653573</td>\n",
              "      <td>0.524790</td>\n",
              "      <td>0.686033</td>\n",
              "      <td>0.490613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.450562</td>\n",
              "      <td>0.402034</td>\n",
              "      <td>0.416582</td>\n",
              "      <td>0.433256</td>\n",
              "      <td>0.285916</td>\n",
              "      <td>0.456257</td>\n",
              "      <td>0.248895</td>\n",
              "      <td>0.540876</td>\n",
              "      <td>0.577157</td>\n",
              "      <td>0.603452</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.431753</td>\n",
              "      <td>0.623506</td>\n",
              "      <td>0.391794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.667674</td>\n",
              "      <td>0.746582</td>\n",
              "      <td>0.755574</td>\n",
              "      <td>0.781277</td>\n",
              "      <td>0.622566</td>\n",
              "      <td>0.795455</td>\n",
              "      <td>0.597384</td>\n",
              "      <td>0.650930</td>\n",
              "      <td>0.727820</td>\n",
              "      <td>0.757310</td>\n",
              "      <td>0.741746</td>\n",
              "      <td>0.550663</td>\n",
              "      <td>0.784944</td>\n",
              "      <td>0.482366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.671808</td>\n",
              "      <td>0.766412</td>\n",
              "      <td>0.753963</td>\n",
              "      <td>0.807058</td>\n",
              "      <td>0.672470</td>\n",
              "      <td>0.794629</td>\n",
              "      <td>0.609317</td>\n",
              "      <td>0.721062</td>\n",
              "      <td>0.800457</td>\n",
              "      <td>0.790645</td>\n",
              "      <td>0.822060</td>\n",
              "      <td>0.576625</td>\n",
              "      <td>0.817262</td>\n",
              "      <td>0.527390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.589105</td>\n",
              "      <td>0.656697</td>\n",
              "      <td>0.672861</td>\n",
              "      <td>0.716757</td>\n",
              "      <td>0.477823</td>\n",
              "      <td>0.728138</td>\n",
              "      <td>0.481895</td>\n",
              "      <td>0.703471</td>\n",
              "      <td>0.778105</td>\n",
              "      <td>0.784275</td>\n",
              "      <td>0.803631</td>\n",
              "      <td>0.553331</td>\n",
              "      <td>0.811488</td>\n",
              "      <td>0.538746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.686035</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.755081</td>\n",
              "      <td>0.781051</td>\n",
              "      <td>0.632722</td>\n",
              "      <td>0.793492</td>\n",
              "      <td>0.609348</td>\n",
              "      <td>0.737478</td>\n",
              "      <td>0.794417</td>\n",
              "      <td>0.799238</td>\n",
              "      <td>0.816571</td>\n",
              "      <td>0.577981</td>\n",
              "      <td>0.832978</td>\n",
              "      <td>0.525753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.738920</td>\n",
              "      <td>0.754927</td>\n",
              "      <td>0.771282</td>\n",
              "      <td>0.787916</td>\n",
              "      <td>0.531692</td>\n",
              "      <td>0.808948</td>\n",
              "      <td>0.517498</td>\n",
              "      <td>0.708957</td>\n",
              "      <td>0.740859</td>\n",
              "      <td>0.757459</td>\n",
              "      <td>0.752877</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.779921</td>\n",
              "      <td>0.544862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.375061</td>\n",
              "      <td>0.286757</td>\n",
              "      <td>0.297980</td>\n",
              "      <td>0.350415</td>\n",
              "      <td>0.063102</td>\n",
              "      <td>0.374889</td>\n",
              "      <td>-0.008955</td>\n",
              "      <td>0.096328</td>\n",
              "      <td>0.152353</td>\n",
              "      <td>0.146603</td>\n",
              "      <td>0.169537</td>\n",
              "      <td>0.030114</td>\n",
              "      <td>0.165167</td>\n",
              "      <td>0.009852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.636995</td>\n",
              "      <td>0.669128</td>\n",
              "      <td>0.671147</td>\n",
              "      <td>0.690485</td>\n",
              "      <td>0.515506</td>\n",
              "      <td>0.699184</td>\n",
              "      <td>0.485816</td>\n",
              "      <td>0.712420</td>\n",
              "      <td>0.773941</td>\n",
              "      <td>0.794968</td>\n",
              "      <td>0.789695</td>\n",
              "      <td>0.630920</td>\n",
              "      <td>0.815623</td>\n",
              "      <td>0.588373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.736504</td>\n",
              "      <td>0.788514</td>\n",
              "      <td>0.796706</td>\n",
              "      <td>0.836832</td>\n",
              "      <td>0.612222</td>\n",
              "      <td>0.829112</td>\n",
              "      <td>0.618363</td>\n",
              "      <td>0.779260</td>\n",
              "      <td>0.828275</td>\n",
              "      <td>0.828471</td>\n",
              "      <td>0.839351</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.843571</td>\n",
              "      <td>0.672157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.600176</td>\n",
              "      <td>0.653700</td>\n",
              "      <td>0.669882</td>\n",
              "      <td>0.705038</td>\n",
              "      <td>0.417199</td>\n",
              "      <td>0.715934</td>\n",
              "      <td>0.410055</td>\n",
              "      <td>0.627888</td>\n",
              "      <td>0.749403</td>\n",
              "      <td>0.743976</td>\n",
              "      <td>0.761168</td>\n",
              "      <td>0.645773</td>\n",
              "      <td>0.757051</td>\n",
              "      <td>0.607138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.721263</td>\n",
              "      <td>0.794234</td>\n",
              "      <td>0.783369</td>\n",
              "      <td>0.828178</td>\n",
              "      <td>0.656274</td>\n",
              "      <td>0.827253</td>\n",
              "      <td>0.600486</td>\n",
              "      <td>0.774364</td>\n",
              "      <td>0.814334</td>\n",
              "      <td>0.791946</td>\n",
              "      <td>0.830364</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>0.808761</td>\n",
              "      <td>0.618838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.498647</td>\n",
              "      <td>0.471335</td>\n",
              "      <td>0.474081</td>\n",
              "      <td>0.556778</td>\n",
              "      <td>0.172960</td>\n",
              "      <td>0.551468</td>\n",
              "      <td>0.137220</td>\n",
              "      <td>0.627959</td>\n",
              "      <td>0.642563</td>\n",
              "      <td>0.669358</td>\n",
              "      <td>0.666371</td>\n",
              "      <td>0.426226</td>\n",
              "      <td>0.691809</td>\n",
              "      <td>0.427355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.406193</td>\n",
              "      <td>0.374065</td>\n",
              "      <td>0.378994</td>\n",
              "      <td>0.445601</td>\n",
              "      <td>0.116163</td>\n",
              "      <td>0.442557</td>\n",
              "      <td>0.126524</td>\n",
              "      <td>0.609193</td>\n",
              "      <td>0.625754</td>\n",
              "      <td>0.626871</td>\n",
              "      <td>0.657132</td>\n",
              "      <td>0.305781</td>\n",
              "      <td>0.658178</td>\n",
              "      <td>0.325007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.588330</td>\n",
              "      <td>0.614221</td>\n",
              "      <td>0.622301</td>\n",
              "      <td>0.690729</td>\n",
              "      <td>0.311820</td>\n",
              "      <td>0.713881</td>\n",
              "      <td>0.309381</td>\n",
              "      <td>0.605028</td>\n",
              "      <td>0.638078</td>\n",
              "      <td>0.684352</td>\n",
              "      <td>0.669079</td>\n",
              "      <td>0.374907</td>\n",
              "      <td>0.723588</td>\n",
              "      <td>0.347461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.569934</td>\n",
              "      <td>0.597293</td>\n",
              "      <td>0.599501</td>\n",
              "      <td>0.680286</td>\n",
              "      <td>0.379775</td>\n",
              "      <td>0.673157</td>\n",
              "      <td>0.267178</td>\n",
              "      <td>0.663666</td>\n",
              "      <td>0.718852</td>\n",
              "      <td>0.714413</td>\n",
              "      <td>0.746955</td>\n",
              "      <td>0.427838</td>\n",
              "      <td>0.752636</td>\n",
              "      <td>0.356874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.561259</td>\n",
              "      <td>0.574906</td>\n",
              "      <td>0.610851</td>\n",
              "      <td>0.656103</td>\n",
              "      <td>0.276512</td>\n",
              "      <td>0.685900</td>\n",
              "      <td>0.266175</td>\n",
              "      <td>0.660960</td>\n",
              "      <td>0.706902</td>\n",
              "      <td>0.708351</td>\n",
              "      <td>0.743039</td>\n",
              "      <td>0.366064</td>\n",
              "      <td>0.758504</td>\n",
              "      <td>0.230655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.573250</td>\n",
              "      <td>0.583948</td>\n",
              "      <td>0.579535</td>\n",
              "      <td>0.664762</td>\n",
              "      <td>0.355509</td>\n",
              "      <td>0.647579</td>\n",
              "      <td>0.316415</td>\n",
              "      <td>0.655755</td>\n",
              "      <td>0.685267</td>\n",
              "      <td>0.694070</td>\n",
              "      <td>0.716521</td>\n",
              "      <td>0.416953</td>\n",
              "      <td>0.721807</td>\n",
              "      <td>0.436931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.651845</td>\n",
              "      <td>0.677077</td>\n",
              "      <td>0.725030</td>\n",
              "      <td>0.713328</td>\n",
              "      <td>0.542603</td>\n",
              "      <td>0.764511</td>\n",
              "      <td>0.547946</td>\n",
              "      <td>0.683489</td>\n",
              "      <td>0.737337</td>\n",
              "      <td>0.743523</td>\n",
              "      <td>0.749482</td>\n",
              "      <td>0.594251</td>\n",
              "      <td>0.757180</td>\n",
              "      <td>0.566824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.483763</td>\n",
              "      <td>0.406815</td>\n",
              "      <td>0.400610</td>\n",
              "      <td>0.482727</td>\n",
              "      <td>0.166631</td>\n",
              "      <td>0.475401</td>\n",
              "      <td>0.085916</td>\n",
              "      <td>0.608671</td>\n",
              "      <td>0.651969</td>\n",
              "      <td>0.634775</td>\n",
              "      <td>0.682594</td>\n",
              "      <td>0.376505</td>\n",
              "      <td>0.660551</td>\n",
              "      <td>0.391212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.641070</td>\n",
              "      <td>0.625525</td>\n",
              "      <td>0.664061</td>\n",
              "      <td>0.692522</td>\n",
              "      <td>0.391105</td>\n",
              "      <td>0.725598</td>\n",
              "      <td>0.441986</td>\n",
              "      <td>0.705194</td>\n",
              "      <td>0.742221</td>\n",
              "      <td>0.772627</td>\n",
              "      <td>0.756931</td>\n",
              "      <td>0.636962</td>\n",
              "      <td>0.787753</td>\n",
              "      <td>0.599196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.690944</td>\n",
              "      <td>0.736169</td>\n",
              "      <td>0.776089</td>\n",
              "      <td>0.799714</td>\n",
              "      <td>0.552940</td>\n",
              "      <td>0.819748</td>\n",
              "      <td>0.549569</td>\n",
              "      <td>0.719304</td>\n",
              "      <td>0.795759</td>\n",
              "      <td>0.792388</td>\n",
              "      <td>0.811107</td>\n",
              "      <td>0.639123</td>\n",
              "      <td>0.820118</td>\n",
              "      <td>0.546569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.641001</td>\n",
              "      <td>0.710913</td>\n",
              "      <td>0.725829</td>\n",
              "      <td>0.756986</td>\n",
              "      <td>0.525289</td>\n",
              "      <td>0.778836</td>\n",
              "      <td>0.442391</td>\n",
              "      <td>0.717575</td>\n",
              "      <td>0.780942</td>\n",
              "      <td>0.786090</td>\n",
              "      <td>0.793191</td>\n",
              "      <td>0.663757</td>\n",
              "      <td>0.808391</td>\n",
              "      <td>0.560856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.697356</td>\n",
              "      <td>0.757284</td>\n",
              "      <td>0.770732</td>\n",
              "      <td>0.806950</td>\n",
              "      <td>0.633745</td>\n",
              "      <td>0.808939</td>\n",
              "      <td>0.639037</td>\n",
              "      <td>0.726303</td>\n",
              "      <td>0.782770</td>\n",
              "      <td>0.776317</td>\n",
              "      <td>0.795358</td>\n",
              "      <td>0.635163</td>\n",
              "      <td>0.796483</td>\n",
              "      <td>0.556385</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ca4a327-1508-4f21-a7d5-7bb5c0f46706')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ca4a327-1508-4f21-a7d5-7bb5c0f46706 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ca4a327-1508-4f21-a7d5-7bb5c0f46706');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperlex: Reported results in the paper**"
      ],
      "metadata": {
        "id": "f0xNZFsjH1vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_paper =[(\"hyperlex-random\", 'logit-correlation'),(\"hyperlex-random\", 'noun-logit-correlation'), (\"hyperlex-random\", 'verb-logit-correlation'),\n",
        "                (\"hyperlex-lexical\", 'logit-correlation'),(\"hyperlex-lexical\", 'noun-logit-correlation'), (\"hyperlex-lexical\", 'verb-logit-correlation')] \n",
        "df_res_hyperlex_paper = df_data_hyperlex[columns_paper]\n",
        "df_res_hyperlex_paper\n",
        "df_res_hyperlex_paper.to_csv(\"datos.csv\")"
      ],
      "metadata": {
        "id": "isicaq2yCP5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOTAS_FILE_REST = DIR_RESULTS + 'sotas_results_literature/hyperlex_Sotas.txt' \n",
        "df_hyperlex_sotas = pd.read_csv(SOTAS_FILE_REST, header=[0,1], index_col=[0], skipinitialspace=True)\n",
        "\n",
        "multi_row_tuples = list(zip(['Sota']*df_hyperlex_sotas.shape[0], list(df_hyperlex_sotas.index)))\n",
        "multi_row = pd.MultiIndex.from_tuples(multi_row_tuples)\n",
        "df_hyperlex_sotas.index=multi_row\n",
        "df_res_sotas_hyperlex = pd.concat([df_res_hyperlex_paper,df_hyperlex_sotas])\n",
        "df_res_sotas_hyperlex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6M--RnvGEtN6",
        "outputId": "ea824fad-2978-4b70-8d8a-bef843a4635e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        hyperlex-random                         \\\n",
              "                      logit-correlation noun-logit-correlation   \n",
              "model        template                                            \n",
              "Bert         T1                0.643511               0.653573   \n",
              "             T2                0.577157               0.585964   \n",
              "             T3                0.727820               0.741746   \n",
              "             TM1               0.800457                0.82206   \n",
              "             TM2               0.778105               0.803631   \n",
              "             TM3               0.794417               0.816571   \n",
              "Roberta      T1                0.740859               0.752877   \n",
              "             T2                0.152353               0.169537   \n",
              "             T3                0.773941               0.789695   \n",
              "             TM1               0.828275               0.839351   \n",
              "             TM2               0.749403               0.761168   \n",
              "             TM3               0.814334               0.830364   \n",
              "bert-base    T1                0.642563               0.666371   \n",
              "             T2                0.625754               0.657132   \n",
              "             T3                0.638078               0.669079   \n",
              "             TM1               0.718852               0.746955   \n",
              "             TM2               0.706902               0.743039   \n",
              "             TM3               0.685267               0.716521   \n",
              "roberta-base T1                0.737337               0.749482   \n",
              "             T2                0.651969               0.682594   \n",
              "             T3                0.742221               0.756931   \n",
              "             TM1               0.795759               0.811107   \n",
              "             TM2               0.780942               0.793191   \n",
              "             TM3               0.782770               0.795358   \n",
              "Sota         LEAR              0.686000                   0.71   \n",
              "             SDNS              0.692000                     na   \n",
              "             GLEN              0.520000                     na   \n",
              "             POSTLE            0.686000                     na   \n",
              "             LexSub            0.533000                     na   \n",
              "             HF                0.690000                     na   \n",
              "\n",
              "                                              hyperlex-lexical  \\\n",
              "                      verb-logit-correlation logit-correlation   \n",
              "model        template                                            \n",
              "Bert         T1                      0.52479          0.686073   \n",
              "             T2                     0.431753          0.402034   \n",
              "             T3                     0.550663          0.746582   \n",
              "             TM1                    0.576625          0.766412   \n",
              "             TM2                    0.553331          0.656697   \n",
              "             TM3                    0.577981          0.741279   \n",
              "Roberta      T1                     0.583824          0.754927   \n",
              "             T2                     0.030114          0.286757   \n",
              "             T3                      0.63092          0.669128   \n",
              "             TM1                    0.716252          0.788514   \n",
              "             TM2                    0.645773            0.6537   \n",
              "             TM3                    0.682727          0.794234   \n",
              "bert-base    T1                     0.426226          0.471335   \n",
              "             T2                     0.305781          0.374065   \n",
              "             T3                     0.374907          0.614221   \n",
              "             TM1                    0.427838          0.597293   \n",
              "             TM2                    0.366064          0.574906   \n",
              "             TM3                    0.416953          0.583948   \n",
              "roberta-base T1                     0.594251          0.677077   \n",
              "             T2                     0.376505          0.406815   \n",
              "             T3                     0.636962          0.625525   \n",
              "             TM1                    0.639123          0.736169   \n",
              "             TM2                    0.663757          0.710913   \n",
              "             TM3                    0.635163          0.757284   \n",
              "Sota         LEAR                         na             0.174   \n",
              "             SDNS                         na                na   \n",
              "             GLEN                         na             0.481   \n",
              "             POSTLE                       na                na   \n",
              "             LexSub                       na                na   \n",
              "             HF                           na                na   \n",
              "\n",
              "                                                                     \n",
              "                      noun-logit-correlation verb-logit-correlation  \n",
              "model        template                                                \n",
              "Bert         T1                      0.73733               0.498642  \n",
              "             T2                     0.433256               0.285916  \n",
              "             T3                     0.781277               0.622566  \n",
              "             TM1                    0.807058                0.67247  \n",
              "             TM2                    0.716757               0.477823  \n",
              "             TM3                    0.781051               0.632722  \n",
              "Roberta      T1                     0.787916               0.531692  \n",
              "             T2                     0.350415               0.063102  \n",
              "             T3                     0.690485               0.515506  \n",
              "             TM1                    0.836832               0.612222  \n",
              "             TM2                    0.705038               0.417199  \n",
              "             TM3                    0.828178               0.656274  \n",
              "bert-base    T1                     0.556778                0.17296  \n",
              "             T2                     0.445601               0.116163  \n",
              "             T3                     0.690729                0.31182  \n",
              "             TM1                    0.680286               0.379775  \n",
              "             TM2                    0.656103               0.276512  \n",
              "             TM3                    0.664762               0.355509  \n",
              "roberta-base T1                     0.713328               0.542603  \n",
              "             T2                     0.482727               0.166631  \n",
              "             T3                     0.692522               0.391105  \n",
              "             TM1                    0.799714                0.55294  \n",
              "             TM2                    0.756986               0.525289  \n",
              "             TM3                     0.80695               0.633745  \n",
              "Sota         LEAR                         na                     na  \n",
              "             SDNS                         na                     na  \n",
              "             GLEN                         na                     na  \n",
              "             POSTLE                     0.60                     na  \n",
              "             LexSub                       na                     na  \n",
              "             HF                           na                     na  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0870a486-a941-4288-b582-944977f4703c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyperlex-random</th>\n",
              "      <th colspan=\"3\" halign=\"left\">hyperlex-lexical</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "      <th>logit-correlation</th>\n",
              "      <th>noun-logit-correlation</th>\n",
              "      <th>verb-logit-correlation</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Bert</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.643511</td>\n",
              "      <td>0.653573</td>\n",
              "      <td>0.52479</td>\n",
              "      <td>0.686073</td>\n",
              "      <td>0.73733</td>\n",
              "      <td>0.498642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.577157</td>\n",
              "      <td>0.585964</td>\n",
              "      <td>0.431753</td>\n",
              "      <td>0.402034</td>\n",
              "      <td>0.433256</td>\n",
              "      <td>0.285916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.727820</td>\n",
              "      <td>0.741746</td>\n",
              "      <td>0.550663</td>\n",
              "      <td>0.746582</td>\n",
              "      <td>0.781277</td>\n",
              "      <td>0.622566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.800457</td>\n",
              "      <td>0.82206</td>\n",
              "      <td>0.576625</td>\n",
              "      <td>0.766412</td>\n",
              "      <td>0.807058</td>\n",
              "      <td>0.67247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.778105</td>\n",
              "      <td>0.803631</td>\n",
              "      <td>0.553331</td>\n",
              "      <td>0.656697</td>\n",
              "      <td>0.716757</td>\n",
              "      <td>0.477823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.794417</td>\n",
              "      <td>0.816571</td>\n",
              "      <td>0.577981</td>\n",
              "      <td>0.741279</td>\n",
              "      <td>0.781051</td>\n",
              "      <td>0.632722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Roberta</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.740859</td>\n",
              "      <td>0.752877</td>\n",
              "      <td>0.583824</td>\n",
              "      <td>0.754927</td>\n",
              "      <td>0.787916</td>\n",
              "      <td>0.531692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.152353</td>\n",
              "      <td>0.169537</td>\n",
              "      <td>0.030114</td>\n",
              "      <td>0.286757</td>\n",
              "      <td>0.350415</td>\n",
              "      <td>0.063102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.773941</td>\n",
              "      <td>0.789695</td>\n",
              "      <td>0.63092</td>\n",
              "      <td>0.669128</td>\n",
              "      <td>0.690485</td>\n",
              "      <td>0.515506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.828275</td>\n",
              "      <td>0.839351</td>\n",
              "      <td>0.716252</td>\n",
              "      <td>0.788514</td>\n",
              "      <td>0.836832</td>\n",
              "      <td>0.612222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.749403</td>\n",
              "      <td>0.761168</td>\n",
              "      <td>0.645773</td>\n",
              "      <td>0.6537</td>\n",
              "      <td>0.705038</td>\n",
              "      <td>0.417199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.814334</td>\n",
              "      <td>0.830364</td>\n",
              "      <td>0.682727</td>\n",
              "      <td>0.794234</td>\n",
              "      <td>0.828178</td>\n",
              "      <td>0.656274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">bert-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.642563</td>\n",
              "      <td>0.666371</td>\n",
              "      <td>0.426226</td>\n",
              "      <td>0.471335</td>\n",
              "      <td>0.556778</td>\n",
              "      <td>0.17296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.625754</td>\n",
              "      <td>0.657132</td>\n",
              "      <td>0.305781</td>\n",
              "      <td>0.374065</td>\n",
              "      <td>0.445601</td>\n",
              "      <td>0.116163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.638078</td>\n",
              "      <td>0.669079</td>\n",
              "      <td>0.374907</td>\n",
              "      <td>0.614221</td>\n",
              "      <td>0.690729</td>\n",
              "      <td>0.31182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.718852</td>\n",
              "      <td>0.746955</td>\n",
              "      <td>0.427838</td>\n",
              "      <td>0.597293</td>\n",
              "      <td>0.680286</td>\n",
              "      <td>0.379775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.706902</td>\n",
              "      <td>0.743039</td>\n",
              "      <td>0.366064</td>\n",
              "      <td>0.574906</td>\n",
              "      <td>0.656103</td>\n",
              "      <td>0.276512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.685267</td>\n",
              "      <td>0.716521</td>\n",
              "      <td>0.416953</td>\n",
              "      <td>0.583948</td>\n",
              "      <td>0.664762</td>\n",
              "      <td>0.355509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">roberta-base</th>\n",
              "      <th>T1</th>\n",
              "      <td>0.737337</td>\n",
              "      <td>0.749482</td>\n",
              "      <td>0.594251</td>\n",
              "      <td>0.677077</td>\n",
              "      <td>0.713328</td>\n",
              "      <td>0.542603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T2</th>\n",
              "      <td>0.651969</td>\n",
              "      <td>0.682594</td>\n",
              "      <td>0.376505</td>\n",
              "      <td>0.406815</td>\n",
              "      <td>0.482727</td>\n",
              "      <td>0.166631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T3</th>\n",
              "      <td>0.742221</td>\n",
              "      <td>0.756931</td>\n",
              "      <td>0.636962</td>\n",
              "      <td>0.625525</td>\n",
              "      <td>0.692522</td>\n",
              "      <td>0.391105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM1</th>\n",
              "      <td>0.795759</td>\n",
              "      <td>0.811107</td>\n",
              "      <td>0.639123</td>\n",
              "      <td>0.736169</td>\n",
              "      <td>0.799714</td>\n",
              "      <td>0.55294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM2</th>\n",
              "      <td>0.780942</td>\n",
              "      <td>0.793191</td>\n",
              "      <td>0.663757</td>\n",
              "      <td>0.710913</td>\n",
              "      <td>0.756986</td>\n",
              "      <td>0.525289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TM3</th>\n",
              "      <td>0.782770</td>\n",
              "      <td>0.795358</td>\n",
              "      <td>0.635163</td>\n",
              "      <td>0.757284</td>\n",
              "      <td>0.80695</td>\n",
              "      <td>0.633745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">Sota</th>\n",
              "      <th>LEAR</th>\n",
              "      <td>0.686000</td>\n",
              "      <td>0.71</td>\n",
              "      <td>na</td>\n",
              "      <td>0.174</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SDNS</th>\n",
              "      <td>0.692000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GLEN</th>\n",
              "      <td>0.520000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>0.481</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>POSTLE</th>\n",
              "      <td>0.686000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>0.60</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LexSub</th>\n",
              "      <td>0.533000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HF</th>\n",
              "      <td>0.690000</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "      <td>na</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0870a486-a941-4288-b582-944977f4703c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0870a486-a941-4288-b582-944977f4703c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0870a486-a941-4288-b582-944977f4703c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Graphs"
      ],
      "metadata": {
        "id": "qF_nqf4495GH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First some plots to visualize the best performing model and template per dataset. The worse performing ones are excluded to obtain better visualization. Used boxplots for visualization to also observe the variability of the 5 iterations conducted per experiment. "
      ],
      "metadata": {
        "id": "864nTZEN4p2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds_plot = []\n",
        "model_plot = []\n",
        "template_plot = []\n",
        "dataset_plot = []\n",
        "\n",
        "for dataset in dict_res:\n",
        "  if dataset == 'cogalexv':\n",
        "    for model in dict_res['cogalexv']:\n",
        "      for template in dict_res['cogalexv'][model]:\n",
        "        for i in dict_res['cogalexv'][model][template]['report']:\n",
        "          preds_plot.append(i['weighted f1-score not random'])\n",
        "          template_plot.append(templates2abrev[template]) \n",
        "          model_plot.append(models2abrev[model])\n",
        "          dataset_plot.append(dataset)\n",
        "  else:\n",
        "    for model in dict_res[dataset]:\n",
        "      for template in dict_res[dataset][model]:\n",
        "        for i in dict_res[dataset][model][template]['report']:\n",
        "          preds_plot.append(i['weighted avg']['f1-score'])\n",
        "          template_plot.append(templates2abrev[template]) \n",
        "          model_plot.append(models2abrev[model])\n",
        "          dataset_plot.append(dataset)\n",
        "\n",
        "df_plot = plot = pd.DataFrame([preds_plot, model_plot, template_plot, dataset_plot])\n",
        "df_plot = df_plot.T\n",
        "df_plot.columns = ['preds', 'model', 'template', 'dataset']\n",
        "df_plot['label'] = df_plot.model.str.cat(df_plot.template)"
      ],
      "metadata": {
        "id": "o90jSwSoTXpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_plot_prov = df_plot[df_plot['preds']>0.7] #just take weight averages higher than 0.7 \n",
        "#to avoid the ones in which data didnt converge and visualize better. \n",
        "#to see al data, comment above line, and change data below to 'df_plot'\n",
        "sns.boxplot(data = df_plot_prov, x=\"template\", y=\"preds\", hue=\"dataset\")"
      ],
      "metadata": {
        "id": "FG0N6zZCSGDL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "64cde381-4f0e-4b0c-905f-1b23e3e40701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468e9073d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU5dn48e+dhSyGzQSCEjEoAdnCIostinEhgFJ4EWtR1FAVbSsEsVK0UiuoL1SpS6jVSkXwrQIWBZHCD1CguKAkSEA2TdAAQYEsgIRASDLP749ZnCSTZAKzJvfnurjInDnnzD2TybnPs4sxBqWUUqq6EH8HoJRSKjBpglBKKeWSJgillFIuaYJQSinlkiYIpZRSLmmCUEop5VKYt04sIvOBEcBRY0wPF88L8BJwE1AKjDfGfGl7Lg2Ybtv1aWPMwvpeLy4uziQmJnooeqWUahq2bt1aaIxp4+o5ryUIYAHwN+DNWp4fDiTZ/g0EXgEGisiFwJ+BfoABtorICmPMsbpeLDExkaysLA+FrpRSTYOI7K/tOa9VMRljNgHFdewyCnjTWH0OtBKRi4ChwDpjTLEtKawDhnkrTqWUUq75sw2iPXDQ6XG+bVtt25VSSvlQUDdSi8j9IpIlIlkFBQX+DkcppRoVb7ZB1OcQcInT4wTbtkNASrXtG12dwBjzGvAaQL9+/XRSKaXOQ3l5Ofn5+Zw5c8bfoSgviIyMJCEhgfDwcLeP8WeCWAFMFJHFWBupTxhjfhCRNcD/ikhr236pwGP+ClKppiI/P5/mzZuTmJiItZOhaiyMMRQVFZGfn0/Hjh3dPs6b3VwXYS0JxIlIPtaeSeEAxphXgVVYu7jmYu3m+mvbc8Ui8hSQaTvVTGNMXY3dSikPOHPmjCaHRkpEiI2NpaFV8V5LEMaY2+t53gAP1vLcfGC+N+JSStVOk0PjdS6/W39WMalGLiMjg9zcXPLz8wFISUkhPT3dz1EppdzVaBOE88Xp9OnTAERFRZGQkECnTp0C/kKVkZHBxo0bHbGDNf5gucg6x29/Dxs3bgQIivjhp/cABM33xlmwx3/kyBFOnjwJQPPmzYmPj3e535NPPklMTAyPPPKIy+eXL19O586d6datm8diy8vL47PPPuOOO+7w2DkDUVB3c63Lxo0b2Za9nYKiYkpOlVJyqpSComK2ZW93/NEEso0bN1JQWOiIveRUKQWFhUERO9jjL6LkzFkqDVQaKCgsCpr47RfX4uJiiouLycnJYePGjWRkZPg7tAZxTtDBIi8vj2+++Ybjx49TUVFBRUUFx48f55tvviEvL6/B51u+fDm7d+/2eIxvv/22R88ZiBptgmgcBELDQMT6D60f9hV7grMnt5IzZ4MuweXm5lbZlpubGxQJrqKiAovFgvNyyMYYLBYLFRUVADzzzDN07tyZq6++mq+//hqAefPm0b9/f3r16sWYMWMoLS3ls88+Y8WKFUydOpXevXuzb98+l/sB/Pvf/6ZHjx706tWLwYMHA1BZWcnUqVPp378/ycnJ/OMf/wDg0Ucf5eOPP6Z379688MILvvx4fKrRVjGlpKTUWcUU6FJSUmpU0VwYG0tKSop/A3OT/fMHyMnJASApKSkoPvvGwJ7gsDVMbvtqF1RWkJ+fH/DVTM2bN6esrAywloCMMYSFhdGsWTMiIiLYunUrixcvJjs7m4qKCvr27cuVV17JLbfcwoQJEwCYPn06r7/+OpMmTWLkyJGMGDGCW2+9FYBWrVq53G/mzJmsWbOG9u3bc/z4cQBef/11WrZsSWZmJmVlZQwaNIjU1FRmz57NnDlzWLlypR8+Id9ptAki0P8I6mOPP1gbedPT02vcxQZTHbg9wTknN0ATnA/Y2xrKy8vZt28fABaLhYsvvpiwsDAWLVrE6NGjiY6OBmDkyJEA7Ny5k+nTp3P8+HFKSkoYOnSoy/PXtt+gQYMYP348t912G7fccgsAa9euZceOHSxduhSAEydOkJOTQ7Nmzbz3AQSQRpsgGoNguZjWJyoqyt8hNJg9we3du5czZ85QWlpKcnJy0PxO7CXQ4mLrEKKoyGZERbUMmhIo4LgxAmsVU2FhIe3atat1//Hjx7N8+XJ69erFggULaq0OrG2/V199lS+++IL//Oc/XHnllWzduhVjDHPnzq2RbIKlqvF8aYJQXhMsF9O6nD17FoD9+/eTnJzs52jcZ//sg7kXk/2zB2uC+PHHH2nXrh2DBw9m/PjxPPbYY1RUVPDBBx/wwAMPcPLkSS666CLKy8t56623aN/eOsdn8+bNHb2hgFr327dvHwMHDmTgwIGsXr2agwcPMnToUF555RWuv/56wsPD+eabb2jfvn2NczZWmiCUqsWwYcMcVQtlZWXcdNNNfo6oYdLT04MqIVTXsmVLTpw4gTEGEaFFixYA9O3bl1/96lf06tWLtm3b0r9/fwCeeuopBg4cSJs2bRg4cKDjAj527FgmTJhARkYGS5curXW/qVOnkpOTgzGGG264gV69epGcnExeXh59+/bFGEObNm1Yvnw5ycnJhIaG0qtXL8aPH8+UKVP88yF5mTj3FAhm/fr1M7pgkPKku+++u0q3ysTERN58s7b1r4Lfnj176Nq1q7/DcCgvL+fbb7/FGENISAiXXXYZYWF6T3s+XP2ORWSrMaafq/21m6tStaje5/5c+uCrcxceHk7Lli0BaNGihSYHP9AEoVQtqq9xrmue+15sbCzR0dHExcX5O5QmqUkkiMLCQiZNmkRRUZG/Q1FBZPr06VUeP/HEE36KpOkKDw+nQ4cOWnrwkyaRIBYuXMiOHTtYuHChv0NRQaRz586OUkNiYqKOgVBNTqNPEIWFhaxevRpjDKtXr9ZShGqQ6dOnc8EFF2jpQTVJjT5BLFy40DGni8Vi0VKEapDOnTuzevVqLT2oJqnRV+ytW7eO8vJywNptbu3atTz88MN+jkqpwPfgQ49wpNBziznGx13Iyy/OqfX5vLw8RowYwc6dO6ts//zzz3nggQewWCz07du3yk1efVN9q/PT6BPEkCFDWLVqFeXl5YSHh5OamurvkJQKCkcKi/nuohTPnfCHjed02OOPP86LL77Iddddx3fffdfg4zdu3MiCBQtYsGDBOb1+U9boq5jS0tIcS+2FhISQlpbm54iUUvX59ttv6dOnD5mZmTRr1swxL1PHjh1r7Lt7925SUlK47LLLGjSdeUpKCtOmTWPAgAF07tyZjz/+2GPxNxaNPkHExcUxfPhwRIThw4cTGxvr75CUUnX4+uuvGTNmDAsWLKB///5cfvnl/PGPf6S2mRL27t3LmjVr2LJlCzNmzHBUKbujoqKCLVu28OKLLzJjxgxPvYVGo9FXMYG1FJGXl6elB6UCXEFBAaNGjeK9996jW7duvP/++5SWlrJq1SrGjBnDf/7zH1q1asXw4cMdCePmm28mIiKCiIgI2rZty5EjR0hISGDgwIGUlZVRUlJCcXExvXv3BuAvf/mLY3ZW+7TeV155pY6Ud6FJJIi4uDjmzp3r7zCUUvVo2bIlHTp04JNPPqFbt26sWbOGwYMH07NnT15//XVGjRrFL3/5S8aOHes4JiIiwvFzaGioY9W5L774Aqi7DcJ+rPNx6idNIkGowGJf7xmCaxEk5X3NmjVj2bJlDB06lJiYGPr06cOSJUu4/fbbueaaaxg9ejTPPPMM+/fv93eoTYImCOUX9mVUlW/ZV/lzZ5XC+LgLz7nnUa3nc8MFF1zAypUrGTJkCH/605/o2bMnvXr1IiYmhuTkZObMmcOtt97KRx995LHYlGtNcrpv5zvYYFxIJdjjh58WtGlIrxN1/uwJwr6U6vDhwx2/i0Cb7lt5XkOn+/ZqCUJEhgEvAaHAP40xs6s9fykwH2gDFAN3GmPybc9VAl/Zdj1gjBnpydiC/Q422ONX/mFPBtX/Dzbl5eX88MMPjnWqlXd47ZMVkVDgZWAIkA9kisgKY8xup93mAG8aYxaKyPXALOAu23OnjTG9PR2X/Q7KWW5uLhkZGUHzx5Kenu54D8F0B+782dvvYO2feTCWgpT/FBUVUVpaWu861er8eDP1DgByjTHfAojIYmAU4JwgugH2eS82AMu9GA9gTQbf7PySiBBr1dqZvEwOlIR6+2U9pnoVQTBdYO2ffYeYSpqVW4fgBNvnD9rI7m/l5eWcOHECgB9//JG4uDgtRXiJNz/V9sBBp8f5wMBq+2wHbsFaDTUaaC4iscaYIiBSRLKACmC2McYjySM/Px9jID7a4thmDI5Gu0AXzAnO/tlD8H7+dlrF5z/OMzIbY7QU4UX+TruPAH8TkfHAJuAQUGl77lJjzCERuQxYLyJfGWP2OR8sIvcD9wN06NDBd1H7UbAnuMbAuYovmEoPtVXxBUPp0+7IkSOcOHHCMUOzMYYff/xRE4SXeDNBHAIucXqcYNvmYIz5HmsJAhGJAcYYY47bnjtk+/9bEdkI9AH2VTv+NeA1sPZicieohIQEzlT8wPR+JY5tT2fFEJmQ0JD3ps6Bq88egufzD/Y2FFdVfN/s/NLPUTWc86A2EaFFixZ+jqjx8maCyASSRKQj1sQwFrjDeQcRiQOKjTEW4DGsPZoQkdZAqTGmzLbPIOBZTwV2oCSUSZusX6r4aAsHSkLp7KmTe1mwJ7gDJaE8nRXDkVLrBSqYPv9gb0NxVcW3/2RoraXPx6Y8yImiwx57/Zax7Zj1wsu1Pl/bdN8pKSnMmTOHfv36ER8fz4UXXsi+fdZ7RRHR9aq9yGsJwhhTISITgTVYu7nON8bsEpGZQJYxZgWQAswSEYO1iulB2+FdgX+IiAXrhIKzq/V+Omf2hV/sd4CRiUl0dtquvMf5Mz4bhJ9/Y2pDcceJosM82ukbj51vdm79+7gjPDycsLAwKioqaNGihTZQe5FXP1ljzCpgVbVtTzj9vBRY6uK4z4Ce3oipev/vYOomahesJSDnKphg/fzLKoX9J0Mpt1inkA8PMZRVChf4OS53BEPps6KignHjxvHll1/SvXt33nzzTcdzR44cYd26dTz//POUlZVxySWX8PzzzyMivPDCC6xYsYKwsDBSU1OZM2cO//73v5kxYwahoaG0bNmSTZs2+fGdBSdNvUFGS0D+k5KSUqMNIikpCQiez796FV+ZRQLq5uLrr7/m9ddfZ9CgQdxzzz38/e9/dzy3f/9+XnrxRea//k+ioqKZN28er77yCnfdfTfLli1j7969iAjHjx8HYObMmaxZs4b27ds7tqmG0QQRZBpaAmoM03IEimAvAbmq4uuclBRQye2SSy5h0KBBANx5551VPt/s7Gxy9+3jjjusTZnl5eX06t2b5s2bExkZyb333suIESMYMWIEAH379mXs2LEMGzaM1NRUOnToQHx8vO/fVBBrkgkiIyPD7W5+gXiBbUj8EHh99p3jr28Euw5K85xgSHD21R9dPQ4LC2PQz3/O316cQ4Wtis8CREZFs2XLFj766COWLl3K3/72N9avX8+zzz7L559/ztq1axk9ejTr1q0LqgRx5MgRTp48CVhnuY2IiPB5/I1+RbnaREVFERUV5da+p0+fDriLrLvxp6enk5SURFJSUkBNJxLMn39GRgZ79+4lOzubmTNn+jucRuXAgQNs3rwZgLfffpurr77a8dzAgQPZlp3NgR8KsEgIJafP8P0Ph6moqODEiRPcdNNNvPDCC2zfvh2AkpISRo0axbRp04iNjaWsrMwv7+l8WCwWLBZL/Tt6SZMsQaSnp7t9oQzEeY8aEn8gcif+2ubMCpSBXWfPngV+WpSmNsFcAmoZ285jPY/s56tPly5dePnll7nnnnvo1q0bv/3tb/nggw8A6N69OxkZGUyaNIkzZ84A8Pjjj9OlSxdGjBjBmTNnMMbw/PPPAzB16lRycnI4e/YsgwYNolevXp57Mz4QHx/vSGr+GgjcJBOEu4J53iMI7vhzc3PZm51NO34q5h7PzsZzvfLP3bBhw1i61Nr57uTJk+Tm5tZZjx9opR/n70Rdpcq6xix4Q2JiInv37q2x3Z5gAa6++mo++OADR4Ju3rw58fHxbNmypcZx7733HmAtlUDN6itVP00QdbBfpMJtjwPlAuWuYI4/Pz8f+9D4WKftBv+POXj66aerPJ45c2aV7pjOAnVaDneq944cOUJZWVmNi7E/2V/fPt13bGysy/3ssQOO0saBAwf8Uo8fzDRB1KMdcC8/3Xm8TnAtsBTs8Qei6ovbB9Ni9+eSpPxZB16b+qb7Lisr40xpKeHg+PafKS31aYyNgSaIOuTn53OSqhfVH4CSIBk1G8zxJyQkcLywsEpyA+t7aeXngV2JiYlVkkJiYqLfYvEm+522vYomUO683Z3uOxxwnoSj0DfheYS9BORc+gF8XgLSBKFUA02fPp377rvP8fiJJ56osU+wT+wXyFU07kz3ffbsWSxUTQrlQKWtuizQ2UtA9tujytJSyv0QhyaIOiQkJLC3sBD71zEWa3E1IYCmJqiLq7vwQLgDD3adO3d2lCISExNdNlAHciO7OwK5iubHH39sEtN9B0IJSBNEHapPa9EqKYlWBM+0CsHuMNaE5pygDwOt/BeSw/Tp05k8ebLL0gMEdiO7uwLhAuVKixYtHGtC1Dbdd7NmzaisqKgRf2izZj6L83wESglIE0QdGsPEfoeBZ22XqkC6wNbHOQkXBGCC7ty5M6tXr/Z3GF5z9uxZZr30LMU/Fju2GaxdRc919tT42Hj+9te/nXdssbGxjjYIV9N9JyYmsnz5clpGRtZ6jgULFpCamsrFF18MwH333cfDDz9Mt27dzju+xkQTRCMWzCWgYJgWoi6B3MjurqIfiym4qsBzJ9zqmdOEh4fTsmVLjh8/fs7TfS9YsIAePXo4EsQ///lPzwTnIYFSAtIE0Yg1hhKQ8o9mzZrhj2Fl//rXv8jIyODs2bMMHDiQ5ORk8vLyeO655wDrhT0rK4sXXniBtLQ0CgoKKCsrY/Lkydx///1VzpV/6BA3/+Y3fGEbiT1v/nxOl5czaNAgsrKyGDduHFFRUWzevJnhw4c7FiVatGgR//u//4sxhptvvpm//OUvAMTExDB58mRWrlxJVFQU77//vlcb7MvB0WYVZnvs66WpmuxcTCo42Cf2y8nJ0QTXyO3Zs4clS5bw6aefkp2dTWhoKDExMSxbtsyxz5IlSxg7dizh4eEsWrSIL7/8kqysLDIyMqr0bgJbnT3Wi+xhflrs/tZbb6Vfv3689dZbZGdnVxk0+P333zNt2jTWr19PdnY2mZmZLF++HIBTp05x1VVXsX37dgYPHsy8efO89llEREQQGR2NCQnBhIQQGh1NZHQ0ERERXntNV7QEUY+GzpwaaII9fnBv1G8gCuRGdnf4ekjlRx99xNatW+nfvz9gnaKkbdu2XHbZZXz++eckJSWxd+9ex3TgGRkZjuRx8OBBcnJyHCOrmzVrRkRkJNgusABh4eFU1jPoLzMzk5SUFNq0aQPAuHHj2LRpE//zP/9Ds2bNHFOJX3nllaxbt87zH4JN9TEoOhdTAAvWC5RdMMcfrBMTBnoje30iIiJ8PneRMYa0tDRmzZpVZfv8+fN55513uOKKKxg9ejQiwsaNG/nwww/ZvHkz0dHRpKSkOMZrALRt25YWLVoQGhpKpK2xOioqivDwcM5VeHi44zMJDQ2loqLinM/ljiNHjvh9DIomiHoE6wXKLtjjD1bB3sgeHx/v87Web7jhBkaNGsWUKVNo27YtxcXFnDx5ktGjR/PMM8+wbds2R3vAiRMnaN26NdHR0ezdu5fPP//c5Xs4evQox44dIzo6mpUrVzJs2DDAOq+Ufa0FZwMGDCA9PZ3CwkJat27NokWLmDRpknffeB1CQvzbCqAJQinlUnxsPGzFcad8vgkjPrbuu99u3brx9NNPk5qaisViITw8nJdffplLL72Url27snv3bgYMGABYZ9R99dVX6dq1K126dOGqq66qcb7i4mImTZrEL37xC+Lj47nsssscz40fP57f/OY3jkZqu4suuojZs2dz3XXXORqpR40adV7v+1zFx8f7fdS62EckBrt+/fqZrKwsf4ehVA3BUoLYs2cPXbt2rbHd3/Xg5yoQZ6P1N1e/YxHZaozp52p/LUEopRqlpp4MPEG7uSrlRdpNVwUzLUEo5WXB2ous+pTTR44c0bvyJkYThFJe1Bh6kfm7J43yH68mCBEZBryEdYT4P40xs6s9fykwH2gDFAN3GmPybc+lAdNtuz5tjFnozViVUlVpaUF57dZAREKBl4HhQDfgdhGpPlXiHOBNY0wyMBOYZTv2QuDPwEBgAPBnEWntrViVUkrV5M0SxAAg1xjzLYCILAZGAbud9ukGPGz7eQOw3PbzUGCdMabYduw6YBiwyIvxKqWc/GHiRI4fOeqx87WKb8uzfzv/6b5dycvL47PPPuOOO+5wbJs1axavv/46oaGhZGRkMHToUABeeukl5s2bhzGGCRMm8NBDD3klpsbAmwmiPXDQ6XE+1hKBs+3ALViroUYDzUUktpZj23svVKVUdcePHGXckSMeO99bDdjXGIMxxu32j7y8PN5++21Hgti9ezeLFy9m165dfP/999x4441888037Nmzh3nz5rFlyxaaNWvGsGHDGDFiRFBMf+IP/m59egS4VkS2AdcCh/hp0sV6icj9IpIlIlkFBR6ct14p5XN5eXl06dKFu+++mx49enDvvffSo0cPevbsyZIlSwBr4pg6dWqN7Y8++igff/wxvXv35oUXXuD9999n7NixRERE0LFjRzp16sSWLVvYs2cPAwcOJDo6mrCwMK699lree+89f77tgObNEsQh4BKnxwm2bQ7GmO+xliAQkRhgjDHmuIgcAlKqHbux+gsYY14DXgPrSGoPxq6U8oOcnBwWLlzIoUOHePXVV9m+fTuFhYX079+fwYMH89lnn5GdnV1j++zZs5kzZw4rV64EYOLEiVWm30hISODQoUP06NGDxx9/nKKiIqKioli1ahX9+rkcRKzwbgkiE0gSkY4i0gwYC6xw3kFE4kTEHsNjWHs0AawBUkWkta1xOtW2TSnViF166aVcddVVfPLJJ9x+++2EhoYSHx/PtddeS2ZmZq3b3dW1a1emTZtGamoqw4YNo3fv3oSG+noZnuDhtQRhjKkAJmK9sO8B3jHG7BKRmSIy0rZbCvC1iHwDxAPP2I4tBp7CmmQygZn2BmulVON1wQUXeOQ87du35+DBn5ox8/Pzad/e2ox57733snXrVjZt2kTr1q3p3LmzR16zMfJqG4QxZpUxprMx5nJjjP3i/4QxZoXt56XGmCTbPvcZY8qcjp1vjOlk+/eGN+NUSgWWa665hiVLllBZWUlBQQGbNm1iwIABtW6vPn33yJEjWbx4MWVlZXz33Xfk5OQ4ZoI9etTaM+vAgQO89957VXo+qap0JLVSyqVW8W0b1PPInfO5a/To0WzevJlevXohIjz77LO0a9eu1u2xsbGEhobSq1cvxo8fz5QpU7jtttvo1q0bYWFhvPzyy46qpDFjxlBUVOSYTrxVq2BZ48/3dLpvpRRQ+3TfqvFo6HTf/u7mqpRSKkBpglBKKeWSJgillFIuaYJQSinlkiYIpZRSLmmCUEop5ZKOg1BKufT7h6ZSVHjMY+eLjWvNX198zmPna6iYmBhKSkr89vrBSBOEUsqlosJj9Isf5bHzZR1532PnUr6hVUxKqYDx5ptvkpycTK9evbjrrrvIy8vj+uuvJzk5mRtuuIEDBw4AsG/fPq666ip69uzJ9OnTiYmJAaCkpIQbbriBvn370rNnT95/33VSeu655+jfvz/Jycn8+c9/BmDZsmXccMMNGGP44Ycf6Ny5M4cPH+aqq65i165djmNTUlJoKoNyNUEopQLCrl27ePrpp1m/fj3bt2/npZdeYtKkSaSlpbFjxw7GjRtHeno6AJMnT2by5Ml89dVXJCQkOM4RGRnJsmXL+PLLL9mwYQO///3vqT5bxNq1a8nJyWHLli1kZ2c7Ju4bPXo0F110ES+//DITJkxgxowZtGvXjl/96le88847APzwww/88MMPTWaKcE0QSqmAsH79en75y18SFxcHwIUXXsjmzZsdk+ndddddfPLJJwBs3ryZX/7ylwBVJtszxvDHP/6R5ORkbrzxRg4dOsSRaqvirV27lrVr19KnTx/69u3L3r17ycnJAWDu3LnMmjWLiIgIbr/9dgBuu+02li5dCsA777zDrbfe6sVPIbBoG4RSqtF46623KCgoYOvWrYSHh5OYmMiZM2eq7GOM4bHHHuOBBx6ocXx+fj4hISEcOXIEi8VCSEgI7du3JzY2lh07drBkyRJeffVVX70dv9MShFIqIFx//fX8+9//pqioCIDi4mJ+/vOfs3jxYsB68b/mmmsAuOqqq3j33XcBHM8DnDhxgrZt2xIeHs6GDRvYv39/jdcZOnQo8+fPd/RoOnToEEePHqWiooJ77rmHRYsW0bVrV55//nnHMb/61a949tlnOXHiBMnJyd75AAKQliCUUi7FxrX2aM+j2LjWdT7fvXt3Hn/8ca699lpCQ0Pp06cPc+fO5de//jXPPfccbdq04Y03rEvDvPjii9x5550888wzDBs2jJYtWwIwbtw4fvGLX9CzZ0/69evHFVdcUeN1UlNT2bNnDz/72c8Aa/fXf/3rX7z66qtcc801XH311fTq1Yv+/ftz880307VrV2699VYmT57Mn/70J499HsFAp/tWSgHBNd13aWkpUVFRiAiLFy9m0aJFtfZYUj9p6HTfWoJQSgWdrVu3MnHiRIwxtGrVivnz59d/kGowTRBKqaBzzTXXsH37dn+H0ehpI7VSSimX3EoQInKBiITYfu4sIiNFJNy7oSmllPInd0sQm4BIEWkPrAXuAhZ4KyillFL+526CEGNMKXAL8HdjzC+B7t4LSymllL+520gtIvIzYBxwr21bqHdCUkoFgofTJ1FYUOCx88W1acPzGXNrfT4vL48RI0awc+dOj71mXTZu3MicOXNYuXLleZ8rMTGRrKwsxzQh7vr+++9JT093TOXREOPHj2fEiBFenfrD3QTxEPAYsO9dxrIAACAASURBVMwYs0tELgM2eC0qpZTfFRYU0CW0wmPn+9qDyeZ8VVR47n2dj4svvvickoOvuFXFZIz5rzFmpDHmL7bH3xpj0us7TkSGicjXIpIrIo+6eL6DiGwQkW0iskNEbrJtTxSR0yKSbfvXdCY/UaoJq6ysZMKECXTv3p3U1FR27dpF3759Hc/n5OQ4HicmJvKHP/yBnj17MmDAAHJzcwEoKChgzJgx9O/fn/79+/Ppp58C8OSTT3LXXXcxaNAg7rrrriqve+rUKe655x4GDBhAnz59HIPuJk+ezMyZMwFYs2YNgwcPxmKx1Pke/vWvfzFgwAB69+7NAw88QGVlJZmZmSQnJ3PmzBlOnTpF9+7d2blzJ3l5efTo0cPx3h955BF69OhBcnIyc+daS1szZ86kf//+9OjRg/vvv7/G7LTeVGcJQkQ+AGqNxhgzso5jQ4GXgSFAPpApIiuMMbuddpsOvGOMeUVEugGrgETbc/uMMb3dehdKqUYhJyeHRYsWMW/ePG677Ta2bdtGy5Ytyc7Opnfv3rzxxhv8+te/duzfsmVLvvrqK958800eeughVq5cyeTJk5kyZQpXX301Bw4cYOjQoezZsweA3bt388knnxAVFcXGjRsd53nmmWe4/vrrmT9/PsePH2fAgAHceOONzJo1i/79+3PNNdeQnp7OqlWrCAmp/b56z549LFmyhE8//ZTw8HB+97vf8dZbb3H33XczcuRIpk+fzunTp7nzzjvp0aMHeXl5jmNfe+018vLyyM7OJiwsjOLiYgAmTpzIE088AVhntF25ciW/+MUvPPip166+KqY5tv9vAdoB/7I9vh044vKInwwAco0x3wKIyGJgFOCcIAzQwvZzS+B798JWSjVGHTt2pHdv633hlVdeSV5eHvfddx9vvPEGzz//PEuWLGHLli2O/e1Tct9+++1MmTIFgA8//JDdu3+6zPz444+OiflGjhxJVFRUjdddu3YtK1asYM4c6yXvzJkzHDhwgK5duzJv3jwGDx7MCy+8wOWXX15n/B999BFbt26lf//+AJw+fZq2bdsC8MQTT9C/f38iIyPJyMioceyHH37Ib37zG8LCrJflCy+8EIANGzbw7LPPUlpaSnFxMd27dw+MBGGM+S+AiPy12lwdH4hIfRMftQcOOj3OBwZW2+dJYK2ITAIuAG50eq6jiGwDfgSmG2M+ruf1lFJBLiIiwvFzaGgop0+fZsyYMcyYMYPrr7+eK6+8ktjYWMc+IlLjZ4vFwueff05kZGSN819wwQUuX9cYw7vvvkuXLl1qPPfVV18RGxvL999b718rKyu58sorAWvCsVdB2c+TlpbGrFmzapynqKiIkpISysvLOXPmTK2xODtz5gy/+93vyMrK4pJLLuHJJ5+sMX25N7nbzfUCW8M0ACLSEesF/XzdDiwwxiQANwH/ZxuQ9wPQwRjTB3gYeFtEWlQ/WETuF5EsEckqCKAGMKWU50RGRjJ06FB++9vfVqleAliyZInjf/vsrKmpqY76e4Ds7Ox6X2Po0KHMnTvXUb+/bds2APbv389f//pXtm3bxurVq/niiy8IDQ0lOzub7OzsKskB4IYbbmDp0qUcPXoUsE5Zbp9y/IEHHuCpp55i3LhxTJs2rUYMQ4YM4R//+IejAb24uNiRDOLi4igpKfF5g7a7vZimABtF5FtAgEuBmqttVHUIuMTpcYJtm7N7gWEAxpjNIhIJxBljjgJltu1bRWQf0BmoUmoxxrwGvAbW2VzdfC9KKTfEtWnj0Z5HcW3anPOx48aNY9myZaSmplbZfuzYMZKTk4mIiGDRokUAZGRk8OCDD5KcnExFRQWDBw+ud5GfP/3pTzz00EMkJydjsVjo2LEjH3zwAffeey9z5szh4osv5vXXX2f8+PFkZma6LJ0AdOvWjaeffprU1FQsFgvh4eG8/PLL/Pe//yU8PJw77riDyspKfv7zn7N+/Xouu8xx3819993HN998Q3JyMuHh4UyYMIGJEycyYcIEevToQbt27RxVV77i9nTfIhIB2CdX32uMKatn/zDgG+AGrIkhE7jDGLPLaZ/VwBJjzAIR6Qp8hLVqKg4oNsZU2kouHwM9jTHFtb2eTvet1PkJ5Om+58yZw4kTJ3jqqacc28517EFT5pXpvkUkGmtVz6XGmAkikiQiXYwxtY4wMcZUiMhEYA3WQXXzbWMoZgJZxpgVwO+BeSIyBWuD9XhjjBGRwcBMESkHLMBv6koOSqnGa/To0ezbt4/169f7O5Qmx90qpjeArcDPbI8PAf8G6hyCaIxZhbXrqvO2J5x+3g0McnHcu8C7bsamlGrEli1b5nK7cxdR5R3uNlJfbox5FigHsM3LJHUfopRSKpi5myDOikgUtkFzInI5tkZkpZRSjZO7VUx/Bv4fcImIvIW1Wmi8t4JSSinlf/UmCNu4hNZYR1NfhbVqabIxptDLsSmllPKjehOEMcYiIn8wxrwD/McHMSmlAsCjD/+BE4XHPHa+lnGtmf38s7U+H8zTfXtaSkoKc+bMoV8/l71PfcbdKqYPReQRYAlwyr5Ru54q1XidKDxGepfbPXa+jK8Xeexc58ub031XVlYSGto4lstxt5H6V8DvgP9iHc1s/6eUUh4TrNN9x8TE8Pvf/55evXqxefPmWqfoTklJYdq0aQwYMIDOnTvz8cfWKeZOnz7N2LFj6dq1K6NHj+b06dOOcy9atIiePXvSo0ePKlN0xMTEMHXqVLp3786NN97Ili1bSElJ4bLLLmPFihXn94uwcTdBdMM6dfd2IBuYiy45qpTysJycHB588EF27dpFq1atqkz3DdQ63ffEiRN56KGHABzTfWdmZvLuu+9y3333OfbfvXs3H374oWNaDjv7dN9btmxhw4YNTJ06lVOnTjFr1iyWLFnChg0bSE9P54033nA53fepU6cYOHAg27dv5+qrr2bixIlkZmayc+dOTp8+XaUaq6Kigi1btvDiiy8yY8YMAF555RWio6PZs2cPM2bMYOvWrYB1xblp06axfv16srOzyczMZPny5Y7XvP7669m1axfNmzdn+vTprFu3jmXLljmmBz9f7iaIhUBXIANrcuhm26aUUh5T13TflZWVLFmyhDvuuMOxv/N035s3bwas02ZPnDiR3r17M3LkSLen+549eza9e/cmJSXFMd13dHQ08+bNY8iQIUycOLHW6b5DQ0MZM2aM4/GGDRsYOHAgPXv2ZP369eza5ZhhiFtuuaXK+wPYtGkTd955JwDJyckkJycDkJmZSUpKCm3atCEsLIxx48axadMmAJo1a8awYcMA6NmzJ9deey3h4eH07NnTY4MI3W2D6GGM6eb0eIOI7K51b6WUOgfBOt13ZGSko92hvim67e8xNDT0vNpCwsPDHe85JCTEcd6QkBCPtbG4W4L4UkSusj8QkYFoG4RSygeCabpv4Jym6B48eDBvv/02ADt37mTHjh0ADBgwgP/+978UFhZSWVnJokWLuPbaa+s9n6e4W4K4EvhMRA7YHncAvhaRrwBjjEn2SnRKKb9pGdfaoz2PWsa1Pudjg2W6b4BWrVo1eIpue/Lr2rUrXbt2dZRQLrroImbPns11112HMYabb76ZUaNG1Xs+T3Frum8RubSu540x+z0W0TnS6b6VOj863Xfj55XpvgMhASilmiad7tt/3K1iUkopv9Dpvv3H3UZqpVQT4O4Kkyr4nMvvVhOEUgqw9hYqKirSJNEIGWMoKiqqs3HdFa1iUkoBkJCQQH5+PgUFBf4ORXlBZGQkCQkJDTpGE4RSCrAOvOrYsaO/w1ABRKuYlFJKuaQJQimllEuaIJRSSrmkCUIppZRLmiCUUkq5pAlCKaWUS15NECIyTES+FpFcEXnUxfMdRGSDiGwTkR0icpPTc4/ZjvtaRIZ6M06llFI1eW0chIiEYl2mdAiQD2SKyApjjPNCQ9OBd4wxr4hIN2AVkGj7eSzWZU0vBj4Ukc7GmEpvxauUUqoqb5YgBgC5xphvjTFngcVA9YnMDdDC9nNL4Hvbz6OAxcaYMmPMd0Cu7XxKKaV8xJsJoj1w0Olxvm2bsyeBO0UkH2vpYVIDjlVKKeVF/m6kvh1YYIxJAG4C/k9E3I5JRO4XkSwRydL5Y5RSyrO8mSAOAZc4PU6wbXN2L/AOgDFmMxAJxLl5LMaY14wx/Ywx/dq0aePB0JVSSnkzQWQCSSLSUUSaYW10XlFtnwPADQAi0hVrgiiw7TdWRCJEpCOQBGzxYqxKKaWq8VovJmNMhYhMBNYAocB8Y8wuEZkJZBljVgC/B+aJyBSsDdbjjXUy+l0i8g6wG6gAHtQeTEop5VvSWBYH6devn8nKyvJ3GEopFVREZKsxpp+r5/zdSK2UUipAaYJQSinlkiYIpZRSLumSo6pRy8jIYOPGjYB1zeVOnTqRnp7u36CUChJaglABpbCwkEmTJlFUVOSxc54+fZrTp0977HxKNRVaglABZeHChezYsYOFCxfy8MMPn/f50tPTyc3NBaylCdU4OZcUU1JStJToIVqCUAGjsLCQVatWYYxh1apVHi1FqMZPS4qepwlCBYyFCxdSUVEBQHl5OQsXLmzwObxRRaUCX3p6OklJSSQlJWnpwYM0QaiAsXbtWuwDN40xrFmzpsHncK6iUkqdH22D8LKMjAxyc3PJz88HtH60LvHx8eTl5VV53BCFhYWsXr0aYwyrV68mLS2N2NhYD0fZOGlvL+WKliCcZGRkcMstt3DLLbd4vEFT60frd+TIkTof12fhwoWOEojFYtFSRAPpd1RVpyWIajz9B2K/C6v+v6opNTWVFStWYIxBRBg6tGFLka9bt47y8nLA2oaxcuVK8vLyyMnJAX767PXuuCbt7aVc0RKEk8bQ0BXMjbRpaWmEh4cDEB4eTlpaWoOOHzJkSJXjW7Zsydc79xBeEUJ4RQil+4/x9c49jguhUqpuWoJoZDw9jsCX4uLiGD58OCtWrOCmm25qcPtBWloaq1evBiAkJIR27dpRbinh9wN+7djnr1ve8GjMdlqHrxojTRBeZG+gBqpUc3jr4tEYGmnT0tLIy8urs/RQ28XYOcEMHz6cvLw8yn0UN3i+elIpf9ME4UW5ubl8vXMPlzRvR3iFtTbv6517vPZ6rhppg6UUUb2311tvvVVnEq3tYuycYGbMmOGVWF3ROnzVGGmC8LJLmrfzSRUH1GykXbt2bdAkCDt37sLruhjHxcUxd+5cr8SmVFOjCaIRGTJkCKtWraK8vJzw8HBSU1P9HZLbmlJvr2Bvr2hK8x41pffqivZiakTS0tIQEcDaSNvQXkDKd4J9zEEwx9/Q8U7B/F7Pl5YgGgl7Hb49QbRv377WBmod3e1fwd5e4Rx/fd+bQL0Dd/eC35D32hhpgvCi/Px8Tp08WaXd4eDJw1yQf8prrxkSEkJISAjdunWrd9+melekfCvQvmeuLvrBXu3nLZogqL07KrgedRuId+DV6+7/8Ic/uL2vv2P3dXfgYBLsF65gugMPtEQWCDRBYO2Oum3XNmgFWKzbth3aBsfrPq6+L1RCQgKllcdq9GKKTmh9nhE3Li4//13bXO5rTyZNafoMvXB5X7BX+3mLJgi7VmBJsVTZFLLRdRt+oN2BNwrVPv/aPvvc3Fx2fbWHsBDrlBqH9hVxvPSoT0KsTUOSVkMTnF64gk9GRgarV6+mtLTUMS5JRIiOjmb48OFBdb3QBKH8Lj8/H05USwrHId/ku9y/VXRbrrtirOPxhr2LvR1inRwlINtfU12lz0BMcMGsodXDqmG8miBEZBjwEhAK/NMYM7va8y8A19keRgNtjTGtbM9VAl/ZnjtgjBnpzViVOi9uloAg8BJcMDvX6mFvSk9PbzSJyWsJQkRCgZeBIUA+kCkiK4wxu+37GGOmOO0/CejjdIrTxpje3orvXOkdi+clJCRQIAU1LrAJ7RP8GJWqLmC/+w2oHlYN480SxAAg1xjzLYCILAZGAbtr2f924M9ejMcj7FUEraLbYjlrHXOg1QQecNz2R11ie1wBtPdnQE2Hu+0i+t1veryZINoDB50e5wMDXe0oIpcCHYH1TpsjRSQL66VitjFmubcCdVkHDrXWg1evIgCtJjgfnTp1cvxsv0gldU+qsl15T0PaUPS737QESiP1WGCpMabSadulxphDInIZsF5EvjLG7HM+SETuB+4H6NChg++iDTDBPo7AOUbngUvKhxrQhuKOgK2OUg3izQRxCLjE6XGCbZsrY4EHnTcYYw7Z/v9WRDZibZ/YV22f14DXAPr162fONVBXdeAQPPXgDRlHoJQ35Ofnk56e7hg8CnDq+MkqU92X7j/GwZOHPf66DSn9u0sTnJU3E0QmkCQiHbEmhrHAHdV3EpErgNbAZqdtrYFSY0yZiMQBg4BnvRhrUHP+oyTG9v/xatuV8gH7oL6oqKgaU92Dd6e79yRXa7l4I8EFOq8lCGNMhYhMBNZg7eY63xizS0RmAlnGmBW2XccCi419RIlVV+AfImLBOuPsbOfeT0qpwJKQkEBGRkaVO+vS/cd88rrulv7rKxVUF8wJzlO82gZhjFkFrKq27Ylqj590cdxnQE9vxtaYeLObaCDOO1Wb6jOHKvc0dKBisNJeWA0XKI3U/le9m2WMdZt2tbTy9nxA1btaVr8bdZfOWxQYXM1kDN6fzbg+rnphfZD9iuN7B9ZSRU5ODu2bxfk6vICjCYJaulm2T4L2rouetcnPz+eWW24BrHf1+fn5XMgFng22Nl4aR+DreaeioqLO+djqM4cGakkn0DT1gYoVlnJM6VlCQ6yliv17dnGqrBw0QWiCAM90syw5c4zy02ewWKx/ZKdPn6asrIwLm3s/QTSGcQTnczEvOXOMnJxix52f/Xx6F+g/rmYyhsCdzbh5szAGxP8U10cHC/wYTeDQBOEhFZZysFQSbrsLaVZxllMVFT557fT09CoNcNC0uuLZ7wD379lFRYV1KI39LvBgWYVPF2xSjUOlMRw8eTjgqsh8TROEh1RaKggBosNCq2w/Wlrs0zjOp4ommFW/AwRYe8A3jY/eauRtimtfnBNtP/QaTRCNhLsXjGAfde1KpaWCk2ctbDlStVulASJCmwXtgk32vvgRoc2AptkPvz6eaj+sLlSk1m6uwfL98QRNEB4SGhJGTJilRj1m2+gL/RhVTa66+u36ao+fowpu3mzkrX6Ramr98OvjifZDVzcYFcb4vPQfiDRBeNDxsvIqjVsVTvWY9i9bWeVZuuDfO5DGth6Bq+QMgZmg8/PzOVF6sspnfrz0KCZfu+eqwKMJwklGRka9/fBd/YEDVFrKQYSI6GjHtrDKSi6IiSE6oTXlOYUAdOnaNWh6FzXG6qhg4mosQVNrJPWFYCn9+4MmiGrOtZE3NCSclq2b895777l8PhhnKXU1H83XO7U66nwkJCQgZUU1SnDtE2L9GBXWht6VtkZ2beRVNpognLgzuMrVHzgEyB+5hzlP9me/mzp48rBOAugjrsYSeKOR1F6iPddGXlfjUICgH4NSvXq4bfSFHDx52O9VxL6kCUKpJq76KPmGlnJdjUMpq7QE9Ujk0BAhJCKM6Et/qh6OvrQ1XWgdNFXEnqAJQtXKV3ewKvg1tpHI0WGhXJqUVKUtMpiqhz1FE4Rq1A6ePMy0jX8FmmYVgVLn4/zWFVQ1FBYWMmnSJIqKiprk6wcSezVBeZiF8jCLtYqgR/D0IlPK37QE4WELFy5kx44dLFy4kIcffrjJvb6/nDxbwZYjxyi11YFHh4VSaTF0TUpy7BPIVQTVl+zU9SxUINAE4UHl5eWsXr0aYwyrV68mLS2N2Fjf9WwqLCys9/Ub40CtsJBwQiLDuDQpydGL5lJbYujUqVOVSQy9xkPdRKuvZ6FVZJ5T2ximisqzlCJ+iiqwaRWTBx0+fBj7yqkWi4WFCxf69PUXLlzo19f3l5jI1iTZGhSTkpIcP5/rokMN1alTJ/p070NMSAwxITH0ad+HPt37NKgqy75kpz1++4DELj26ahWZ8hstQXjQsWPHHOtBlJeXs3btWp9W86xbt47y8vI6Xz9gB2oFsfPtJurr8zZVtY1hWvZlBtFhllqOato0QXhQ69at+fHHHykvLyc8PJzU1FSfvv6QIUNYtWpVg1+/5Mwxduw4xLXXXusogYgIIkKLcB+tiOemxlhFFiy0iqbp0QThQe3atePkyZMAhISEkJaW5tPXT0tLY/Xq1X57fXVualsRL9jnvAqmkcgnz1aw8ZBtQFxYKCfP+maxr0CnCeIcHC89yoa9iyk5Y50eOCayNcdLj9I+PJbhw4ezYsUKhg8f7tMGaoC4uLhzev2YyNZ0ubxTjSqM9PR0Svcfq+Uo/2iMVWSuRiLv3L7dz1HVVFsVzdKs56m0VJ0uu9IYoDIoRiLbOznYOXdwaOo0QTRQ1QVKrHdG7S+PpT2xdOrUiTvuuIO8vDy/3b2npaX59fXVuak+Ern64kfBJlSEVq1a+XQkcnl5OXl5eRQVFTXo5iwmsjXtL/9pf23r+YkmiGoKCwuZMWMGTz75pMsvmTsLlMydO9e7QdYhLi7Or6+vGjdXpWdjLDSPqJngEhLOf8Gkhjh8+DCnTp2qcwxQraV/grcE6k2aIKppCgPNqv+RVFjKA+YPpL4EDdb4P8h+BdA/cF+qrfRcmlPMydLSKgMVKy3Gp7H95S9/ccwe8MEHH7gcA1Rf6d8n42WCjFcThIgMA14CQoF/GmNmV3v+BeA628NooK0xppXtuTRguu25p40xXu/U785As4aqvvC8r/rm18bVH0n3pMDpW19fgq4+NbXzH7g7Cz6pc1db6dnVwlJdk5J8+p3as+endUqMMS6/P/WV/vX7UpPXEoSIhAIvA0OAfCBTRFYYY3bb9zHGTHHafxLQx/bzhcCfgX5Y157fajvWqxWzrgaaeaoUca4LEXlaQ9fw9eWSqe4k6LrGBmRkZATM51yX6iWg8soyjlVaZ0CttH3/DBDqw3U3nJNrQ3tQeWJd6PN1+PBhx88Wi6XOMUh6I+E+b5YgBgC5xphvAURkMTAK2F3L/rdjTQoAQ4F1xphi27HrgGHAIi/G69ZAM2hYqSCYv3ydOnUiPz+fQ6cLOV1uHWdw4YUXeu3O0N0EXdvFzNWCT+dz4WsId1/HVQno5N4CysrKsIg43n9YWBitWrXyeJx1cTe5BuIFtqFjgKq/19p+f4H4Xn3JmwmiPXDQ6XE+MNDVjiJyKdARWF/HsV5fAPF8v2TBwt0EZ9+Wm5tbZRI5b/2RuJugoWGfva9+T+68TvWLDsCIESMA15+zrxKcO6spOnN1gXX1nfLVBbYhY4Bqe6+1/f6C9e/cEwKlkXossNQYU9mQg0TkfuB+gA4dOpx3EO5+yRrLXYS7FzRfcTdBN+Ri1tAL37k63wtsXccG2gWqrvfqKlZfxH+uY4DsantPvvr+BCqxF2k9fmKRnwFPGmOG2h4/BmCMmeVi323Ag8aYz2yPbwdSjDEP2B7/A9hojKm1iqlfv34mKyvrvOP+61//yooVKxg1alSj7cUUqAoLCxk7dixnz54lIiKCxYsX+3ywoQpe7vSAUzWJyFZjTD9Xz3lzNtdMIElEOopIM6ylhBUugrsCaA1sdtq8BkgVkdYi0hpItW3zurS0NJKTk3WgmR/Y7wJFxC8j0VVws48B0u+N53itiskYUyEiE7Fe2EOB+caYXSIyE8gyxtiTxVhgsXEqyhhjikXkKaxJBmCmvcHa23SgmX/pSHClAofXqph8zVNVTEop1ZT4q4pJKaVUENMEoZRSyiVNEEoppVzSBKGUUsolTRBKKaVcajS9mESkANjvxZeIAwq9eH5v0/j9S+P3r2CO39uxX2qMaePqiUaTILxNRLJq6woWDDR+/9L4/SuY4/dn7FrFpJRSyiVNEEoppVzSBOG+1/wdwHnS+P1L4/evYI7fb7FrG4RSSimXtAShlFLKpUBZMChgiEgs8JHtYTugEiiwPf4SGAEcNcb08EN49aoj/ubAASAe65LHrxljXvJLkHWoI/5IoBTrzMBhWBeY+rPLk/hIPd+VXsBbxpg7bfuGAT8AXxhjRojIOGAaIMBJ4LfGmO1BFP8o4CnAAlQADxljPgmi+K8A3gD6Ao8bY+b4MnZX6nk/A2yPs4BDxpgRvohJE0Q1xpgioDeAiDwJlNi/PCIyGPgb8KbfAqxHbfGLyEXARcaYL0WkObBVRNYZY2pbI9wv6ohfgAuMMSUiEg58IiKrjTGfB1qstsclQA8RiTLGnAaGAIecDv8OuNYYc0xEhmOtZ3a5JG+Axv8RsMIYY0QkGXgHuCKI4i8G0oH/8WXMdanr/di2PQzsAVr4KiatYmoAY8wmrF+soGOM+cEY86Xt55NYv2heX+fbU4xVie1huO1foDegrQJutv18O+BYEdEY85kx5pjt4edAgo9jc0dd8Zc4reFyAYH5u6gr/qPGmEyg3B+BNZSIJGB9L//05etqgmiCRCQR6AN84d9IGkZEQkUkGzgKrDPGBHr8i4GxIhIJJFP7530vsNpnUbmvzvhFZLSI7AX+A9zjh/jq4+7nHwxeBP6AtUrPZzRBNDEiEgO8i7XO+Ed/x9MQxphKY0xvrHfbA0QkINuB7IwxO4BErHevq1ztIyLXYU0Q03wXmXvqi98Ys8wYcwXWapqnfBtd/dz5/IOBiNjbPbf6+rU1QTQhtrr7d7E23r3n73jOlTHmOLABGObvWNywApiDU/WGna3u/p/AKFv9cyCqNX47W9XrZSIS57Oo3Fdv/EFgEDBSRPKwloquF5F/+eKFNUE0EbZG3teBPcaY5/0dT0OJSBsRaWX7OQpro+Ne/0bllvnADGPMV84bRaQD8B5wlzHmG79E5p7a4u9k+04hIn2BCCAQk5zL+IOJMeYxY0yCMSYRGAust/fO8jbtxdQA0rOOnQAAAtVJREFUIrIISAHiRCQf+LMx5nX/RuW2QcBdwFe2enyAPxpjgqXofRGwUERCsd7YvGOMWennmOpljMkHMlw89QQQC/zddp2tCMTJ5OqIfwxwt4iUA6eBXzk1WgeM2uIXkXZYu4y2ACwi8hDQLdiqXb1NR1IrpZRySauYlFJKuaQJQimllEuaIJRSSrmkCUIppZRLmiCUUkq5pAlCKSci0kpEfufl10gUkZ1u7HOHN+NQqj6aIJSqqhXg1QThpkRAE4TyK00QSlU1G7hcRLJF5DkRmSoimSKyQ0RmgOPufq+ILBCRb0TkLRG5UUQ+FZEcERlg2+9JEfk/Edls2z6h+ovZzvWxiHxp+/dzpziuscUxxTZR4XNOsTzgs09ENVk6klqpqh4FehhjeotIKnAr1sVaBFhhWxPkANAJ+CXWWUwzsd7tXw2MBP7IT+sMJANXYZ0Se5uI/Kfa6x0FhhhjzohIEtY5g/rZ4njEvjCMiNwPnDDG9BeRCOBTEVlrjPnOK5+CUmiCUKouqbZ/22yPY4AkrAniO/v8PiKyC/jItnjOV1irh+zety1Yc1pENmBNNtlOz4cDfxOR3lhXDOtcRyzJInKr7XFLWyyaIJTXaIJQqnYCzDLG/KPKRut6GmVOmyxOjy1U/buqPpdN9cdTgCNYl8gMAc7UEcskY8waN2NX6rxpG4RSVZ3Eun43wBrgHtsaGohIexFp28DzjRKRSNt6wylYq6OctQR+MMZYsE6mGOoiDnssv7VN2Y6IdBaRCxoYi1INoiUIpZwYY4psjc07sa7y9jaw2TbjaglwJ9aqIHftwLp2RRzwlDHme1sJxO7vwLsicjfw/4BTTsdVish2YAHwEtaqqy9t02wXEEDrKavGSWdzVcpLXC08r1Qw0SompZRSLmkJQimllEtaglBKKeWSJgillFIuaYJQSinlkiYIpZRSLmmCUEop5ZImCKWUUi79fypdZXMdwOTnAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_plot_prov = df_plot[df_plot['preds']>0.7]\n",
        "sns.boxplot(data = df_plot, x=\"model\", y=\"preds\", hue=\"dataset\")"
      ],
      "metadata": {
        "id": "j9xRT9EUXyhq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "130fef5d-12e3-498e-c4ca-90d705a8a202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468e47a730>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cogalexv weighed f1 of all categories compared to SOTA"
      ],
      "metadata": {
        "id": "-RapkEHxa6YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfm_cogalexv = df_res_sotas_cogalexv.T\n",
        "dfm_cogalexv = pd.melt(dfm_cogalexv)\n",
        "dfm_cogalexv"
      ],
      "metadata": {
        "id": "jmG3HwxTcTE1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "e11aa822-f882-4e2d-85a4-210b298c8511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    model template     value\n",
              "0    Bert       T1  0.770474\n",
              "1    Bert       T1  0.680452\n",
              "2    Bert       T1  0.715095\n",
              "3    Bert       T1  0.563975\n",
              "4    Bert       T1  0.690274\n",
              "..    ...      ...       ...\n",
              "155  Sota  RelBert  0.794000\n",
              "156  Sota  RelBert  0.616000\n",
              "157  Sota  RelBert  0.702000\n",
              "158  Sota  RelBert  0.505000\n",
              "159  Sota  RelBert  0.664000\n",
              "\n",
              "[160 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5c1845a-a5b5-499b-975a-e8632c8298cf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.770474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.680452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.715095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.563975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.690274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.505000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBert</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160 rows √ó 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5c1845a-a5b5-499b-975a-e8632c8298cf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5c1845a-a5b5-499b-975a-e8632c8298cf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5c1845a-a5b5-499b-975a-e8632c8298cf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=dfm_cogalexv, x= 'template', y='value', hue='model')"
      ],
      "metadata": {
        "id": "dbpZZl5CbbtG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "d114e3f3-5654-4fd5-deaf-cb955156b509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468dd8bbe0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=dfm_cogalexv, x= 'model', y='value', hue='template')"
      ],
      "metadata": {
        "id": "RvWF7kIQfhf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "b49f3ee2-6da9-4b9f-df87-1a0c3fd4dab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468ddbf700>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Large experiments (the ones with better results and less variability on first sight), with T1(simplest template) compared to SOTA with bar plots. Here variability is not taken into account, just overall f1 weighed score."
      ],
      "metadata": {
        "id": "WGdhfizEf4Ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compare_f1_sota = pd.DataFrame()\n",
        "for dataset in ['K&H+N', 'BLESS', 'EVALution', 'ROOT09']:\n",
        "  df_prov = pd.melt(df_res_sotas[dataset].T)\n",
        "  df_prov['dataset'] = dataset\n",
        "  compare_f1_sota = pd.concat([df_prov, compare_f1_sota])\n",
        "\n"
      ],
      "metadata": {
        "id": "KFE_jBFmoPzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_f1_sota"
      ],
      "metadata": {
        "id": "9x3ylhKjvODr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "1c283eba-972a-49de-992a-f3e56845ed12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   model  template     value dataset\n",
              "0   Bert        T1  0.926534  ROOT09\n",
              "1   Bert        T1  0.926418  ROOT09\n",
              "2   Bert        T1  0.926258  ROOT09\n",
              "3   Bert        T2  0.930032  ROOT09\n",
              "4   Bert        T2  0.928549  ROOT09\n",
              "..   ...       ...       ...     ...\n",
              "91  Sota  SphereRE  0.989000   K&H+N\n",
              "92  Sota  SphereRE  0.990000   K&H+N\n",
              "93  Sota   RelBERT       NaN   K&H+N\n",
              "94  Sota   RelBERT       NaN   K&H+N\n",
              "95  Sota   RelBERT  0.949000   K&H+N\n",
              "\n",
              "[384 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a845e64-a748-4cf0-97be-a3d2a3725b21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>template</th>\n",
              "      <th>value</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926534</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926418</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T1</td>\n",
              "      <td>0.926258</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T2</td>\n",
              "      <td>0.930032</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bert</td>\n",
              "      <td>T2</td>\n",
              "      <td>0.928549</td>\n",
              "      <td>ROOT09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Sota</td>\n",
              "      <td>SphereRE</td>\n",
              "      <td>0.989000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>Sota</td>\n",
              "      <td>SphereRE</td>\n",
              "      <td>0.990000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>Sota</td>\n",
              "      <td>RelBERT</td>\n",
              "      <td>0.949000</td>\n",
              "      <td>K&amp;H+N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>384 rows √ó 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a845e64-a748-4cf0-97be-a3d2a3725b21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a845e64-a748-4cf0-97be-a3d2a3725b21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a845e64-a748-4cf0-97be-a3d2a3725b21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=compare_f1_sota, x= 'model', y='value', hue='dataset')"
      ],
      "metadata": {
        "id": "QAo2ZVWEvVG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "9007b534-7683-4f9c-e8b5-28676ecb27cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468da7d640>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(data=compare_f1_sota, x= 'template', y='value', hue='dataset')"
      ],
      "metadata": {
        "id": "pJDFG80rvkpr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "e36b3cd3-ac3e-496b-a769-70abda5b83fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f468d9a0490>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}