{'train_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/train.tsv', 'val_file': None, 'test_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/test.tsv', 'train_templates': [' <W1> <MASK> <W2> '], 'model': 'roberta-large', 'nepochs': 10, 'dir_output_results': '/content/drive/MyDrive/Research/NLP/results/maskedNew/', 'nrepetitions': 5, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'CogALexV', 'parameters_list': None, 'date': '11/30/22-13:27:54', 'raw_model': False}
11/30/22-13:27:54
{'ant': {'precision': 0.9027355623100304, 'recall': 0.825, 'f1-score': 0.8621190130624092, 'support': 360}, 'hyper': {'precision': 0.7579250720461095, 'recall': 0.6884816753926701, 'f1-score': 0.7215363511659808, 'support': 382}, 'part_of': {'precision': 0.7574468085106383, 'recall': 0.7946428571428571, 'f1-score': 0.7755991285403049, 'support': 224}, 'random': {'precision': 0.954632587859425, 'recall': 0.9767898005884276, 'f1-score': 0.9655841008240427, 'support': 3059}, 'syn': {'precision': 0.6301369863013698, 'recall': 0.5872340425531914, 'f1-score': 0.6079295154185022, 'support': 235}, 'accuracy': 0.9070422535211268, 'macro avg': {'precision': 0.8005754034055146, 'recall': 0.7744296751354293, 'f1-score': 0.7865536218022479, 'support': 4260}, 'weighted avg': {'precision': 0.904338859883195, 'recall': 0.9070422535211268, 'f1-score': 0.9052368864284026, 'support': 4260}}
