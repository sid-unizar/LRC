{'train_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/train.tsv', 'val_file': None, 'test_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/test.tsv', 'train_templates': [' <W1> <SEP> <W2> '], 'test_templates': [' <W1> <SEP> <W2> '], 'model': 'bert-large-uncased-whole-word-masking', 'nepochs': 10, 'dir_output_results': '/content/drive/MyDrive/Research/NLP/results/', 'nrepetitions': 5, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'CogALexV', 'parameters_list': None, 'date': '11/26/22-16:57:31', 'raw_model': False}
11/26/22-16:57:31
{'ANT': {'precision': 0.7806267806267806, 'recall': 0.7611111111111111, 'f1-score': 0.770745428973277, 'support': 360}, 'HYPER': {'precision': 0.7325581395348837, 'recall': 0.6596858638743456, 'f1-score': 0.6942148760330579, 'support': 382}, 'PART_OF': {'precision': 0.691699604743083, 'recall': 0.78125, 'f1-score': 0.7337526205450733, 'support': 224}, 'RANDOM': {'precision': 0.9456695429849792, 'recall': 0.96730957829356, 'f1-score': 0.9563671622495152, 'support': 3059}, 'SYN': {'precision': 0.5737704918032787, 'recall': 0.44680851063829785, 'f1-score': 0.5023923444976077, 'support': 235}, 'accuracy': 0.8838028169014085, 'macro avg': {'precision': 0.744864911938601, 'recall': 0.7232330127834629, 'f1-score': 0.7314944864597063, 'support': 4260}, 'weighted avg': {'precision': 0.8787424317735303, 'recall': 0.8838028169014085, 'f1-score': 0.8804245010223731, 'support': 4260}}
