{'train_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/train.tsv', 'val_file': None, 'test_file': '/content/drive/MyDrive/Research/NLP/Datasets/CogALexV/test.tsv', 'train_templates': [' <W1> <MASK> <W2> '], 'model': 'roberta-large', 'nepochs': 10, 'dir_output_results': '/content/drive/MyDrive/Research/NLP/results/maskedNew/', 'nrepetitions': 5, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'CogALexV', 'parameters_list': None, 'date': '11/30/22-13:27:54', 'raw_model': False}
11/30/22-13:27:54
{'ant': {'precision': 0.888235294117647, 'recall': 0.8388888888888889, 'f1-score': 0.8628571428571428, 'support': 360}, 'hyper': {'precision': 0.7640117994100295, 'recall': 0.6780104712041884, 'f1-score': 0.7184466019417476, 'support': 382}, 'part_of': {'precision': 0.8394495412844036, 'recall': 0.8169642857142857, 'f1-score': 0.828054298642534, 'support': 224}, 'random': {'precision': 0.9577735124760077, 'recall': 0.978751225890814, 'f1-score': 0.9681487469684721, 'support': 3059}, 'syn': {'precision': 0.6033755274261603, 'recall': 0.6085106382978723, 'f1-score': 0.6059322033898306, 'support': 235}, 'accuracy': 0.9110328638497652, 'macro avg': {'precision': 0.8105691349428497, 'recall': 0.7842251019992099, 'f1-score': 0.7966877987599454, 'support': 4260}, 'weighted avg': {'precision': 0.9087503131722878, 'recall': 0.9110328638497652, 'f1-score': 0.9095118359247448, 'support': 4260}}
