{'train_file': '/content/drive/MyDrive/PhD/METAFORAS/RelationClassification/DATASETS/BLESS/train.tsv', 'val_file': '/content/drive/MyDrive/PhD/METAFORAS/RelationClassification/DATASETS/BLESS/val.tsv', 'test_file': '/content/drive/MyDrive/PhD/METAFORAS/RelationClassification/DATASETS/BLESS/test.tsv', 'train_templates': ['Today, I finally discovered the relation between <W1> and <W2>.'], 'test_templates': ['Today, I finally discovered the relation between <W1> and <W2>.'], 'model': 'bert-base-uncased', 'nepochs': 10, 'dir_output_results': '/content/drive/MyDrive/results_28_11_22/', 'nrepetitions': 5, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'BLESS', 'parameters_list': None, 'date': '11/28/22-15:02:51', 'raw_model': False}
11/28/22-15:02:51
{'attri': {'precision': 0.9236209335219236, 'recall': 0.9382183908045977, 'f1-score': 0.9308624376336421, 'support': 696}, 'coord': {'precision': 0.9620958751393534, 'recall': 0.9784580498866213, 'f1-score': 0.9702079820123665, 'support': 882}, 'event': {'precision': 0.8636363636363636, 'recall': 0.9350785340314136, 'f1-score': 0.8979386626445449, 'support': 955}, 'hyper': {'precision': 0.9390581717451524, 'recall': 0.9685714285714285, 'f1-score': 0.9535864978902954, 'support': 350}, 'mero': {'precision': 0.9270557029177718, 'recall': 0.9369973190348525, 'f1-score': 0.9319999999999999, 'support': 746}, 'random': {'precision': 0.9764216366158114, 'recall': 0.9361702127659575, 'f1-score': 0.9558723693143245, 'support': 3008}, 'accuracy': 0.9436492391140575, 'macro avg': {'precision': 0.9319814472627295, 'recall': 0.9489156558491452, 'f1-score': 0.9400779915825289, 'support': 6637}, 'weighted avg': {'precision': 0.9452330354534756, 'recall': 0.9436492391140575, 'f1-score': 0.944014838136591, 'support': 6637}}
