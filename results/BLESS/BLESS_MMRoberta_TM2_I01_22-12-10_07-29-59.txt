{'train_file': '/kaggle/input/lexical-relation-dataset/BLESS/train.tsv', 'val_file': '/kaggle/input/lexical-relation-dataset/BLESS/val.tsv', 'test_file': '/kaggle/input/lexical-relation-dataset/BLESS/test.tsv', 'train_templates': [' <W1> <MASK> <W2> '], 'model': 'roberta-large', 'nepochs': 10, 'dir_output_results': './results/', 'nrepetitions': 2, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'BLESS', 'parameters_list': None, 'date': '12/10/22-06:09:22', 'raw_model': False}
12/10/22-06:09:22
{'attri': {'precision': 0.9039242219215156, 'recall': 0.9597701149425287, 'f1-score': 0.9310104529616724, 'support': 696}, 'coord': {'precision': 0.9764044943820225, 'recall': 0.9852607709750567, 'f1-score': 0.9808126410835215, 'support': 882}, 'event': {'precision': 0.9372900335946248, 'recall': 0.8764397905759163, 'f1-score': 0.9058441558441558, 'support': 955}, 'hyper': {'precision': 0.9222520107238605, 'recall': 0.9828571428571429, 'f1-score': 0.9515905947441217, 'support': 350}, 'mero': {'precision': 0.9228758169934641, 'recall': 0.9463806970509383, 'f1-score': 0.9344804765056255, 'support': 746}, 'random': {'precision': 0.9630500503862949, 'recall': 0.953125, 'f1-score': 0.9580618212197158, 'support': 3008}, 'accuracy': 0.9478680126563206, 'macro avg': {'precision': 0.9376327713336304, 'recall': 0.9506389194002639, 'f1-score': 0.9436333570598022, 'support': 6637}, 'weighted avg': {'precision': 0.9482507336714835, 'recall': 0.9478680126563206, 'f1-score': 0.9477430006615493, 'support': 6637}}
