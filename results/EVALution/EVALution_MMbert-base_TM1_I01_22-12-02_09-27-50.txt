{'train_file': 'DATASETS/EVALution/train.tsv', 'val_file': 'DATASETS/EVALution/val.tsv', 'test_file': 'DATASETS/EVALution/test.tsv', 'train_templates': ["' <W1> ' <MASK> ' <W2> '"], 'model': 'bert-base-uncased', 'nepochs': 10, 'dir_output_results': 'results/', 'nrepetitions': 5, 'batch_size': 32, 'warm_up': 0.1, 'dataset': 'EVALution', 'parameters_list': None, 'date': '12/02/22-09:14:06', 'raw_model': False}
12/02/22-09:14:06
{'antonym': {'precision': 0.6863157894736842, 'recall': 0.7855421686746988, 'f1-score': 0.7325842696629213, 'support': 415}, 'hasa': {'precision': 0.773972602739726, 'recall': 0.795774647887324, 'f1-score': 0.7847222222222222, 'support': 142}, 'hasproperty': {'precision': 0.8099415204678363, 'recall': 0.860248447204969, 'f1-score': 0.8343373493975905, 'support': 322}, 'isa': {'precision': 0.6504854368932039, 'recall': 0.7298474945533769, 'f1-score': 0.6878850102669405, 'support': 459}, 'madeof': {'precision': 0.6973684210526315, 'recall': 0.6162790697674418, 'f1-score': 0.6543209876543209, 'support': 86}, 'partof': {'precision': 0.832, 'recall': 0.7172413793103448, 'f1-score': 0.7703703703703704, 'support': 145}, 'synonym': {'precision': 0.5688622754491018, 'recall': 0.34296028880866425, 'f1-score': 0.42792792792792794, 'support': 277}, 'accuracy': 0.7058504875406284, 'macro avg': {'precision': 0.7169922922965978, 'recall': 0.6925562137438313, 'f1-score': 0.6988783053574705, 'support': 1846}, 'weighted avg': {'precision': 0.7000474982964094, 'recall': 0.7058504875406284, 'f1-score': 0.6968365213229843, 'support': 1846}}
